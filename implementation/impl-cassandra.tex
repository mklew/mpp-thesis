%!TEX root = ../thesis.tex

\chapter{Implementation of the algorithm in Cassandra}\label{sec:mpp:impl}
In this section the discussion will point to implementation details of the algorithm and its components in Cassandra. 


\section{Assumptions}
There are few assumptions, as to implementation.
Implementation should be an extension to existing features of the database - cannot break nor change specification of existing functionality. Algorithm’s implementation should be part of database, not an application on top of database. User should use transactions according to the specification, any use opposed to the specification is not guaranteed to work as intended. Moreover, transactions should not be forced onto user, user has to use them as opt-in in a way they were designed. Transactions should not block normal updates - implementation won’t block user from doing non transactional updates to tables.

Implementation does not prevent user from misusing transactions. Implementation allows to use Cassandra in transactional way, but only if it is done according to the specification. In principle transactions are run with other transactions, whereas normal updates are independent from each other, as well as from transactions, which means that a normal update might be perceived, as inconsistent modification of data.

\section{Transactional consistency levels}
Transactions are implemented, as opt-in feature of Cassandra. We introduce new consistency levels that represent transactional workload. Adding new levels follows convention of how new features are added to Cassandra, as was the case with \lwt. 
 
Cassandra can operate on different consistency levels performing reads and writes. Different consistency levels have different performance, but also different guarantees. Examples of consistency levels are:
\begin{description}
\item[ONE] -- read/write performed only on single node
\item[QUORUM] -- read/write performed on quorum of replicas
\item[ALL] -- read/write from/to all replicas
\item[SERIAL / LOCAL_SERIAL] -- uses \lwt to do read/write with condition
\end{description}

New transactional consistency levels are \code{TRANSACTIONAL, LOCAL_TRANSACTIONAL}. Both use \mpt, but latter is confined to the local data center.

\section{Redefined Transaction state}
We redefine structure of transaction state \txState (introduced in Definition \ref{def:transactionStateChapter3}), as well as transaction items \txItems (presented in Definition \ref{def:transactionItemChapter3}) for implementation purposes, due to structure of $k$ in Cassandra, which is presented in Definition \ref{def:keyCassandra}. Definition \ref{def:transactionStateChapter4} presents new \txState with a unique transaction id. Definition \ref{def:transactionItemChapter4} shows new structure of transaction item \txItem, which we previously defined, as equivalent of key $k$ and we said that \txItem is a reference to the mutation \mutation{k}{v}. Cassandra allows us to have a reference to the \mutation{k}{v} without the need to know the exact $k$, as long as token value of $k$ is known. \txItem and topology \topology determine $N^'$, where private transaction storage \txStorage of each node $n$ can be checked for private data of transaction \transaction and then information from \txItem allows us to find \mutation{k}{v}.
% Names of keyspace and table are required
 % Cassandra organizes tables in \emph{keyspaces}, thus transaction items need additional information about keyspace to locate data at a node. Transaction item in Cassandra becomes:


\begin{definition}
\label{def:keyCassandra}
\emph{Key} -- key in Cassandra $k(k_p,k_c)$ consists of two keys: 
\begin{enumerate*}[label=\alph*)]
\item $k_p$ is a partitioning key, which is used by topology \topology to determine $\mathit{N}^{RF}_k$,
\item $k_c$ is a clustering key, which determines column in wide rows.
\end{enumerate*}
\end{definition}

\begin{definition}
\label{def:tokenChapter4}
\emph{Token} is a value computed by hash function from key $k_p$, which is used by topology \topology to determine replicas $\NRF_k$.
\end{definition}


\begin{definition}
\label{def:transactionItemChapter4}
\emph{Transaction item} $\lambda(\mathit{keyspace}, \mathit{table}, \mathit{token})$ is a triple with \begin{enumerate*}[label=\alph*)]
\item $\mathit{keyspace}$ -- name of keyspace,
\item $\mathit{table}$ -- name of table,
\item $\mathit{token}$ -- value of token (Definition \ref{def:tokenChapter4}) computed for a Cassandra key (Definition \ref{def:keyCassandra}).
\end{enumerate*}
\end{definition}

\begin{definition}
\label{def:transactionStateChapter4}
\emph{Transaction state} $\Lambda(\mathit{id}, \{\lambda_{i}, \lambda_{j}, ...\})$ a pair of unique id of type \emph{timeuuid}, which is UUID with encoded timestamp \cite{CassandraUUID} and a set of transaction items (Definition \ref{def:transactionItemChapter4}). Its meaning and purpose is the same, as in Definition \ref{def:transactionStateChapter3}.
\end{definition}


% \begin{lstlisting}[language=Java,style=outcode,label={lst:txState},caption={Transaction item in Cassandra}]

% class TransactionItem
% {
%    long token
%    String keyspaceName;
%    String tableName;
% }
% \end{lstlisting}

% Keyspace and table name are always required. Type of a token is just \emph{long} type. Type of transaction id is \emph{UUID with encoded timestamp} \cite{CassandraUUID}.


\section{Extending CQL}
Cassandra Query Language was extended to support new functionality and new statements. Adding any new feature to CQL requires a change to its syntax. We wanted to reuse, as much of functionality, as possible, therefore we modified syntax of: insert, update and delete statements and extended \emph{USING} clause to represent modification in a transaction. Listing \ref{lst:insertUsingTx} presents insert\footnote{which still is the same operation, as update} done in a transaction. Extended \code{using} clause supports \code{transaction} keyword and accepts transaction id. Normal modifications do not have any results in the response. However in case of transactional modification, response has results set with transaction item that must be added to transaction state by the client. Moreover, additional \emph{CQL} syntax is added to: start, commit and rollback transaction \transaction.

\paragraph{Start transaction statement} \code{START TRANSACTION;}
Statement starts transaction and has response with result set with a single row, which represents initial empty transaction state \txStatei{0}. 
 Important fact is that transaction id is generated inside cluster, thus timestamp encoded in id is in sync with clock in the cluster, therefore is reliable for comparing it with timestamps of other transactions.

\paragraph{Commit transaction statement} \code{COMMIT TRANSACTION AS JSON <\txState as json>;}
Begins the commit procedure of \mpt algorithm. A client has to serialize transaction state \txState into json and attach it to the statement.
%Client has to serialize Transaction State into json and execute commit statement. Only client has transaction state with all transaction items existing in the cluster for that transaction.


\paragraph{Rollback transaction statement} \code{ROLLBACK TRANSACTION AS JSON <\txState as json>;}
Performs the rollback of a transaction. 
%Transaction State is also required in order to send rollback messages only to nodes which have some data associated with transaction.


\begin{lstlisting}[style=outcode,label={lst:insertUsingTx},caption={Insert statement with using transactional clause}]
INSERT INTO ks.users VALUES(...) 
WHERE ...
USING TRANSACTION = <transaction_id>;
\end{lstlisting}

% example of tx id 

\section{Private Transaction Storage}
The private transaction storage implementation uses private \emph{memtables} to store transaction's mutations \mutations. It maps transaction id to its data \code{TransactionData}, which stores mutations mapped by keyspace, table and $k$, thus \txStorage has information about which keys are mutated in transaction \transaction. 
%Each owned transaction item has corresponding entry in transaction data map.
When new mutation \mutation{k}{v} is done in transaction \transaction, then \mutation{k}{v} is added to \code{TransactionData}. 
It is either merged with present mutation for the same partition key, or it is added as new entry in the map.


\subsection{Applying changes locally}
When transaction \transaction is committed at node \node{i}, its mutations are applied to target tables.
Cassandra uses timestamps to detect most recent writes, therefore all partition updates -- stored in mutations -- 
which were added to \txStorage at arbitrary time during execution of \transaction, have to be applied to target tables with timestamp that comes from \paxos ballot.
%have to be written with timestamp that comes from Paxos ballot, therefore timestamps of partition updates have to be modified and  Since mutations were added at arbitrary time during execution of transaction, their timestamps have to be updated to ballot's timestmap.,

% thus binary trees with data stored in \code{PartitionUpdates} have to be transformed into new binary trees with all timestamps updated to ballot's timestamp. Fortunately, Cassandra provides a method to rewrite binary trees with updated timestamps.


\subsection{In memory data}
Implementation of private transaction storage \txStorage stores mutations \mutations only in-memory, thus there is a risk that given many transactions Cassandra runs out of memory. However transactions are supposed to be short lived, thus memory should not be an issue.
%Number of concurrent transactions can be even controlled by client of the cluster. 
Another issue can be raised that \mutations are lost when node shuts down and comes back immediately. 
%In that case transaction’s mutations is at this node. 
However mutations are saved to quorum of replicas and we expect at least quorum of nodes to stay alive during transaction’s life span, thus in-memory storage is sufficient for transactions.


\section{Transaction Index}
The transaction index \txIndex needs to be thread-safe and stay consistent when concurrent transactions try to register. One solution is to create global lock on \txIndex, but this can cause contention. Other solution with less contention uses many locks acquired per transaction item \txItem.

\subsection{Locking of the transaction index}
In order to register transaction state \txState in the transaction index \txIndex, \txIndex has to be acquired.
\txIndex is a map from \code{IndexKey} to \code{PaxosParticipants}, in which keys are computed from transaction
state’s items replicated by this node. Each transaction item \txItem is mapped into index key by conflict function,
which results in set of index keys. Locks are acquired from map of locks accessed by hash value of an index key. In
order to avoid deadlock, index keys have to be sorted so that locks are acquired in the same order by different
transactions and released in reversed order.

\input{implementation/impl-conflict-functions.tex}

\section{Transaction Log}
The transaction log \txLog is implemented using Cassandra table local to each node. 
Transactions are added to the table with flag \code{commited = true} for committed transactions and \code{false} otherwise. We do not need to know about old transactions, thus we need to periodically clean the table. Cassandra supports TTL\footnote{Time to live} setting, which deletes rows after set time on the next run of compaction. Each row added to \txLog table has TTL set.


\section{Paxos State}
Paxos state is implemented as a table in \emph{system} keyspace, which is local to each node and stores information about \paxos round's state, as in Listing \ref{lst:paxosStateImpl}.

\begin{lstlisting}[style=outcode,label={lst:paxosStateImpl},caption={Table definition for multi partition transactions Paxos state}]
CREATE TABLE mpt_paxos (
paxos_id UUID,
in_progress_ballot timeuuid,
most_recent_commit blob,
most_recent_commit_at timeuuid,
most_recent_commit_version int,
proposal blob,
proposal_ballot timeuuid,
proposal_version int,
PRIMARY KEY (paxos_id))
\end{lstlisting}



\section{Write path extensions}
Write path of Cassandra \cite{CassandraWritePath} was modified in order to support transactional writes to private transaction storage \txStorage.
Write is acknowledged by coordinator node, as soon as mutation \mutation{k}{v} is written to quorum of replica nodes.
Write path was extended by detecting whether \emph{using transaction} clause was used. 

All mutations in Cassandra are represented by \code{Mutation} class. If transaction \transaction is detected, mutation is wrapped with \code{TransactionalMutation} which has \mutation{k}{v} and transaction id.
When transactional mutation is applied by its \code{apply} method, all the changes go to the private transaction storage instead of main tables.

\section{Read path extensions}
Read path \cite{CassandraReadPath} was also modified. 
Syntax of select statement stays without changes, because logic of executing select depends only on consistency level used to execute query. When select is performed with one of new consistency levels, then coordinator performs transactional read.


Transactional read guarantees that it will read consistent data and finish any in-progress transactions that can be found. It runs \mpt, but completes early when there are no in-progress proposals or when it has repaired all in-progress transactions.


Since we want to reuse \mpt code we need to create a read-only transaction with a single transaction item \txItem representing key $k$ which we want to select. For read transactions, \mpt starts assuming all replicas are already in setup phase. 
We marked transaction state \txState, as read-only \txState by encoding a future timestamp in its transaction id.
The Transaction Index \txIndex detects read-only transaction and return \paxos round id \paxosRoundId of first conflicting \txState it finds. There is no need to register read-only transaction in \txIndex, because it has to be invisible to other transactions.
Read is performed only when the coordinator has became the leader and completed all in-progress transactions. In such case tables have only committed data. Quorum consistency level is used for normal read, because \mpt relies at least quorum being consistent.

% is represented by the same Transaction State data structure, we need to convey information that it is indeed read-only
 transaction. Simple implementation could add another flag which would be worthless for all other use cases. We decided to convey read-only information in the timestamp of transaction by creating a UUID from timestamp far in the future. This way, there is no need for modifications to Transaction State data structure. This special read-only transaction state is only used between cluster nodes so internal details are not visible to the client.


% Transactional read algorithm looks as follows:
% 1. Select query for key K with transactional CL
% 2. Create read-only transaction state
%    1. It has single transaction item with token from K, keyspace and table as in query
% 1. Run MPP round starting with replicas in pre prepared phase
% 2. Execute MPP until it has successfully transitioned into prepare phase
%    1. If there is in progress proposal with transaction, try to complete it
% 1. Perform normal read with CL=Quorum


% \section{Replica Groups Phases Executor}
% Replica groups phases executor is the implementation of the state machine that performs transitions on replicas and moves them from phase to phase according to the specification.

% Phases need to be compared with each other for their advancement. Enums with proper ordering of enum values is suitable for java implementation.

% \begin{lstlisting}[style=outcode,label={lst:phaseEnum},caption={Phase enum}]
% public enum Phase
% {
%    NO_PHASE,
%    ROLLBACK_PHASE,
%    PRE_PREPARE_PHASE,
%    BEGIN_AND_REPAIR_PHASE,
%    PREPARE_PHASE,
%    PROPOSE_PHASE,
%    COMMIT_PHASE,
%    AFTER_COMMIT_PHASE;
% }
% \end{lstlisting}

% It is important that, begin and repair phase (repairing phase) is less advanced than prepare phase. Otherwise some replicas could try to propose a new value instead of waiting until all replica groups are at least in the prepared phase.

