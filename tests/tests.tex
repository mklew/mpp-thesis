%!TEX root = ../thesis.tex

\chapter{Tests and performance analysis}\label{chapter:testing}

This chapter is focused on explaining theory behind testing of \mpt algorithm and its components that gives high certainty about correctness of algorithm and its implementation. Many tests were performed on different levels that had to verify if implementation is correct, but what is more important if new \mpt algorithm delivers its promise of multi--partition read-committed transactions. 
\mpt algorithm developed during this research has been proven to work in practice for all written tests. 
%Writing a formal proof is a complex task which would require further research and is out of scope of this work.

\section{Unit testing}
Components such as \emph{Transaction Index}, \emph{Transaction Log}, \emph{Private Transaction Storage} were  tested for their expected behaviour. Tests also covered parts of \mpt which were not cluster dependent such as \emph{phase} transitions. Unit tests are not enough to check for correctness especially when testing distributed algorithms which are  exposed to network latencies, concurrency issues like deadlocks and other problems in the distributed environment. 

\section{Cluster tests}
Tests on real cluster of nodes were major part of all tests performed to validate \mpt correctness. I have conducted multiple experiments which covered all aspects of \mpt algorithm. 

\subsection{Testing environment}
Cluster tests were performed independently on two clusters with $3$ and $5$ nodes per cluster. Tests run on $5$ nodes yielded important results, because with $5$ nodes and $N=3$ there are at most $10$ replica groups. 

Clusters were run locally using \emph{Cassandra Cluster Manager} which is a tool to quickly set up local cluster.

\subsection{Testing method}

\subsubsection{Counters}
Tests are performed using testing schema with counter tables. Counter table is shown on Fig. \ref{fig:counterTable}. It has $5$ columns with \emph{int} counter values: $c1,c2,c3,c4,c5$
Testing schema consists of $2$ keyspaces with number of counter tables in each. Counter tables in each keyspace differ in name, but structure is the same.

\begin{figure}[h]
\centering
\begin{tabular}{c||c|c|c|c|c}
        \toprule
        id 		 & $c_{1}$ & $c_{2}$ & $c_{3}$ & $c_{4}$ & $c_{5}$ \\ \midrule
        $counter_{1}$ & 1  & 1  & 1  &  1 & 1  \\
        $counter_{2}$ & 1  & 2  & 3  &  4 & 5  \\ \bottomrule
      \end{tabular}
      \caption{A counter table with two sample rows}
  \label{fig:counterTable}
\end{figure}

\subsubsection{Iterations}
Each test case is done in many iterations i.e. $300$. During each iteration, there are many concurrent transactions which execute at the same time. Each transaction performs operations on counters. Most basic operation is to increment counter column using transactional update operation.

\subsubsection{Counter executors}
Counter executors run in parallel through all iterations. 
Counter executors operate on set of counters. They perform same operation on each counter in each iteration, where each iteration is a new transaction that is committed at the end of an iteration. 
Basic executor increments specific counter column $c$ of each counter in the set.

\subsection{Testing of repair of in progress proposal}
Tests required modification of an implementation. During this test, counters from table named \code{stop_after_proposed} are incremented. \mpt has been modified to fail after algorithm reached has successfully proposed, thus before commit. It leaves transaction as accepted in progress proposal that should be finished by next proposer. 
Normal select is used to check that changes were not yet committed by the transaction. 
Then a transactional select is used which has to run \mpt algorithm until \emph{prepare phase} is reached. In progress proposal is detected during prepare and has to be completed. Transactional select commits transaction and returns incremented column. Results of select statement are checked for correctness. 

\subsection{Testing of independent transactions}
Tests employs multiple counter executors which receive disjoint sets of counters. Each executor increments counters by one and commits transactions. Each executor keeps track of number of committed transactions.

After iterations, number of committed transactions must match counter values. In case of non conflicting counters, all transactions need to commit, thus counters must have counts equal to number of iterations.

\subsection{Testing of conflicting transactions}
There are $5$ counter executors which receive same set of counters. Each executor increments different count column. 

After iterations, number of committed transactions must match counter values. In case of conflicting transactions counts are less than number of iterations. Counters may have different counts in columns, but all counter rows must have same values in $c_{x}$ columns, because they were incremented by single executor.

\subsection{Testing of partially conflicting transactions}
Test runs the same way as test for conflicting transactions, but executors receive additional disjoint sets of counters, so each executor has intersection on counters plus additional ones private for executor. 

Assertions are the same as in the previous test.

\subsubsection{Different conflict functions}
Testing schema has counter tables with different conflict functions set. \mpt has to work for any conflict functions. Testing scenario remains the same. Expected results are also the same.

\subsection{Testing correctness of commit and rollback}
Testing correctness of rollback and commit is the most crucial test, because tests with incrementations are dependent on truth about rollback and commit responses.

For this test, different counter executors were used. There are $2$ different counter executors. Both of them receive set of counters with even number of counters. Size of the set is double number of iterations. All counters start at $0$.

First counter executor, \emph{Next Two Counter Executor}, in each iteration increments next two counters. In iteration $0$, increments $counter_{0}$, $counter_{1}$, in iteration $1$ increments $counter_{2}, counter_{3}$ and so continues until end of iterations. Second counter, \emph{Even Counter Executor}, in each iteration increments all counters which have even index in the set.

Rationale behind test is that when \emph{Next Two Counter Executor} fails to commit and receives rollback, then for this particular iteration counter with odd index should not be incremented by executor, thus must have initial $0$ count. Otherwise transaction should be committed instead of rolled back.

\subsection{Testing during partial failure}

All previously described tests were also run with cluster during partial failure. Node was shutdown and tests were run. Since \mpt tolerates partial failures, as long as quorum is alive and responds to requests, tests yield same results.

\section{Theoretical network partition tolerance}
The algorithm makes progress, as long as quorum in each replica group moves in the expected phase, thus the algorithm is resilient to failures of $k$ nodes. The best case and the worst case is shown in the Fig. \ref{fig:tests:networkPartitionTolerance}. $RGs$ denotes number of replica groups. The key conclusion is that resilience scales with the size of a transaction, since it relies only on quorums in each replica group.

\begin{figure}[hbt]
  %\centering
  \setlength{\unitlength}{1.3cm}  
  \subfloat{
    \renewcommand{\tabcolsep}{0.1cm}
    \resizebox{\textwidth}{!}{\begin{tabular}{c|c|c}
      \toprule
      case & k & conclusion \\ \midrule
      best case & $RGs * (N - (\frac{N}{2} + 1)$ & occurs, when replica groups are disjoint  \\
      worst case & $N - (\frac{N}{2} + 1)$  & occurs, when all replica groups intersect  \\  \bottomrule      
    \end{tabular}}
  }
  \caption{The best and the worst case of the network partition tolerance}
  \label{fig:tests:networkPartitionTolerance}
\end{figure}

\section{Analyzing number of requests}
Assume transaction $t$, which modifies $PC=3$ partitions: $p_{1}, p_{2}, p_{3}$,
, which might be replicated on one up to three unique replica groups, thus number of nodes affected spans from minimum of $N$ to maximum of $PC*N$. Number of unique replica groups depends whether replica groups are independent, thus contain different nodes, of if they overlap in nodes.

Starting a transaction requires a single request, thus $1$ request to any node in the cluster, which responds with Transaction State.
Each modification of the partition needs to be written to the private memtables, and acknowledged by the quorum from a replica group, thus writes perform $[N,PC*N]$ requests.
Nextly transaction is committed, thus the single commit request $1$ is performed.
The commit procedure performs most of the requests.
Replicas need to transition to the pre prepare phase, thus leader has to send $[N,PC*N]$ requests and receive responses.
Pre prepare phase performs data consistency check, thus each node reads from quorum of replicas, resulting in possible requests count: $[N * (\frac{N}{2} + 1),PC * N * (\frac{N}{2} + 1)]$.
Nextly the leader transitions replicas to prepare phase, which requires $[N,PC*N]$ requests, followed by propose phase and another wave of $[N,PC*N]$ requests, and lastly it runs transition to the commit phase with another batch of $[N,PC*N]$ requests.

The total number of requests, assuming the optimistic path without contention and no failures, 
is: $[2 + 5 * N + N * (\frac{N}{2} + 1), 2 + 5 * PC*N + PC * N * (\frac{N}{2} + 1)]$,
which generalises to $2 + 5 * PC*N + PC * N * (\frac{N}{2} + 1)$ where $PC \in [1, N]$.

Number of requests could be limited if at each step only quorums were considered, thus replacing single $N$ in formula by $\frac{N}{2} + 1$, which would result in the formula: 
$2 + 5 * PC*(\frac{N}{2} + 1) + PC * (\frac{N}{2} + 1) * (\frac{N}{2} + 1)$ where $PC \in [1, N]$.
The Fig. \ref{fig:tests:requestsCount} depicts concrete numbers of requests in both cases.


\begin{figure}[hbt]
  \centering
  \setlength{\unitlength}{1.3cm}  
  \subfloat{
    \renewcommand{\tabcolsep}{0.1cm}
    \begin{tabular}{c|c|c|c}
      \toprule
      $N$ & $PC$ & requests to all & requets to quorums  \\ \midrule
      $N=3$ & $PC=1$   & 23  & 16 \\
      $N=3$ & $PC=2$   & 44  & 30 \\
      $N=3$ & $PC=3$   & 65  & 44 \\
      $N=5$ & $PC=1$   & 37  & 16 \\
      $N=5$ & $PC=2$   & 72  & 30 \\
      $N=5$ & $PC=3$   & 107 & 44  \\
      $N=5$ & $PC=4$   & 142 & 58  \\
      $N=5$ & $PC=5$   & 177 & 72  \\ \bottomrule  
    \end{tabular}
  }
  \caption{Request count given different $N$ and unique partitions count $PC$}
  \label{fig:tests:requestsCount}
\end{figure}

\section{Theoretical performance analysis}
Let us analyze the performance of the \mpt algorithm considering independent increase of the following parameters: 
\begin{enumerate*}
\item $p$ - modified partitions,
\item $n$ - number of nodes in a cluster,
\item $N$ - replication factor,
\item concurrent conflicting transactions,
\item concurrent independent transactions.
\end{enumerate*}

\subsection{Increasing number of partitions}
\label{sec:tests:perf:partitions}
When \mpt affects more partitions, it is more likely that these partitions are replicated on different replicas, thus the number of replica groups increases as well. Replica groups are bound in size by the total number of nodes in the cluster. If replica groups cover all nodes, then increase in modified partitions will not further affect number of requests performed. Therefore number of requests increases linearly with the replica groups, but is has a limit, after which it remains constant.

\subsection{Increasing number of nodes}
Let us consider a cluster of $n$ nodes and a transaction that modifides set of partitions $S_{p}$, which are replicated in the subset of nodes $S_{n}$.
When nodes are added to the cluster token ranges are reassigned, thus nodes $n'$ are responsible for smaller token ranges, which causes two possible outcomes:
\begin{enumerate*}
\item $S_{n}$ increases to $S'_{n}$, since after reassignment $S_{p}$ is replicated by more nodes,
\item $S_{n}$ remains the same, if the token ranges of $S_{p}$ were not affected by the reassignment.
\end{enumerate*}

The former case increases the number of requests during \mpt, as if number of partitions is increased \ref{sec:tests:perf:partitions}.

\subsection{Higher replication factor}
In this case $N$ is increased to $N'$, thus replica groups contain more replicas. \mpt relies on the quorum of each replica group, thus number of requests increases as well, but only if replica groups remain disjoint. In case they overlap, the number of requests stays the same, even if replication factor increases.

\subsection{Increase in concurrent conflicting transactions}
Let us consider a partition $p$ and an set of transactions $S_{t}$, in which each transaction modifies $p$, that increases to a larger set $S'_{t}$. 
Transactions participate in same \paxos round, due to the fact that all of them modify same partition $p$, and any conflict function \label{sec:theory:conflictFunctions} will detect it.
Concurrent transactions could cause contention and leadership changes of the \paxos round, but only if transactions are committed at the same time. Contention is detected by rejecting proposed ballot, in which case the coordinator of a transaction waits until a timeout occurs and tries again with a higher ballot. The timeout provides the time window, in which the current leader proposes its transaction. Even if leadership changes, the proposed and accepted transaction is committed and other conflicting transactions are rolledback, thus if contention is resolved, then the algorithm finishes the one transaction from the set $S'_{t}$. Initial contention might have negative effect on the overall performance of the algorithm.

Moreover, transactions $S'_{t}$ will store its modifications of $p$ on the same replica group $RG$, and register in an index, which increases memory footprint on the nodes in $RG$.

\subsection{Increase in concurrent independent transactions}
Independent transactions, which do not modify same partitions, do not cause contention on each other, thus the algorithm scales and its performance is not affected by the increase. Likewise the memory footprint is divided among the cluster, thus there are no single nodes, which would have to handle more data than other nodes.

% TODO opis teoretyczny od czego zależy wydajność
% liczba partycji
% liczba nodów
% liczba replik
% liczba współbieżnych transakcji 

% głównie tekst + tabelki i diagramy

% Opisać różne przypadki, jak MPP zachowuje się przy wzroście nodów, replik, ilości konfliktujących transkacji

% TODO każdy czynnik rozważyć osobno (pół strony ) i uzasadnić dlaczego się tak zachowuje.

% TODO, co tu opisac?


