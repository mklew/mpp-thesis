%!TEX root = ../thesis.tex

\chapter{Tests and performance analysis}\label{chapter:testing}

This chapter is focused on explaining theory behind testing of \mpt algorithm and its components that gives high certainty about correctness of algorithm and its implementation. Many tests were performed on different levels that had to verify if implementation is correct, but what is more important if new \mpt algorithm delivers its promise of multi--partition read-committed transactions. 
\mpt algorithm developed during this research has been proven to work in practice for all written tests. 
%Writing a formal proof is a complex task which would require further research and is out of scope of this work.

\section{Unit testing}
Components such as \emph{Transaction Index}, \emph{Transaction Log}, \emph{Private Transaction Storage} were  tested for their expected behaviour. Tests also covered parts of \mpt which were not cluster dependent such as \emph{phase} transitions. Unit tests are not enough to check for correctness especially when testing distributed algorithms which are  exposed to network latencies, concurrency issues like deadlocks and other problems in the distributed environment. 

\section{Cluster tests}
Tests on real cluster of nodes were major part of all tests performed to validate \mpt correctness. I have conducted multiple experiments which covered all aspects of \mpt algorithm. 

\subsection{Testing environment}
Cluster tests were performed independently on two clusters with $3$ and $5$ nodes per cluster. Tests run on $5$ nodes yielded important results, because with $5$ nodes and $N=3$ there are at most $10$ replica groups. 

Clusters were run locally using \emph{Cassandra Cluster Manager} which is a tool to quickly set up local cluster.

\subsection{Testing method}

\subsubsection{Counters}
Tests are performed using testing schema with counter tables. Counter table is shown on Fig. \ref{fig:counterTable}. It has $5$ columns with \emph{int} counter values: $c1,c2,c3,c4,c5$
Testing schema consists of $2$ keyspaces with number of counter tables in each. Counter tables in each keyspace differ in name, but structure is the same.

\begin{figure}[h]
\centering
\begin{tabular}{c||c|c|c|c|c}
        \toprule
        id 		 & $c_{1}$ & $c_{2}$ & $c_{3}$ & $c_{4}$ & $c_{5}$ \\ \midrule
        $counter_{1}$ & 1  & 1  & 1  &  1 & 1  \\
        $counter_{2}$ & 1  & 2  & 3  &  4 & 5  \\ \bottomrule
      \end{tabular}
      \caption{A counter table with two sample rows}
  \label{fig:counterTable}
\end{figure}

\subsubsection{Iterations}
Each test case is done in many iterations i.e. $300$. During each iteration, there are many concurrent transactions which execute at the same time. Each transaction performs operations on counters. Most basic operation is to increment counter column using transactional update operation.

\subsubsection{Counter executors}
Counter executors run in parallel through all iterations. 
Counter executors operate on set of counters. They perform same operation on each counter in each iteration, where each iteration is a new transaction that is committed at the end of an iteration. 
Basic executor increments specific counter column $c$ of each counter in the set.

\subsection{Testing of repair of in progress proposal}
Tests required modification of an implementation. During this test, counters from table named \code{stop_after_proposed} are incremented. \mpt has been modified to fail after algorithm reached has successfully proposed, thus before commit. It leaves transaction as accepted in progress proposal that should be finished by next proposer. 
Normal select is used to check that changes were not yet committed by the transaction. 
Then a transactional select is used which has to run \mpt algorithm until \emph{prepare phase} is reached. In progress proposal is detected during prepare and has to be completed. Transactional select commits transaction and returns incremented column. Results of select statement are checked for correctness. 

\subsection{Testing of independent transactions}
Tests employs multiple counter executors which receive disjoint sets of counters. Each executor increments counters by one and commits transactions. Each executor keeps track of number of committed transactions.

After iterations, number of committed transactions must match counter values. In case of non conflicting counters, all transactions need to commit, thus counters must have counts equal to number of iterations.

\subsection{Testing of conflicting transactions}
There are $5$ counter executors which receive same set of counters. Each executor increments different count column. 

After iterations, number of committed transactions must match counter values. In case of conflicting transactions counts are less than number of iterations. Counters may have different counts in columns, but all counter rows must have same values in $c_{x}$ columns, because they were incremented by single executor.

\subsection{Testing of partially conflicting transactions}
Test runs the same way as test for conflicting transactions, but executors receive additional disjoint sets of counters, so each executor has intersection on counters plus additional ones private for executor. 

Assertions are the same as in the previous test.

\subsubsection{Different conflict functions}
Testing schema has counter tables with different conflict functions set. \mpt has to work for any conflict functions. Testing scenario remains the same. Expected results are also the same.

\subsection{Testing correctness of commit and rollback}
Testing correctness of rollback and commit is the most crucial test, because tests with incrementations are dependent on truth about rollback and commit responses.

For this test, different counter executors were used. There are $2$ different counter executors. Both of them receive set of counters with even number of counters. Size of the set is double number of iterations. All counters start at $0$.

First counter executor, \emph{Next Two Counter Executor}, in each iteration increments next two counters. In iteration $0$, increments $counter_{0}$, $counter_{1}$, in iteration $1$ increments $counter_{2}, counter_{3}$ and so continues until end of iterations. Second counter, \emph{Even Counter Executor}, in each iteration increments all counters which have even index in the set.

Rationale behind test is that when \emph{Next Two Counter Executor} fails to commit and receives rollback, then for this particular iteration counter with odd index should not be incremented by executor, thus must have initial $0$ count. Otherwise transaction should be committed instead of rolled back.

\subsection{Testing during partial failure}

All previously described tests were also run with cluster during partial failure. Node was shutdown and tests were run. Since \mpt tolerates partial failures, as long as quorum is alive and responds to requests, tests yield same results.

\section{Theoretical performance analysis}

\subsection{Theoretical number of requests during algorithm}
> \code{TODO to nie jest gotowe, tylko zostalo przeniesione z rozdzialu o implementacji} <
There can be many modifications to different keys resulting in many transaction items, but what is important are the number of replicas. At worst case each key will reside on different replica set. Most likely case is that some replica groups have replicas in common.


Assuming worst case scenario we have number of replica groups: RGC (Replica Group count)
And replication factor N. For sake of argument let’s assume that all modifications share same replication factor although it is not required by algorithm.


Total number of replicas: TR = N * RGC


Pre prepare phase requires reads from other replicas to make data consistent. Number of requests to make data consistent depends on number of replica groups to which keys owned by given node are replicated. So we have:
keys per node: KPN
And replica groups which own data for these keys: RGOC
We know that RGOC < RGC and at worst case it will be just less than 1 so RGOC = RGC - 1


To make data consistent each node has to make at least quorum of RGOC, but since each node belongs to all the other replica groups, number of requests can be further reduced to QUORUM-1 which equals to (((RGC -1 ) / 2 + 1) - 1 = (RGC -1) / 2
Since each nodes has to do it pre prepare phase requires TR * (RGC -1) / 2 of requests. Plus each replica has to receive message in order to do pre prepare phase therefore pre prephase phase looking at whole algorithm requires TR + TR * ((RGC -1) / 2) 


Prepare phase, propose phase and commit phase require TR requests each.


To sum it up, total number of requests at worst case equals to 
TR ((RGC-1) / 2) + 4*TR


For a five nodes cluster with replication factor N=3 it has
10 different replica sets.

% TODO opis teoretyczny od czego zależy wydajność
% liczba partycji
% liczba nodów
% liczba replik
% liczba współbieżnych transakcji 

% głównie tekst + tabelki i diagramy

% Opisać różne przypadki, jak MPP zachowuje się przy wzroście nodów, replik, ilości konfliktujących transkacji

% TODO każdy czynnik rozważyć osobno (pół strony ) i uzasadnić dlaczego się tak zachowuje.

% TODO, co tu opisac?
%\section{Performance testing}


