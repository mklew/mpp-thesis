%!TEX root = ../thesis.tex

\chapter{Theory}\label{chapter:theory}

\section{Distributed databases}\label{sec:theory:distDbs}
Distributed databases are .. \ref{fig:hierarchyElements}. \cite{CassandraDataStaxDocs} \cite{chandra2007PaxosMadeLive} \cite{lamport1982byzantine}

\subsection{Cassandra}
% TODO referencje do Cassandry, krotki wstep o Cassandrze
Cassandra is one of more widely used NoSql databases. 

Cassandra


\section{Transactions}\label{sec:theory:transactions}
Transactions in RDBMS

% TODO organizacja rozdziału, czy paxos powinien mieć pełny opis, czy tylko referencja? 
\subsection{ACID}

\subsection{2 phase commit}

\section{Distributed transactions}

\subsection{3 phase commit}\label{sec:theory:transactions:3pc}

\subsection{Paxos}

\subsection{Raft}
\emph{Raft} algorithm has same guarantees, as \paxos. \emph{Raft} has been designed to be an example of more understandable distributed consensus algorithm than \paxos. \cite{ongaro2014search} The difference to \paxos  is that \emph{Raft} has been decomposed into independent subproblems which clearly address problems. \emph{Raft} uses \emph{Leader election}. Leader is elected for specific term. \emph{Raft} uses \emph{Log Replication} to arrive at consensus. Leader proposes new value to \emph{followers} which append value to uncommitted log. Once leader has majority of acknowledges it sends commit message. Followers receive commit message and commit value in the log. Visualization of \emph{Raft} algorithm is available at \cite{raftVisual}.

\subsection{RAMP}\label{sec:theory:transactions:ramp}
\emph{Read Atomic Multi Partition} transactions \cite{Bailis:2014} are most recent compared to mentioned algorithms. \emph{RAMP} identify new transaction isolation level \emph{Read Atomic}. \emph{RAMP} transactions are designed to match requirements of use cases, such as secondary indexing, foreign key
enforcement, and materialized view maintenance. \emph{RAMP} ensures \emph{atomic visibility}: either all or none of each transaction's updates are seen by other transactions. \emph{RAMP} is resilient to partial failures (via \emph{synchronization independence}) and has network partition independence by minimized communication between servers. \emph{RAMP} uses multi-versioning with a variable but small amount of metadata per write. \emph{RAMP} transactions are linearly scalable. Scalability has been tested up to 100 servers.

\subsection{LWT}\label{sec:theory:transactions:lwt}

 How LWT are implemented


LWT is based on paxos algorithm without leader election phase. It is done without leader election to avoid having single point of failure for just brief moment of having leader that is going to propose new value. Without leader election any node can act as a leader. Therefore any node can propose new value to other nodes at any time. It increases risk of contention, but it was a tradeoff that creators of C* accepted. 


In order to use paxos, it is necessary to have a place to store paxos round’s information. That is proposed value, ballot, most recent commit and so on as required by paxos. That data structure is called Paxos State.
Value in LWT is an update U for single key K. Key points to one partition with data to be modified. We could also say, that LWT are defined for single token on token ring since partition is represented by a token.
Therefore value is identified by K and so paxos state can be identified by same key. It is crucial to be able to find paxos state given proposed value. In case where value is a single update, paxos state can be naturally identified by same key. It won’t be the case when paxos value consists of many keys.


LWT consists of phases which are:
1. Prepare
2. Get promise
3. Read row
4. Get results
5. Check condition
6. Propose new value
7. Acceptance
8. Broadcast commit
9. Wait for acknowledges


Let’s analyze flow of actions given first example of LWT with table users and user John.
Assume that replication factor N=3 so replicas are: node1, node2, node3.


1. LWT is initiated by client request: insert John if not exists
2. Request is routed to some node X that becomes coordinator of LWT
3. Coordinator wants to be a leader so it creates a ballot and sends prepare to replicas: node1, node2, node3. This is prepare phase.
4. Replicas promise to not accept values from leaders who have lower ballots.
5. Coordinator gets 3 of out 3 promises. Quorum promised to listen.
6. Coordinator performs read: SELECT * FROM users WHERE user_id = 1
7. Coordinator checks condition “IF NOT EXISTS”. If such row did not exist in quorum of replicas it proceeds further
8. Coordinator, as a leader, proposes new value which is an insert to users table.
9. All three nodes reply with accept. Quorum accepted insert.
10. At this point, insert is an in progress proposal that has been accepted by majority of replicas. Even if coordinator crashes, once someone selects user with id=1 or retries to insert it. This proposal will get committed. Let’s assume that coordinator is well and proceeds to commit.
11. Coordinator broadcasts commit message
12. Replicas apply insert locally and reply with acknowledge message
13. Coordinator receives quorum of acknowledges. LWT has been successfully completed.
14. Coordinator responds to client: John was inserted.


Sequence above is a happy path. At any point coordinator might lose his leadership if some other node sends prepare with higher ballot. In that case coordinator waits a bit and retries whole process again. 
Also any replica might stop responding, which is fine as long as quorum is up and well. If quorum responds to messages, coordinator can proceed. If there is no quorum then paxos conditions are not met and LWT cannot be continue. However during normal operation is it rather rare to see many nodes down so in general LWT can successfully finish with 2 out of 3 replicas running. Note that cluster size might be much bigger, for example hundred nodes, but since replication factor N=3, only three nodes are interesting from perspective of LWT and Paxos.


