%!TEX root = ../thesis.tex
\section{Transactions}\label{sec:theory:transactions}
Transactions in RDBMS

TODO napisac ze transakcje w relational sa ACID a w noSql sa to jakies podzbiory tych funkcjonalnosci lub inne rozwiania

% TODO organizacja rozdziału, czy paxos powinien mieć pełny opis, czy tylko referencja? 
\subsection{ACID}
TODO Czy ja w ogóle powinienem to rozszerzac? Czytelnik powinien to znac.
Transactions in relational databases are ACID compliant, which means that a transaction is \begin{enumerate*} 
\item Atomic - either all statements in the transaction succeed or none,
\item Consistent - TODO
\item Isolated - transactions do not interfere with each other during execution,
\item Durable - changes made by a committed transaction are preserved even after the database failure.
\end{enumerate*}

\subsection{2 phase commit}

\section{Distributed transactions}
% TODO rozrysować co czym jest, Raft, Paxos -> distributed consensous algorithm
% RAMP atomic commit

% LWT (CAS) distributed transaction, compare and set with distributed consensous. Implementacja w Cassandrze.

\subsection{3 phase commit}\label{sec:theory:transactions:3pc}

\subsection{Paxos}

since it has network partition tolerance, it has advantage over \emph{3PC} \ref{sec:theory:transactions:3pc}.

% TODO opisać na tyle szczegółowo żeby móc je porównać. 
% bez dokładnego schematu
% ogólny schemat, najbardziej typowy przypadek opisać, bez dowodów, bez wyników testów.
% żeby było wiadomo o co chodzi.

% po to żeby móc porównać, Paxos jest lepszy od 3PC bo (ma partycje) i źródło "dokładnie jest to opisane tu i tam"
% ma wynikać z jakiego powodu jest lepszy


%  Paxos, Raft, 3PC opisane na jednym poziomie szczegółowości

% żeby do zrozumienia pracy nie trzeba było zaglądać gdzieś indziej.


\subsection{Raft}
% TODO napisać jakie to są guarantees
\emph{Raft} algorithm offers the same guarantees, as \paxos. 
The aim of Raft designers was to create a consensus algorithm which would be more understandable to its predecessor \cite{ongaro2014search}. 
The main idea behind the algorithm is decomposition of the consensus problem into the following individual tasks:
\begin{enumerate*}
\item leader election,
\item log replication,
\item safety,
\item membership changes.
\end{enumerate*} 
%Safety property is that: if any server
%has applied a particular log entry to its state machine,
%then no other server may apply a different command for the same log index.
 A server supporting \emph{Raft} is in one of three states: \begin{enumerate*} \item leader, \item follower, \item candidate \end{enumerate*}. The leader can be only one and it handles all client requests. When a follower receives a request, it has to redirect request to the leader. The candidate state is used during leader election. 
% TODO reszta opisu

 %The difference to \paxos  is that \emph{Raft} has been decomposed into independent subproblems which clearly address problems. 

% TODO %
% Napisać jak ten RAFT działa
\emph{Raft} uses \emph{Leader election}. Leader is elected for specific term. \emph{Raft} uses \emph{Log Replication} to arrive at consensus. Leader proposes new value to \emph{followers} which append value to uncommitted log. Once leader has majority of acknowledges it sends commit message. Followers receive commit message and commit value in the log. 

\emph{Raft} algorithm is visualized in \cite{raftVisual}.

\subsection{RAMP}\label{sec:theory:transactions:ramp}
\emph{Read Atomic Multi Partition} transactions \cite{Bailis:2014} is the newest algorithm among the algorithms discussed in this chapter. 
\emph{RAMP} algorithm was designed to address the requirements of the following use cases: secondary indexing, foreign key
enforcement, and materialized view maintenance.
\emph{RAMP} identifies new transaction isolation level \emph{Read Atomic}, which 
assures consistent visibility of changes made by the transaction: either all or none of each transaction's updates are seen by other transactions.
\emph{RAMP} is resilient to partial failures -- failures of part of the nodes in the cluster -- for the reason that RAMP's transaction does not contact nodes that are not affected by the transaction. 
\emph{RAMP} also guarantees \emph{synchronization independence}, which means that transactions do not stall or fail other transactions, due to the fact that reads and writes do not block each other. 

%The transaction is able to detect non-atomic partial read and repair it with additional rounds of communication with nodes. 
The key principle behind \emph{RAMP} is its ability to detect non-atomic partial read, and to repair it with additional rounds of communication with the nodes by the means of \emph{the metadata} attached to each write and \emph{multi-versioning}, which means that the data is accessible in different versions and each modification to the data creates a new version. Versions are either committed or not yet committed, in which case the data can be discarded by aborting the transaction \cite[p. 6]{Bailis:2014}. 
There are three variants of \emph{RAMP} algorithm and the difference between them is the type of the metadata. \emph{RAMP-small} uses a 64 bit timestamp as the metadata and requires additional two rounds of communication when non-atomic read is detected. Other variants of \emph{RAMP} are described in \cite[p. 5]{Bailis:2014}.
\emph{RAMP} was proven to be linearly scalable with the number of nodes during trials spanning up to 100 servers \cite[p. 10]{Bailis:2014}.

%  TODO Może to gdzieś się opłaca dodać:
% \emph{RAMP} transactions \ref{sec:theory:transactions:ramp} have support for multi-partition operations. 
% However \emph{RAMP} relies on 64 bit timestamp \cite[p. 8]{Bailis:2014} which is not the current standard \cite{timestamp64wiki}, thus \emph{RAMP} cannot be used at the time being.
%

\subsection{LWT}\label{sec:theory:transactions:lwt}
% TODO źródło w (more on this)
As mentioned earlier, Light Weight Transactions is the name of compare-and-swap functionality implemented in Cassandra. Internally, LWT uses the Paxos algorithm with some modifications which were aimed to prevent potential single point of failure, thus improving high availability. SPOFs may surface in case the leader fails when it is about to propose a new value (more on this…). The problem was addressed by sacrificing the standard leader election in favour of allowing any node to act as a leader, so that it can propose a value at any time. On the downside, such approach increases the risk of contention. 

One of the key requirements of Paxos protocol is storage of PaxosState - a data structure composed of the proposed value, proposed ballot, promised ballot and most recent commit. In case of LWT, the value represents the update itself along with the token of the partition key where that update should be applied. That token uniquely identifies PaxosState so that several proposals for the same partition can be associated with the same Paxos instance. 


LWT includes the following phases: \begin{enumerate*}
\item prepare,
\item get promise,
\item read row,
\item get results,
\item propose a new value,
\item acceptance,
\item broadcast commit,
\item wait for acknowledgements.
\end{enumerate*} Let us analyze each of them in the environment with 5 nodes, replication factor 3 and request to insert user named \emph{John} to \emph{users} table, but only if \emph{John} does not exist. For simplicity, \emph{users} table has only two columns: \begin{enumerate*} 
\item name -- text column and primary key, \item email -- text column. \end{enumerate*} LWT begins with a node 5 receiving request. Node 5 is then called coordinator node. The coordinator has to become the leader of paxos round identified by the token computed from partition key of \emph{users} table, thus from string value \emph{John}. To become the leader, the coordinator has to create the new ballot and send it along with the token and table name to replica nodes: node 1, node 2, node 3. Each replica has to check whether the ballot is the highest ballot it has seen, and if it is, then replica saves it in the PaxosState and responds with a promise. The leader has to wait for majority of promises before it proceeds. When majority promises, the coordinator may proceed to read the row and check the condition. The condition is that \emph{John} cannot exist, thus read has to not find any rows querying users by user name \emph{John}. Let us assume that indeed user \emph{John} did not exist, thus condition is met which allows the coordinator to propose an insert of \emph{John} with email \emph{john.doe@gmail.com} to \emph{users} table. The coordinator sends proposal message, with an insert and same ballot, to replicas and waits for acceptances. A replica saves proposed insert to PaxosState and replies with acceptance message.
When the coordinator receives majority of acceptances it broadcasts the commit message and waits for majority of acknowledgements. Upon receiving commit message a replica applies an update stored in PaxosState, thus it performs the insert of \emph{John} to \emph{users} table and acknowledges commit with a reply.

At any time during whole procedure the minority of replicas might stop responding and LWT can continue. In case of the example, one node can fail without aborting \emph{LWT}. If more than one node fails, then LWT is aborted. 

Secondly, at any time the leader might lose leadership if a other node does prepare with a higher ballot. In that case the coordinator has to wait, giving the other leader time to complete its \lwt, and try again with new ballot.

Note that \lwt affects only limited number of nodes -- replicas of the key, thus majority is considered only at the level of replicas, not at the level of the cluster.


%1. LWT is initiated by client request: insert John if not exists
%2. Request is routed to some node X that becomes coordinator of LWT
%3. Coordinator wants to be a leader so it creates a ballot and sends prepare to replicas: node1, node2, node3. This is prepare phase.
%4. Replicas promise to not accept values from leaders who have lower ballots.
%5. Coordinator gets 3 of out 3 promises. Quorum promised to listen.
%6. Coordinator performs read: SELECT * FROM users WHERE user_id = 1
%7. Coordinator checks condition “IF NOT EXISTS”. If such row did not exist in quorum of replicas it proceeds further
%8. Coordinator, as a leader, proposes new value which is an insert to users table.
%9. All three nodes reply with accept. Quorum accepted insert.
%10. At this point, insert is an in progress proposal that has been accepted by majority of replicas. Even if coordinator crashes, once someone selects user with id=1 or retries to insert it. This proposal will get committed. Let’s assume that coordinator is well and proceeds to commit.
%11. Coordinator broadcasts commit message
%12. Replicas apply insert locally and reply with acknowledge message
%13. Coordinator receives quorum of acknowledges. LWT has been successfully completed.
%14. Coordinator responds to client: John was inserted.


%Sequence above is a happy path. At any point coordinator might lose his leadership if some other node sends prepare with higher ballot. In that case coordinator waits a bit and retries whole process again. 
%Also any replica might stop responding, which is fine as long as quorum is up and well. If quorum responds to messages, coordinator can proceed. If there is no quorum then paxos conditions are not met and LWT cannot be continue. However during normal operation is it rather rare to see many nodes down so in general LWT can successfully finish with 2 out of 3 replicas running. Note that cluster size might be much bigger, for example hundred nodes, but since replication factor N=3, only three nodes are interesting from perspective of LWT and Paxos.


\subsection{Datomic}
% TODO podejście na jeszcze wyższym poziomie abstrakcji. 