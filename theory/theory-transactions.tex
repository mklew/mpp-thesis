%!TEX root = ../thesis.tex
\section{Transactions}\label{sec:theory:transactions}
Transactions in RDBMS are fully ACID compliant, whereas non-relational databases lack transaction support in most cases. 
In case where transactions are supported, NoSql databases provide only partial functionality offered by ACID compliant transactions \cite{StonebrakerSqlVsNosql}. 
Some of NoSql databases introduce new concepts, which offer transactions, but are specific to the database and its schema.


\subsection{Regular transactions}
Regular transactions, known from RDBMS, are ACID compliant \cite{ACID}. ACID is an acronym that stands for \emph{Atomicity} -- many operations, such as inserts, updates and removals, are performed either all together or none at all, \emph{Consistency} -- operations move data in the database from one consistent state to another state, which satisfies all data integrity constraints, \emph{Isolation} -- transactions do not interfere with each other during execution, \emph{Durability} -- changes made by a committed transaction are preserved even after the database shutdown.

\subsubsection{A transactional system}
A transactional system should provide following operations to run transactions:
% TODO żeby wymagania na operacje nie były pomiesznae. Napisać że wymagania które chce spełnić są takie: ... alg ma zapewniać ... 
% Napisać co jest charakterystyczne dla transakcji rozproszonych, wspomnieć o BASE
\begin{enumerate*}[label=\alph*)]
%\item isolation from other transactions -- transaction's state and data should be isolated from other transaction's state and data.
\item begin transaction -- transaction is begun, subsequent operations are part of the transaction. 
\item commit transaction -- as a result, transaction is either committed or rolled back.
\item rollback transaction -- rollback the entire transaction and its modification if needed
\item do an operation within a transaction -- any data manipulation operation on unlocked\footnote{not acquired by another transaction} data should be possible to execute in context of a transaction
\end{enumerate*}
% \item concurrency control -- 
% \item atomicity
% \item durability
% \item consistency 


The system should also provide \emph{concurrency control} -- if two transactions modify same piece of data and commit at same time, only one transaction can be committed, 
thus the other transaction has to be rolled back.


\subsubsection{The consensus}
In general terms, a database has to reach a \emph{consensus} on the state of the database, which means that it has to know which transaction can be committed, and which should be rolled back, as well what is data after operations.

Solving the problem of consensus is the core of distributed transactions. We present set of algorithms, which allow to reach consensus.

\subsection{Distributed transactions}

% TODO rozrysować co czym jest, Raft, Paxos -> distributed consensous algorithm
% RAMP atomic commit

This section presents the following algorithms: \emph{2PC}, \emph{3PC}, \paxos and \emph{Raft} to solve the consensus problem. \lwt (see Subsection \ref{sec:theory:transactions:lwt}) is an implementation of distributed transaction, which uses \paxos to provide distributed consensus. \emph{RAMP} (see Subsection \ref{sec:theory:transactions:ramp}) is an algorithm, which provides atomic commit.

\subsubsection{2 phase commit}
2PC \cite{2phaseC}  operates in two distinct phases:
\begin{enumerate}
\item Contact every node, propose a value and gather responses
\item If all nodes agree, contact every node again to let it know about agreement. Otherwise, contact every node to abort the consensus.
\end{enumerate}

A value is proposed by any node, which is then called the \emph{coordinator} who initiates round of 2PC.
Figure \ref{fig:2phaseCommit} depicts the algorithm during fault-free execution. Note that, in 2PC nodes cannot suggest other value than the proposed one, they can only accept it, or decline it. Another proposal can be done only through another round.

\paragraph{Fault tolerance} is not provided by 2PC. The 2PC solves the consensus problem in the absense of failures. If a failure occurs, even to a single node, then the coordinator will not proceed to phase 2, thus consensus will not be reached.

\paragraph{Latency} of the algorithm in case of $n$ nodes is $3n$, one round of messages for a proposal, second for votes and third for commit or abort.

\paragraph{Blocking} occurs, when during sending the proposal the coordinator fails, in which case some participants received the proposal, voted and then wait in phase 2 possibly blocking resources, whereas other participants did not receive the proposal, thus are unware of the round.


\input{theory/theory-2pc-diagram.tex}


\subsubsection{3 phase commit}\label{sec:theory:transactions:3pc}
3PC \cite{3phaseC}, depicted on Figure \ref{fig:3phaseCommit}, divides phase 2 of 2PC into two sub-phases: prepare to commit phase and commit phase, thus it has three phases in total. Upon the prepare to commit message nodes enter the state, in which they are able to commit the value, but also to abort and undo any changes made. The key principle behind this phase is to eliminate the possibility that any node actually commits the value before other nodes are aware of the decision to commit.

In case of the coordinator failure, a recovery node takes over the protocol and queries the state of the remaining nodes. If a node that committed the transaction failed along with the coordinator, then it is known that all nodes received prepare to commit message therefore the recovery node is able to finish the protocol by committing the value. If a node replies that it did not recive prepare to commit message, then it means that none of the nodes received commit message, thus the recovery node can re-run the protocol. 

\paragraph{Fault tolerance} is provided by 3PC in a way that the protocol can be run to its conclusion no matter which node fails. 3PC tolerates configurable number of $f$ failures, because the coordinator can move to phase 3 after it receives $f+1$ acknowledges of prepare to commit.

3PC does not work well during network partition, because if part of nodes on one side of the partition received prepare to commit message and all nodes on the other side did not receive that message, then recovery node on the former side commits transaction, whereas the other recovery node aborts it. After network partition is gone, nodes remain in inconsistent state, in which transaction is committed and aborted at the same time.

\paragraph{Latency} is higher compared to 2PC. 3PC requires $6n$ messages, thus twice, as 2PC. Higher latencies are cost of availability.

\paragraph{Blocking} does not occur, because recovery node can always run 3PC to its end and either commit or abort transaction.

2PC can be applied to systems, which value low latencies the most, whereas 3PC is more suitable for systems where availability is more important than latency.

\input{theory/theory-3pc-diagram.tex}

\subsubsection{Paxos}
\paxos is an algorithm to solve distributed consensus problem \cite{Lamport1998partTimeParliment} \cite{lamport2001paxosMadeSimple}, which was used, as the solution to the distributed consensus problem as part of the larger systems in \cite{chandra2007PaxosMadeLive}, \cite{lampson1996build}, and more recently in the Google's Chubby service \cite{burrows2006chubby}, which is a lock service intended to provide coarse-grained locking, as well as reliable storage. 

\paxos works within an asynchronous, non-Byzantine communications model, in which nodes can fail, but if they do not fail then they must operate according to \paxos protocol.

Actors in \paxos have one or more of the following roles: \begin{enumerate*}[label=\alph*)]
\item proposer - actor which proposes a new value,
\item acceptor - actor which is supposed to accept or decline a proposal,
\item learner - actor which learns accepted proposal.
\end{enumerate*} To follow naming from 2PC and 3PC let us assume that proposer is the same, as the coordinator.

\paxos round consists of $3$ phases: \begin{enumerate*}[label=\alph*)] \item prepare phase, \item propose phase, \item commit phase \end{enumerate*}.

\paragraph{Prepare phase}
A proposer has to establish leadership among majority of acceptors before sending a proposal. Every proposal consists of a value and a ballot \ballot, which is a increasing number that should be distinct for each proposal. Any proposer should be able to generate new \ballot.

Proposer sends proposal \ballot to acceptors, which are supposed to promise to listen to that proposer, but only if \ballot is the highest ballot acceptor received so far. Otherwise an acceptor declines to promise to a proposer, in which case that proposer can start over with next \ballot. 

Futhermore, acceptors reply with a value associated with the highest \ballot already accepted by them. Then proposer is bound to ask acceptors to only accept that value (which might be different from its original proposal) -- this forces proposer to propose other not yet committed proposals instead of his own.

\paragraph{Propose phase}
A proposer sends a proposal to acceptors and has to obtain majority of acceptances. If quorum of acceptors agree to the proposal then proposer can proceed to the commit phase. Otherwise proposer needs to start over. Acceptor must accept the proposal if that proposal is associated with promised \ballot.

\paragraph{Commit phase}
A proposer notifies learners about an accepted new value. Note that, actors\footnote{nodes} can play different roles, thus learners can be the same, as acceptors. \paxos round is finished when majority of learners acknowledge commit.

% Paxos defines requirements and constraints that the protocol must operate within . Some of them are: \begin{enumerate}
%   \item every proposal has a sequence number called ballot \ballot, that can be generated by any proposer. Moreover, \ballot should be distinct and increasing. 
%   \item proposer has to establish leadership among majority of acceptors before sending a proposal. Proposer sends \ballot to acceptors, which are supposed to promise to listen to that proposer, but only if \ballot is the highest ballot acceptor received so far.
%   \item acceptors reply with a value associated with the highest \ballot already accepted by them. Then proposer is bound to ask acceptors to only accept that value (which might be different from its original proposal) -- this forces proposer to propose other not completed proposals instead of his own,\label{sec:mpp:requirements:finishInProgress}
%   \item proposer can finish protocol only when majority of acceptors acknowledge commit.
% \end{enumerate}

% \paragraph{Example of Paxos round}

Figure \ref{fig:paxosBasic} and Figure \ref{fig:paxosBasicPart2} show complete \paxos round and exchanged messages between $2$ proposers and acceptors.
During that round, propopser $p_1$ starts its proposal, but it then overtaken by $p_2$, which finishes in-progress proposal of $p_1$.
 The proposer $p_1$ has to propose a new value $v_1$, therefore it begins next \paxos round represented by a higher ballot \ballot.
% In this case acceptors are also learners.
% Nodes which learn that new value are called \emph{acceptors}\footnote{nodes have 2 roles: acceptor, learner}

In prepare phase $p_1$ has to establish leadership between acceptors, therefore it sends prepare message with $\beta_1$ to acceptors $acc_1$, $acc_2$, $acc_3$. An acceptor promises to listen iff \ballot is higher than currently promised. In this case there are neither other ballots, nor proposals, thus all acceptors promise and $p_1$ gains leadership, which allows $p_1$ to send proposal.

When $p_1$ begins propose phase, second proposer $p_2$ begins its prepare phase, in which $p_2$ sends $\beta_2$ that is higher than $\beta_1$. Messages of two proposers are interleaved so that when $p_1$ sends proposals with value $v_1$, $p_2$ sends prepare messages. $acc_1$ receives proposal from $p_1$ and accepts it, after which it receives prepare message with $\beta_2$, to which $acc_1$ responds with already accepted proposal $\beta_1,v_1$.
$acc_2$ and $acc_3$ receive prepare from $p_2$ before proposals from $p_1$ arrive, in which case both acceptors promise to $p_2$ and decline proposal from $p_1$. $p_1$ failed to obtain majority of acceptances, therefore aborts the protocol. $p_1$ can start over with another ballot for the same proposal $v_1$. 

Figure \ref{fig:paxosBasicPart2} continues with the example, in which
 $p_2$ established leadership among majority of acceptors, thus proceeds to propose phase. $p_2$ received proposal value $v_1$, which is proposed instead of $v_2$, therefore $p_2$ sends proposals with $\beta_2,v_1$, which are accepted by all acceptors.

 In the next phase $p_2$ notifies learners about value $v_1$ by sending commit message, which is acknowledeged by each learner. Note that, in this example acceptors and learners are the same actors.

 Next action, not shown on Figures, would be $p_2$ performing another proposal with $\beta_3$ for its $v_2$, which could run uninterrupted. 

 \paragraph{Fault tolerance} A proposer proceeds to next phase, as long as majority requirement is met, thus \paxos tolerates failures of 
 $(\frac{n}{2}-1)$ acceptors and learners.

 \paragraph{Latency} let us assume that number of acceptors and learners is the same and equals $n$, thus number of messages exchanged is equal to $6n$, which is the same, as for 3PC.

 \paragraph{Blocking} \paxos does not block, other proposers are able to propose at any time, which might interrupt former proposer.


% In propose phase \coordinator sends a message including \ballot and the proposal, which is either one of already accepted proposals received from acceptors or \paxosValue if there are no other proposals pending.
% Acceptors check if their ballot is smaller than the one in the message and if it is then they reply with acceptance. If \coordinator received majority of acceptances then it can commit. If it received majority of rejections then it has to abandon the proposal and start again.

% In commit phase \coordinator broadcasts the commit message and waits for majority of acknowledges. If acknowledges do not arrive it has to start over.

% \paxos depends only on majorities within each phase, thus is resilient to node failures, therefore \paxos has advantage over \emph{3PC}.

\begin{figure}[H]
  \centering
  \begin{sequencediagram} 
  	\newthread[blue]{c1}{:Proposer 1} 
  	\newinst{n1}{:Acceptor 1}
  	\newinst{n2}{:Acceptor 2}
  	\newinst{n3}{:Acceptor 3}
  	\newthread{c2}{:Proposer 2}

	\mess[1]{c1}{prepare $b_1$}{n1}
	%\prelevel
	\mess[1]{c1}{prepare $b_1$}{n2}
	\mess[1]{n1}{promise}{c1}
	\prelevel
	\prelevel
	\prelevel
	\mess[1]{c1}{prepare $b_1$}{n3}
	\mess[1]{n2}{promise}{c1}
	\mess[1]{n3}{promise}{c1}

	\mess[1]{c2}{prepare $b_2$}{n3}
	\mess[1]{c2}{prepare $b_2$}{n2}

	\mess[1]{c1}{propose $b_1, v_1$}{n1}
	\mess[1]{c1}{propose $b_1, v_1$}{n2}
	\mess[1]{c1}{propose $b_1, v_1$}{n3}	
	\mess[1]{n3}{promise}{c2}
	\mess[1]{n2}{promise}{c2}

	\mess[1]{n1}{accept}{c1}
	\mess[1]{n2}{decline}{c1}
	\mess[1]{n3}{decline}{c1}
	\mess[1]{c2}{prepare $b_2$}{n1}	
	\mess[1]{n1}{promise $b_1, v_1$}{c2}
	
	
\end{sequencediagram}

	
  \caption{Paxos round with two proposers part 1}
  \label{fig:paxosBasic}
\end{figure}

\begin{figure}[H]
  \centering
  \begin{sequencediagram} 
  	\newthread[blue]{c1}{:Proposer 1} 
  	\newinst{n1}{:Acceptor 1}
  	\newinst{n2}{:Acceptor 2}
  	\newinst{n3}{:Acceptor 3}
  	\newthread{c2}{:Proposer 2}

	\mess[1]{c2}{propose $b_2, v_1$}{n3}
	\mess[1]{c2}{propose $b_2, v_1$}{n2}
	\mess[1]{c2}{propose $b_2, v_1$}{n1}

	\mess[1]{n3}{accept}{c2}
	\mess[1]{n2}{accept}{c2}
	\mess[1]{n1}{accept}{c2}

	\mess[1]{c2}{commit $b_2$}{n3}
	\prelevel
	\mess[2]{c2}{commit $b_2$}{n2}
	\prelevel
	\prelevel
	\mess[2]{c2}{commit $b_2$}{n1}
	
	\mess[1]{n3}{ack}{c2}
	\prelevel
	\mess[1]{n2}{ack}{c2}
	\prelevel
	\mess[1]{n1}{ack}{c2}
\end{sequencediagram}
	\caption{Paxos round with two proposers part 2}
  \label{fig:paxosBasicPart2}
\end{figure}

% These are requirements that need to be satisfied in order to be sure that \paxos can work the way it was designed and proven formally \cite{Lamport1998partTimeParliment}. 

%\paragraph{Paxos guarantees}
%\label{sec:theory:paxos:guarantees}
%TODO co gwarantuje Paxos, tak zeby opis Rafta do tego nawiazywal ale bez referencji, bo Raft jest zaraz po Paxosie.

% \begin{example}

% 	TODO przyklad Paxosa, jakos trzeba go wytlumaczyc

% \end{example}

% TODO opisać na tyle szczegółowo żeby móc je porównać. 
% bez dokładnego schematu
% ogólny schemat, najbardziej typowy przypadek opisać, bez dowodów, bez wyników testów.
% żeby było wiadomo o co chodzi.

% po to żeby móc porównać, Paxos jest lepszy od 3PC bo (ma partycje) i źródło "dokładnie jest to opisane tu i tam"
% ma wynikać z jakiego powodu jest lepszy


%  Paxos, Raft, 3PC opisane na jednym poziomie szczegółowości

% żeby do zrozumienia pracy nie trzeba było zaglądać gdzieś indziej.


\subsubsection{Raft}
% TODO napisać jakie to są guarantees
%\emph{Raft} algorithm offers the same guarantees, as \paxos. 
The aim of \emph{Raft} designers was to create a consensus algorithm which would be more understandable to its predecessor \cite{ongaro2014search}. 
The main idea behind the algorithm is decomposition of the consensus problem into the following individual tasks:
\begin{enumerate*}[label=\alph*)]
\item leader election,
\item log replication,
% \item safety,
\item membership changes.
\end{enumerate*} 
%Safety property is that: if any server
%has applied a particular log entry to its state machine,
%then no other server may apply a different command for the same log index.
 
 A server supporting \emph{Raft} is in one of three states: \begin{enumerate*}[label=\alph*)] \item leader, \item follower, \item candidate \end{enumerate*}. The leader can be only one and it handles all client requests.
  When a follower, which is a node listening to the leader, does not receive a heartbeat message within certain time it becomes the candidate and requests election votes from other nodes. If it receives the majority of votes it becomes the leader for the next \emph{term}. Client's request is handled by the leader, which saves the value in uncommitted state in its log and communicates the value to the followers, which acknowledge it. Upon majority of acknowledges the value is committed at the leader node, and the followers are notified about the commit on the next heartbeat message. The leader responds to the client, as soon as the value is committed just at the leader node. When a follower, receives a request from a client, it has to redirect request to the leader. \emph{Raft} algorithm is visualized in \cite{raftVisual}.

\paragraph{Fault tolerance} \emph{Raft} is fault tolerant, because the the leader has to acquire only majority of acknowledges, thus is resilient to faults of minority. However failure of the leader causes delay, because a new leader has to be elected before client's request is handled.

\paragraph{Latency} Assuming that the leader is already elected, a new value is learnt by followers in at least $3n$ messages. Election process is an additional overhead. Morever, heartbeat messages are exchanged, thus total number of messages exchanged can be greater than in previous algorithms.

\paragraph{Blocking} In principle \emph{Raft} does not block, because all client requests are handled by the same leader. However election of the leader can be delayed in case of \emph{split vote}, in which two candidates receive the same number of votes, thus neither of them obtained majority vote and election has to start over in the next term after certain timeout.


\subsubsection{RAMP}\label{sec:theory:transactions:ramp}
\emph{Read Atomic Multi Partition} transactions \cite{Bailis:2014} is the newest algorithm among the algorithms discussed in this chapter. 
\emph{RAMP} algorithm was designed to address the requirements of the following use cases: secondary indexing, foreign key
enforcement, and materialized view maintenance.
\emph{RAMP} identifies new transaction isolation level \emph{Read Atomic}, which 
assures consistent visibility of changes made by the transaction: either all or none of each transaction's updates are seen by other transactions.
%The transaction is able to detect non-atomic partial read and repair it with additional rounds of communication with nodes. 
The key principle behind \emph{RAMP} is its ability to detect non-atomic partial read, and to repair it with additional rounds of communication with the nodes by the means of \emph{the metadata} attached to each write and \emph{multi-versioning}, which means that the data is accessible in different versions and each modification to the data creates a new version. Versions are either committed or not yet committed, in which case the data can be discarded by aborting the transaction \cite[p. 6]{Bailis:2014}. 
There are three variants of \emph{RAMP} algorithm which differ in type of the metadata. \emph{RAMP-small} uses a 64 bit timestamp as the metadata and requires additional two rounds of communication when non-atomic read is detected. Other variants of \emph{RAMP} are described in \cite[p. 5]{Bailis:2014}.
\emph{RAMP} is linearly scalable, which was tested during trials with the number of nodes spanning up to 100 servers \cite[p. 10]{Bailis:2014}.

\paragraph{Fault tolerance} \emph{RAMP} is resilient to failures of part of the nodes in the cluster -- for the reason that RAMP's transaction does not contact nodes that are not affected by the transaction. 

\paragraph{Latency} depends on variant of \emph{RAMP}. In case of \emph{RAMP-small} latency is $2n$. Other variants allow to have latency $n$ in the race-free case and $2n$ in the event of race.

\paragraph{Blocking} \emph{RAMP} does not block, read transactions race write transactions, but such races are detected and handled by \emph{RAMP}.


% \emph{RAMP} also guarantees \emph{synchronization independence}, which means that transactions do not stall or fail other transactions, due to the fact that reads and writes do not block each other. 


%  TODO Może to gdzieś się opłaca dodać:
% \emph{RAMP} transactions \ref{sec:theory:transactions:ramp} have support for multi-partition operations. 
% However \emph{RAMP} relies on 64 bit timestamp \cite[p. 8]{Bailis:2014} which is not the current standard \cite{timestamp64wiki}, thus \emph{RAMP} cannot be used at the time being.
%

\subsubsection{LWT}\label{sec:theory:transactions:lwt}
% TODO źródło w (more on this)

As mentioned earlier, Light Weight Transactions is the name of compare-and-swap functionality implemented in Cassandra. Internally, LWT uses the Paxos algorithm with some modifications which were aimed to prevent potential single point of failure, thus improving high availability. SPOFs may surface in case the leader fails when it is about to propose a new value. The problem was addressed by sacrificing the standard leader election in favour of allowing any node to act as a leader, so that it can propose a value at any time. On the downside, such approach increases the risk of contention. 

One of the key requirements of Paxos protocol is storage of PaxosState - a data structure composed of the proposed value, proposed ballot, promised ballot and most recent commit. In case of LWT, the value represents the update itself along with the token of the partition key where that update should be applied. That token uniquely identifies PaxosState so that several proposals for the same partition can be associated with the same Paxos instance. 


LWT includes the following phases: \begin{enumerate*}[label=\alph*)]
\item prepare,
\item get promise,
\item read row,
\item get results,
\item propose a new value,
\item acceptance,
\item broadcast commit,
\item wait for acknowledgements.
\end{enumerate*} 

Let us analyze each of them in the environment with \N{5}, \RF{3} and request to insert user named \emph{John} to \emph{users} table, but only if \emph{John} does not exist. For simplicity, \emph{users} table has only two columns: \begin{enumerate*}[label=\alph*)]
\item name -- text column and primary key, \item email -- text column. \end{enumerate*} \lwt begins with a node 5 receiving request. Node 5 is then called coordinator node. The coordinator has to become the leader of paxos round identified by the token computed from partition key of \emph{users} table, thus from string value \emph{John}. To become the leader, the coordinator has to create the new ballot and send it along with the token and table name to replica nodes: node 1, node 2, node 3. Each replica has to check whether the ballot is the highest ballot it has seen, and if it is, then replica saves it in the PaxosState and responds with a promise. The leader has to wait for majority of promises before it proceeds. When majority promises, the coordinator may proceed to read the row and check the condition. The condition is that \emph{John} cannot exist, thus read has to not find any rows querying users by user name \emph{John}. Let us assume that indeed user \emph{John} did not exist, thus condition is met which allows the coordinator to propose an insert of \emph{John} with email \emph{john.doe@gmail.com} to \emph{users} table. The coordinator sends proposal message, with an insert and same ballot, to replicas and waits for acceptances. A replica saves proposed insert to PaxosState and replies with acceptance message.
When the coordinator receives majority of acceptances it broadcasts the commit message and waits for majority of acknowledgements. Upon receiving commit message a replica applies an update stored in PaxosState, thus it performs the insert of \emph{John} to \emph{users} table and acknowledges commit with a reply.

At any time during whole procedure the minority of replicas might stop responding and LWT can continue. In case of the example, one node can fail without aborting \emph{LWT}. If more than one node fails, then LWT is aborted. 

Secondly, at any time the leader might lose leadership if other node does prepare with a higher ballot. In that case the coordinator has to wait, giving the other leader time to complete its \lwt, and try again with new ballot.

Note that \lwt affects only limited number of nodes -- replicas of \emph{John}, thus majority is considered only at the level of replicas, not at the level of the cluster.

\lwt is limited to a single partition, thus \lwt cannot be used for atomic updates of many partitions, thereby this paper presents a new algorithm, which overcomes limitations of \lwt.
The proposed algorithm supports multi-partition operations, and provides read-committed isolation level, whereas \lwt provides serializable isolation.

\paragraph{Fault tolerance} is the same, as in \paxos.

\paragraph{Latency} is greater than in \paxos, single \lwt instance can run even two\footnote{second round does not require prepare phase} rounds of \paxos, in case there is in-progress proposal.

\paragraph{Blocking} does not happen, as instances of \lwt for different partitions are independent. However \lwt transactions for the same partition can stall each other the same way concurrent proposers interrupt each other in \paxos.

