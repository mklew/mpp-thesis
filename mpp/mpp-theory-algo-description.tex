%!TEX root = ../thesis.tex

\section{The algorithm's description}
\mpt algorithm depends on previously described \paxos concepts and guarantees they provide. 
This part of whole solution is responsible for committing transaction. 

Figure \ref{fig:seqCommitBasic} shows how a client \client starts the commit procedure by sending message \txCommitMessage and then node \node{i} becomes the executor of the transaction \transaction and sends a series of messages called \emph{transitions}, which transition nodes $\mathit{N}^'$ between \emph{phases}, which are steps of the algorithm. When all $\mathit{N}^'$ reach \emph{committed phase} or \emph{rolled back phase} then \node{i} replies to the client with message \txCommitResonseMessage. Transaction \transaction is committed by performing multi partition transactions \paxos round, 
in which transaction state \txState is accepted by majority in each replica group.

\input{mpp/mpt-seq-diagram-commit-basic.tex}

\subsection{Replica groups}
\label{sec:mpp:replicaGroups}
\lwt supports a single key $k$, thus it communicates with single replica group $\tau(k) = N^{\RFaloneInMath}_{k} \subset \mathit{N}$, whereas \mpt is designed for mutations of multiple keys, thus there are multiple replica groups 
$(\tau(k_1) \cup \tau(k_2) \cup ... \cup \tau(k_i) ) = (N^{\RFaloneInMath}_{k_1} \cup N^{\RFaloneInMath}_{k_2} \cup ... \cup N^{\RFaloneInMath}_{k_i} ) = \mathit{N^'} \subset \mathit{N}$, where $(N^{\RFaloneInMath}_{k_1} \cap N^{\RFaloneInMath}_{k_2} \cap ... \cap N^{\RFaloneInMath}_{k_i})$ can be non empty set, due to the fact that a node is a replica for different keys.

Assume we have transaction state $\text{\txState}(\text{\txItemi{1}, \txItemi{2}})$, \RF{3} and $N=5$. Let $\mathit{N^{\RFaloneInMath}_{k_1}} = \tau(\text{\txItemi{1}}), \mathit{N^{\RFaloneInMath}_{k_2}} = \tau(\text{\txItemi{2}})$, where $\mathit{N^{\RFaloneInMath}_{k_1}} = (n_1,n_2,n_3), \mathit{N^{\RFaloneInMath}_{k_2}} = (n_2,n_3,n_5)$ thus $\mathit{N}^' = (N^{\RFaloneInMath}_{k_1} \cup N^{\RFaloneInMath}_{k_2}) = (n_1, n_2, n_3, n_5)$.
In this example transaction has mutations on $4$ nodes and has
$2$ replica groups, which have nodes $n_2, n_3$ in common.  


\subsection{Phases}
The algorithm execution occurs in \emph{phases}, which are the following: 
\begin{enumerate*}[label=\alph*)]
\item idle phase (start phase),
\item setup phase,
\item repairng phase (optional),
\item prepared phase,
\item accepted phase,
\item committed phase (end phase),
\item rolled back phase (end phase).
\end{enumerate*}

Replicas advance in phase after successful \emph{transitions}. The algorithm is done when quorum of each replica group is in an end phase or when a timeout occurs, in which case commit procedure is aborted. Figure \ref{fig:phasesBasicPath} presents the transitions from idle phase to committed phase assuming successful transitions of each $n\in \text{\nodesTx}$. 

%Three phases: prepare, propose and commit, might be already recognizable, since they are parts of the \paxos algorithm. 

%One of phases, specific to \mpt, is a \emph{rollback phase}. Replicas can transition to rollback phase at anytime after setup phase. When replica is in such phase it means that transaction was rolled back by other concurrent transaction at that replica. 

\newcommand{\setupTransition}{$\mathit{setup\_transition()}$\xspace}
\newcommand{\prepareTransition}{$\mathit{prepare\_transition()}$\xspace}
\newcommand{\proposeTransition}{$\mathit{propose\_transition()}$\xspace}
\newcommand{\commitTransition}{$\mathit{commit\_transition()}$\xspace}
\newcommand{\repairingTransition}{$\mathit{reparing\_transition()}$\xspace}

\input{mpp/mpt-diagram-basic-transitions.tex}

\subsubsection{Transitions between phases}
Transitions are:
\begin{itemize}
\item \setupTransition presented on Figure \ref{fig:transitionToSetup}, which can transition to:
	\begin{itemize}
		\item setup phase, when node \node{i} fails
		\item committed phase, when transaction state \txState is recorded as committed in transaction log \txLog  
		\item rolled back phase, when transaction state \txState is recorded as rolled back in transaction log \txLog
		\item idle phase, when node \node{i} does not respond to the transition to setup message
	\end{itemize}
\item \prepareTransition shown on Figure \ref{fig:transitionToPrepare} which can transition to:
	\begin{itemize}
		\item committed phase, when transaction state \txState is recorded as committed in transaction log \txLog  
		\item rolled back phase, when transaction state \txState is recorded as rolled back in transaction log \txLog
		\item setup phase, when node \node{i} can not promise
		\item repair phase, when node \node{i} promises and replies with in-progress \txState
		\item prepare phase, when node \node{i} promises and does not have in-progress transaction state \txState 
	\end{itemize}
\item \repairingTransition presented on Figure \ref{fig:transitionRepairing}, which can transition to:
	\begin{itemize}
		\item repairing phase, when repair of in-progress transaction is not done,
		\item transition to prepared phase, when repair is done
		\item transition to setup phase, when repair is done, but node \node{i} did not receive promise
	\end{itemize}
\item \proposeTransition depicted on Figure \ref{fig:transitionToPropose}, which can transition to:
	\begin{itemize}
		\item accepted phase, when proposal is accepted by \node{i}
		\item rolled back phase, when node \node{i} refuses the proposal and notifies that transaction state \txState was rolled back
		\item setup phase, when node \node{i} refuses the proposal
	\end{itemize}
\item \commitTransition presented on Figure \ref{fig:transitionToCommitted}, which transitions to committed phase
\end{itemize}

\input{mpp/mpt-diagram-setup-transition.tex}

\input{mpp/mpt-diagram-prepare-transition.tex}

\input{mpp/mpt-diagram-propose-transition.tex}

\input{mpp/mpt-diagram-commit-transition.tex}

\input{mpp/mpt-diagram-repairing-transition.tex}

\input{mpp/mpt-diagram-all-transitions.tex}





%As a corollary, any operation done in \mpt must be done once per node that is affected by the transaction, thus once
 per $n1$, $n2$, $n3$, $n4$, $n5$, and $n7$ in order to not repeat operation on the same nodes. However the results of
 these operations must be evaluated in the context of the whole replica group.


\subsection{Moving forward in phases}
Replicas start in \emph{idle phase}, which is a starting phase for all replicas, and move to more advanced phases through phase transitions, until they reach an end phase, which is either committed or rolled back phase. Phase transition messages are sent to each $n\in \mathit{N}^'$.

Each replica is in one of phases and moves between phases, as a result of phase transitions, which can be either successful transitions or not, in which case replica goes back in phase. 
For example, \proposeTransition depicted on Figure \ref{fig:transitionToPropose}, can fail if replica refuses to accept a proposal, in which case replica goes back to setup phase and then has to transition again into prepare phase. Figure \ref{fig:allTransitions} presents outcomes of all transitions.

% The most basic optimistic phase transition sequence is: \begin{enumerate*}[label=\alph*)] \item idle phase, \item setup phase, \item prepared phase, \item proposed phase, \item committed phase, \end{enumerate*} after which the transaction is committed. Phase transition sequence, during normal operations, might be different due to concurrent, conflicting transactions and network issues.
%Its transitions however depend on results of transitions of other replicas and other replica groups.


Replica advances in phase only if conditions are met: \begin{description}
\item[successful transition] -- its transition is successful and
\item[the same advancement within replica group] -- each replica group to which that node belongs has quorum of nodes
with same advancement of phase and
\item[in sync with other replica groups] -- minimum phase among quorums in all replica groups must have the same
advancement, for example all replica groups must promise to the leader, before any proposal is sent to any replica, therefore all replica groups must be in \emph{prepared phase}.
\end{description}


In consequence, all replicas and all replica groups move in sync with each other. None of nodes will do a proposal if there is some replica group for which quorum is not in prepare phase. 
Similarly, none of nodes will do the commit if there exists at least one replica group which did not get acceptance of the proposal.  

Given contention with other transactions, it might happen that replica groups need to go back in phase and retry
operation in order to stay in synchronized phase with other replica groups, then all of them proceed in the same phase.

\section{Replica phases}
In this section we discuss details of each phase.

\subsection{Idle phase}
Idle phase is a technical representation of starting phase.

\subsection{Setup phase}
Setup phase provides a foundation for further phases and provides guarantees that if a replica is in the phase, then it is eligible to participate in a \paxos round and its private data are ready to be committed. Two main steps in this phase are: \begin{enumerate*}[label=\alph*)]
\item transaction \transaction registers in transaction index \txIndex on each $n\in \text{\nodesTx}$, in order to participate in a paxos round,
\item transaction \transaction performs mutations consistency check, in order for \node{i} to have full subset of mutations \mutations in its private transaction storage \txStorage.
\end{enumerate*}


\subsubsection{Registration in transaction index}
When transaction \transaction registers in the transaction index \txIndex (Subsection \ref{sec:mpp:txIndex}), from that moment on \transaction participates in a \paxos round.
It might participate alone or with other concurrent transactions that are known to be conflicting. 
\transaction is vulnerable to rollback by other transaction \transactionj, as soon as it has registered itself in the index.
Since \txIndex is accessed by multiple concurrent \transactions, it has to be exclusively acquired by \transaction before any operation that changes its state takes place. 
Locking, or other concurrency control mechanism can be used to guarantee that state of \txIndex stays consistent.


        
\subsubsection{Making data consistent}
Transaction’s data are kept in private transaction storage \txStorage (Subsection \ref{sec:mpp:privateTxStorage}).
Mutations messages are sent and received by quorum of a replica group, thus mutations \mutations exist in private transaction storage \txStorage at least on the quorum of nodes in a replica group. However, between a time transaction \transaction is started and a time it is committed, some nodes $N^{''} \subset \text{\nodesTx}$ could restart and lose mutations \mutations in their \txStorage. Moreover, quorum subsets chosen as target of mutation messages can differ between messages, thus different subsets of replica nodes store different subsets of \mutations, therefore we need the data check procedure, which verifies that all replica groups  $(N^{\RFaloneInMath}_{k_1} \cup N^{\RFaloneInMath}_{k_2} \cup ... \cup N^{\RFaloneInMath}_{k_i} ) = \mathit{N^'} \subset \mathit{N}$ have all required \mutationsFullEnd, thus quorum of $N^{\RFaloneInMath}_{k_1}$ has mutation \mutation{k_1}{v_1} in their \txStorage, quorum of $N^{\RFaloneInMath}_{k_2}$ has \mutation{k_2}{v_2} in their \txStorage up to quorum of replica group $N^{\RFaloneInMath}_{k_i}$, which has mutation \mutation{k_i}{v_i} in their \txStorage.

\subsubsection{Example of inconsistent private transaction storage \txStorage}
We would like to present an example which argues why consistency checking is inevitable.
Assume \RF{3}, \N{7}, transaction
$\text{\transaction}(\text{\mutation{k_1}{v_1}}, \text{\mutation{k_2}{v_2}}, \text{\mutation{k_3}{v_3}})$, its transaction state
$\text{\txState}(\text{\txItemi{k_1}}, \text{\txItemi{k_2}}, \text{\txItemi{k_3}})$, topology mapping to replica nodes
\topologyItem{k_1}{\text{\nodesTx}}, 
\topologyItem{k_2}{\text{\nodesTx}}, 
\topologyItem{k_3}{\mathit{N^{''}}}, 
where $\text{\nodesTx}=(n_1, n_2, n_3)$, 
$\mathit{N^{''}}=(n_5,n_6,n_7)$,
thus $k_1, k_2$ have the same replica group \nodesTx.

Assume that mutation \mutation{k_1}{v_1} was sent to $\mathit{quorum}(\text{\nodesTx})=(n_1, n_2)$, \mutation{k_2}{v_2} was sent to $\mathit{quorum}(\text{\nodesTx})=(n_2, n_3)$ and
\mutation{k_3}{v_3} was sent to $\mathit{quorum}(\mathit{N^{''}})=(n_6, n_7)$.
 After mutations were added to \txStorage of each $n$, a client \client sends message \txCommitMessage, but before the commit procedure begins, $n_2$ becomes unavailable, in which case \node{i} can proceed, as there are available quorums of each replica group 
 $\mathit{quorum}(\text{\nodesTx})=(n_1,n_3), \mathit{quorum}(\mathit{N^{''}})=(n_6,n_7)$, 
 but $n_1$ in its \txStorage stores \mutation{k_1}{v_1} and $n_3$ in its \txStorage stores \mutation{k_2}{v_2}. Therefore state of each of their \txStorage is inconsistent, because replicas do not store whole subset of mutations for that replica group: $\text{\mutation{k_1}{v_1}}, \text{\mutation{k_2}{v_2}}$.

We have to achieve data consistency on each node in order to move mutations from \txStorage to \database at each $n$ during the commit procedure. Figure \ref{fig:seqSetupTransition} presents \setupTransition and data consistency check procedure.

% To this end, the following algorithm was employed:

\input{mpp/mpt-seq-diagram-making-data-consistent.tex}

% TODO dodać diagram

% Inputs:
        % TransactionState - state of transaction, it is source of truth of all modifications that happened during transaction. Originally send by client, further forwarded by leader node to this replica node that has to run setup operation.











\subsection{Rollback phase}
Replica in a rollback phase, means that transaction was rolled back at the given replica node. 
This phase is special, because even if one replica group decides to move to rollback phase, then all the other replicas have to move to rollback phase. Secondly, there is no transition out of rollback phase. \mpt algorithm ends and yields result that transaction was rolled back.
% Any replica affected by transaction can transition to rollback  from the following phases: \begin{enumerate*}[label=\alph*)]
% \item prepare phase,
% \item propose phase,
% \item repairing phase.
% \end{enumerate*}

\transaction cannot be rolled back once its proposal was accepted by all quorums of all replica groups. \paxos guarantees that once proposal is accepted, current or other leader will finish proposal in-progress in that round and effectively transaction will be committed. Process of repairing in-progress transaction is described in repairing phase.

If replica has to move to rollback phase upon decision of all replica groups and not by its own, then it should notify the node about rollback, which then purges its private data from \txStorage, remove transaction state \txState from transaction index \txIndex and records rolled back transaction \transaction in transaction log \txLog.


\subsection{Prepare phase}
During prepare each replica receives a ballot, which is just a name for a timeuuid\footnote{which is universal unique identifier with encoded timestamp}. Replica needs to check Paxos State to see if ballot is the highest it has seen and only then replica can reply with promise. However, Paxos State is identified by \paxosRoundId which is stored in transaction index \txIndex. To find \paxos round id \paxosRoundId, we need to look up \txIndex using transaction state \txState. That requires that ballot is passed along with \txState, which is used only to find \paxosRoundId and to compare ballots.

In order to find \paxosRoundId using transaction state \txState, \txIndex is exclusively acquired and queried. Assumption is that transaction \transaction has successfully registered in \txIndex beforehand. Therefore if it is not found in \txIndex it can mean one of:
\begin{enumerate*}[label=\alph*)]
\item \transaction was rolled back by concurrent transaction \transactionj,
\item \transaction was committed by other leader which repaired in-progress transaction - this is the case, when replicas go back in phase but at least one of them has successfully proposed \txState,
\item this replica was down during setup phase and did not get the message. It could happen due to network partition or hardware failure.
\end{enumerate*}


In order to check which case it is transaction log \txLog can be queried. If \txState exists in \txLog, then it was either committed or rolled back. In that case replica can answer that it can not promise anything, but it knows that \transaction was committed or rolled back.

In other case, when \txState is neither in transaction index \txIndex nor in transaction log \txLog we can assume that this replica missed setup phase. In this situation replica cannot promise, because it doesn’t know what Paxos State is. Other replicas can continue without it. Replica might answer without a promise, but also that it doesn’t know what state of transaction is. Response looks exactly the same, as in the case when ballot is not the highest.
Figure \ref{fig:preparePayload} presents payload of $\mathit{prepare\_response}$.

\begin{figure}
\centering
\begin{tabular}{|p{3cm}|p{1.5cm}|p{7cm}|}
        \toprule
        name & type & description \\ \midrule
        $\mathit{promised}$ & boolean & whether this replica promises to listen to leader \\ 
        $\mathit{present\_in\_index}$ & boolean & if transaction state \txState was present in the transaction index \txIndex at this node \\ 
        $\mathit{log\_state}$ & $\mathit{log\_state}$ & value of $\mathit{find(\text{\txState})}$ from \txLog at this node \\ 
        \txStatei{in-progress} & \txState & if replica has already accepted other transaction state and this transaction interrupted its execution becoming new leader then the in-progress proposal - some other \txStatei{in-progress} - is returned to the leader. In that case leader will have to repair \mpt round. Replica will transition into repairing phase. \\ \bottomrule
      \end{tabular}     
     \caption{Payload of $\mathit{prepare\_response}$}
     \label{fig:preparePayload}
\end{figure}

Prepare phase extends original \paxos’s prepare phase, since it returns more information, but its purpose stays the same: replica has to compare ballots and promise to listen to a leader with the highest ballot. 
There might be in-progress proposal that has to be completed. Additional information allows to identify whether \transaction can continue or not. Responses from all replicas are considered according to moving in phase logic.
After successful transition of all replicas to prepare phase, next transition can be applied.


\subsection{Propose phase}
The purpose of propose phase is to propose transaction state \txState to replicas and to get majority of acceptances. In the propose phase, it is known that \txState existed in the transaction index \txIndex, therefore if during proposal the \txIndex does not know about \txState then it has to be recorded in the transaction log \txLog, as either committed or rolled back.

Proposal with \txState can be accepted if the proposer is still the leader and has the highest ballot. Proposal is saved in the Paxos State and replica can reply with $\mathit{M}(n_j,n_i,\mathit{propose\_response}(accepted, log))$ where $\mathit{accepted}=true$. If \node{i} lost his leadership then proposal cannot be accepted, which results in refusal response.

\subsection{Commit phase}
Commit phase can only happen when transaction state \txState was accepted by all quorums in all replica groups. Upon commit message replica performs:
\begin{enumerate}
\item commit mutations \mutations
\begin{enumerate}
   \item find transaction’s data in the private transaction storage \txStorage
   \item apply mutations to database \database  -- move \mutations from \txStorage into \database
   \end{enumerate}
\item record this \txState in transaction log \txLog as committed.
\item rollback other concurrent and conflicting transactions \conflictingTxSet
 \begin{enumerate}
   \item find \conflictingTxSet in the \txIndex
   \item rollback each conflicting \txStatei{i}
   \item record each conflicting \txStatei{i} in \txLog, as rolled back.
  \end{enumerate}
\item remove \txState from \txIndex
\item send acknowledge message 
\end{enumerate}

When the leader receives acknowledgments from all replica groups, it can safely finish \mpt round and respond to a client \client, that \transaction was committed.


\subsection{Repairing phase}
It might happen that during prepare leader receives in-progress proposal which means that it has interrupted some other leader which was executing his transaction. As required by \paxos algorithm, in-progress proposal has to be completed before proposer proposes his own value. When replica group sees that it has in-progress proposal, this replica group has to transition to repairing phase. 
Other replica groups are also affected because they cannot advance in phase until in-progress transaction is repaired. Repaired means that \mpt round is run on in-progress transaction until it finishes with commit or rollback. 
After repair main transaction might continue. If it turns out that repaired transaction was rolled back, first transaction has a chance to be committed. If however repaired transaction is committed, main transaction will see that it was rolled back and will stop.
%This means that another \mpt round is nested in main \mpt round.
%It is slightly different because it can start from more advanced phase.
%TODO przepisać do końca akapitu
%If other transaction managed to proposed itself it means that is was in prepared phase hence in setupd phase also. Therefore repairing in progress transaction can start with replicas in setup phase. First transition run on replicas is transition to prepare phase skipping registration in index and making transactions data consistent as it was already done by previous leader.


%\subsubsection{Moving in phases and repair phase}
%Since all replicas have to wait with further transitions until in-progress transactions are repaired for all replica groups, repair phase has to be less advanced than prepare phase, but more advanced than setup phase since without being in setup none of replicas would notice any in-progress transaction. This is an example where replicas have to move back in phase in order to stay in sync with all replicas executing transaction.


\subsection{The example of \mpt}
Assume we have 
\N{5}, \RF{3}, transaction
$\text{\transaction}(\text{\mutation{k_1}{v_1}, \mutation{k_2}{v_2}})$,  its transaction state
$\text{\txState}(\text{\txItemi{k_1}}, \text{\txItemi{k_2}})$, topology mappings
\topologyItem{k_1}{N^'_1}, 
\topologyItem{k_2}{N^'_2}, 
where $N^'_1=(n_1, n_2, n_3)$, 
$N^'_2=(n_3,n_4,n_5)$. Figure \ref{fig:mptExampleInitState} presents current state of the algorithm. Furthermore let us assume that \mutation{k_1}{v_1} is present in each \txStorage of each $n\in N^'_1$ and \mutation{k_2}{v_2} is present in each \txStorage of each $n\in N^'_2$ except $n_5$, thus \transaction is ready for the commit procedure, which we analyze step by step:

\begin{figure}
\centering
\begin{tabular}{c||c|c|c|c}
	\toprule
    Node  & phase & \txStorage & \txIndex & replica group  \\ \midrule
    $n_1$ & idle & \mutation{k_1}{v_1} & & $N^'_1$ \\
    $n_2$ & idle & \mutation{k_1}{v_1} & & $N^'_1$ \\
    $n_3$ & idle & \mutation{k_1}{v_1}, \mutation{k_2}{v_2} & & $N^'_1$, $N^'_2$ \\
    $n_4$ & idle & \mutation{k_2}{v_2} & & $N^'_2$ \\
    $n_5$ & idle &  & & $N^'_2$ \\ \bottomrule
  \end{tabular}     
 \caption{Initial state of nodes}
 \label{fig:mptExampleInitState}
\end{figure}


\begin{enumerate}
\item Nodes start at \emph{idle phase} and first transition has to move them to setup phase.

\item Run transition to setup phase. Assume all nodes do the setup operation successfully. \\
 After the transition, quorums in $N^'_1$, $N^'_2$ are in \emph{setup phase}. Minimum phase among replica groups is
 \emph{setup phase}, therefore all replica groups have the same advancement in phase and can continue. Each replica
 upon receiving a setup message does:
 \begin{enumerate}
 	\item execute the data consistency check procedure. $n_5$ detects that \txState has \txItemi{2} and reads data from
 	other replicas: $n_3, n_4$ which answer with \mutation{k_2}{v_2}. $n_5$ adds \mutation{k_2}{v_2} to \txStorage
 	\item registers \txState in \txIndex, since there are no concurrent transactions, a new \paxosRoundId is created for \transaction on each node. 	
 \end{enumerate}
 Figure \ref{fig:mptExampleSetupState} presents state of nodes after setup transition.


\begin{figure}
\centering
\begin{tabular}{c||c|c|c|c}
	\toprule
    Node  & phase & \txStorage & \txIndex & replica group  \\ \midrule
    $n_1$ & setup & \mutation{k_1}{v_1} & \paxosRoundIdi{1} & $N^'_1$ \\
    $n_2$ & setup & \mutation{k_1}{v_1} & \paxosRoundIdi{2} & $N^'_1$ \\
    $n_3$ & setup & \mutation{k_1}{v_1}, \mutation{k_2}{v_2} & \paxosRoundIdi{3} & $N^'_1$, $N^'_2$ \\
    $n_4$ & setup & \mutation{k_2}{v_2} & \paxosRoundIdi{4} & $N^'_2$ \\
    $n_5$ & setup & \mutation{k_2}{v_2} & \paxosRoundIdi{5} & $N^'_2$ \\ \bottomrule
  \end{tabular}     
 \caption{State of nodes after setup transition}
 \label{fig:mptExampleSetupState}
\end{figure}


\item Run transition to prepare phase. Let us assume successful transition, after which all replica groups are in prepared phase. \\
  After the transition, quorums in $N^'_1$, $N^'_2$ are in \emph{prepare phase}. Minimum phase among replica groups
  is \emph{prepare phase}, therefore all replica groups have the same advancement in phase and can continue. Each
  replica upon receiving a message does:

  \begin{enumerate}
  	\item finds Paxos state querying \txIndex using \txState from the message and checks whether the ballot is the higher than promised ballot, since it will be the first ballot in this round, all replicas promise successfully and register the ballot in the Paxos state.
  \end{enumerate}

\item Run transition to \emph{accepted phase}, let us assume successful transition. \\
	Each replica upon receving a propose message performs:

	\begin{enumerate}
		\item finds Paxos state using \txState,
		\item checks whether ballot from the message is the promised one,
		\item records the proposal from the message, which is the \txState in the Paxos state for \transaction, and replies with acceptance.
	\end{enumerate}

\item Run transition to \emph{commit phase}, in which each replica receives a commit message and performs:

	\begin{enumerate}
		\item finds the transaction's data in \txStorage. In case of $N^'_1$ it is \mutation{k_1}{v_1}, and in case of $N^'_2$,  \mutation{k_2}{v_2},
		\item apply mutations locally,
		\item performs rollback on other concurrent conflicting transactions present in \conflictingTxSet which are assigned to the same \paxosRoundIdi{i},
		\item record \txState in \txLog, as committed,
		\item reply to the message with acknowledgement.
	\end{enumerate}

\item The algorithm ends with the committed \transaction.

\end{enumerate}

%TODO wczesniej byl tutaj bardziej skomplikowany przyklad, ale wydaje mi sie, że jego opis nie wyjasni za wiele.
%Moze jako nastepny przyklad? Na pewno musze miec ten prosty bazowy przyklad zeby bylo wiadomo o co chodzi.

% 8. Next transition will do proposal in order to move to proposed phase, however in the meantime other transaction did prepare on node3 and node5 and received promise. Now things should get complicated. 
% 9. Node 1 gets proposal and accepts it. Replica 1 wants to transition to Proposed phase
% 10. Node 2 gets proposal and accepts it. Replica 2 wants to transition to Proposed phase
% 11. Node 3 receives proposal, but it promised to accept higher ballot therefore it responds with refusal. Replica 3 has failed to transition, it is no longer prepared, because it sees that proposal was refused. It knows that it is at most in setupd phase. 
% 12. Node 4 gets proposal and accepts it. Replica 4 wants to transition to Proposed phase
% 13. Node 5 does exactly same as Node 3.
% 14. Phases of replicas in RG1 were { node 1 -> prepared, node 2 -> prepared, node 3 -> prepared } and in RG2 { node 3 -> prepared, node 4 -> prepared, node 5 -> prepared }
%         After transition replica groups look as following:


% RG1 { node 1 -> proposed, node 2 -> proposed, node 3 -> setupd }
% RG2 { node 3 -> setupd, node 4 -> proposed, node 5 -> setupd }


% Quorum of replicas in RG1 is in proposed phase.
% Quorum of replicas in RG2 is in setupd phase.
% Minimum phase among replica groups is setup phase.
%         Therefore replicas cannot advance further than setupd phase. However node 1 for example has already advanced to proposed phase. It will be forced to go back in phase in order to stay in sync with all replica groups. It does not necessarily mean that it has repeat operation. Actual transition performed on replica depends on current phase for all replica groups and current phase of replica. Replica can just do nothing and wait till rest of replicas catch up in phase.
% 1. Phase of all replica groups is setupd phase.
% 2. Next transition has to transition replica from its phase to prepared phase.
% 3. Node 1, node 2, node 4 can do a no-operation and just wait.
% 4. Node 3, node 5 have to do actual prepare request with new high ballot
% 5. Phases after transitions are evaluated and algorithm continues.
        
% Above example shows how different \mpt is from LWT. Example shows how phase depends on all replica groups which depend on replicas and all the other replica groups. All of that is required to simply have replicas that stay in sync and where each replica group has satisfied quorum. That is requirement which comes from paxos and all these dependencies relations make it possible.
