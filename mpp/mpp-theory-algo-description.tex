%!TEX root = ../thesis.tex

\section{The algorithm's description}
Multi partition paxos algorithm depends on previously described concepts and guarantees they provide. This part of whole solution is responsible for committing transaction. Transaction is committed by performing multi partition paxos round, in which transaction is accepted by majority from all replica groups \ref{sec:mpp:replicaGroups}. 
% TODO wyjaśnić czym są replica groups.

\subsection{Phases}
The algorithm consists of \emph{phases}, which are: 
\begin{enumerate*}
\item no phase
\item pre prepare phase
\item repair phase
\item prepare phase
\item propose phase
\item commit phase
\item rollback phase
\end{enumerate*}
Following sections contain descriptions of each phase.



        High level description
Algorithm works in phases. Phases are:
1. Pre prepare phase
At each node:
   1. Transaction has to successfully register in Transaction Index
   2. Node has to make transaction’s data consistent and freeze it
1. Prepare phase
Send ballot and receive promise
1. Propose phase
Propose TransactionState, wait for acceptance
1. Commit phase
        Broadcast commit message. Each replica on commit has to
   1. Flush transaction’s data from Private Transaction Storage
   2. Rollback other round participants
        
It looks very like paxos implementation, but details are significantly different. 
Paxos works for single partition, therefore it communicates with replica group of size N. It can proceed to next phase after quorum successfully responds. 
MPP is designed for operations on many partitions therefore there are many replica groups. 
Furthermore, there is another phase not existing in paxos, which is named rollback phase. Replicas can transition to rollback phase at anytime after pre prepare phase. When replica is in such phase it simply means that transaction was rolled back by other concurrent transaction at that replica. 


\subsection{Replica groups in details}
\label{sec:mpp:replicaGroups}
Assume $N=3$ and transaction for keys: $k1$, $k2$, $k3$, $k4$.
Key $k1$ belongs to replica group $n1$, $n2$, $n3$.
Key $k2$ belongs to replica group $n2$, $n3$, $n4$.
Key $k3$ belongs to replica group $n1$, $n5$, $n7$.
Key $k4$ belongs to replica group $n1$, $n2$, $n3$.


Replica groups for a key\footnote{each transaction item} might have some or all nodes in common.
In this example, $k1$ and $k4$ belong to the same replica group. Keys $k2$, $k3$ however have one node in common with $k1$ and $k4$.

As a corollary, any operation done in \mpp must be done once per node that is affected by the transaction, thus once per $n1$, $n2$, $n3$, $n4$, $n5$, $n7$ in order to not repeat operation on same nodes, but the results of these operations have to be evaluated in context of the replica groups. 


\subsubsection{Moving forward in phases}
Each replica starts without any phase. In ideal scenario it has to transition from no phase to pre prepared phase, then to prepared phase followed by proposed phase finishing by committed phase. Its transitions however depend on results of transitions of other replicas and other replica groups.


Replica advances in phase only if conditions are met: \begin{enumerate*}
\item its operation is successful, and
\item each replica group to which that node belongs has quorum of nodes with same advancement of phase, and
\item minimum phase among quorums in all replica groups has same advancement.
\end{enumerate*}


In consequence, all replicas and all replica groups move in sync with each other. None of nodes will do a proposal if there is some replica group for which quorum is not in prepare phase. Similarly, none of nodes will do commit if there exists at least one replica group which didn’t get acceptance of proposal.


Given contention with other transactions, it is possible that replica groups have to go back in phase in order to stay with other replica groups. Either all of them proceed in same phase or none.


\subsubsection{Example}
For example, let us consider two replica groups within a cluster of $5$ nodes.
Replica group 1 ($RG1$) has nodes: $node1$, $node2$ and $node3$. 
Replica group 2 ($RG2$) has nodes $node3$, $node4$ and $node5$.

\begin{enumerate}
\item Nodes start at no phase, and first transition has to move them to pre prepared phase.
\item Run transition to pre prepared phase. Assume all nodes do the pre prepare operation successfully.
\item Quorums in $RG1$, $RG2$ are in \emph{pre prepare phase}.
\item Minimum phase among replica groups is \emph{pre prepare phase}, therefore all replica groups have same advancement in phase.
\item Run transition to prepare phase. Let us assume successful transition, after which all replica groups are in prepared phase.
\item a
\end{enumerate}

8. Next transition will do proposal in order to move to proposed phase, however in the meantime other transaction did prepare on node3 and node5 and received promise. Now things should get complicated. 
9. Node 1 gets proposal and accepts it. Replica 1 wants to transition to Proposed phase
10. Node 2 gets proposal and accepts it. Replica 2 wants to transition to Proposed phase
11. Node 3 receives proposal, but it promised to accept higher ballot therefore it responds with refusal. Replica 3 has failed to transition, it is no longer prepared, because it sees that proposal was refused. It knows that it is at most in pre prepared phase. 
12. Node 4 gets proposal and accepts it. Replica 4 wants to transition to Proposed phase
13. Node 5 does exactly same as Node 3.
14. Phases of replicas in RG1 were { node 1 -> prepared, node 2 -> prepared, node 3 -> prepared } and in RG2 { node 3 -> prepared, node 4 -> prepared, node 5 -> prepared }
        After transition replica groups look as following:


RG1 { node 1 -> proposed, node 2 -> proposed, node 3 -> pre prepared }
RG2 { node 3 -> pre prepared, node 4 -> proposed, node 5 -> pre prepared }


Quorum of replicas in RG1 is in proposed phase.
Quorum of replicas in RG2 is in pre prepared phase.
Minimum phase among replica groups is pre prepare phase.
        Therefore replicas cannot advance further than pre prepared phase. However node 1 for example has already advanced to proposed phase. It will be forced to go back in phase in order to stay in sync with all replica groups. It does not necessarily mean that it has repeat operation. Actual transition performed on replica depends on current phase for all replica groups and current phase of replica. Replica can just do nothing and wait till rest of replicas catch up in phase.
1. Phase of all replica groups is pre prepared phase.
2. Next transition has to transition replica from its phase to prepared phase.
3. Node 1, node 2, node 4 can do a no-operation and just wait.
4. Node 3, node 5 have to do actual prepare request with new high ballot
5. Phases after transitions are evaluated and algorithm continues.
        
Above example shows how different multi partition paxos is from LWT. Example shows how phase depends on all replica groups which depend on replicas and all the other replica groups. All of that is required to simply have replicas that stay in sync and where each replica group has satisfied quorum. That is requirement which comes from paxos and all these dependencies relations make it possible.


\subsubsection{Algorithm requirements on cluster}
Algorithm can make progress as long as quorum in each replica group is satisfied, thus the algorithm is resilient to failures of n nodes:

% TODO tableka, 
% best case, N, wniosek
% worst case, N, wniosek

        N-denotes number of nodes that can be down[g]
* At best case, when replica groups are disjoint, n = (number of replica groups) * (N - (N/2 + 1)), that is, the minority of replicas can be down for each replica group.
* At worst cas[h]e, when all replica groups intersect, only minority of replicas can be down therefore n= N - (N/2 + 1)



Therefore resilience scales with size of transaction.


\section{Replica phases}
TODO co to jest, po co, lista tych phases a potem konkretny opis.
        Detailed description of phases
In this section discussion will point to details of each phase.


\subsection{Pre prepare phase}
Pre prepare phase is the first phase in multi partition paxos. There two main components in that phase:
* Transaction registers in transaction index 
* Transaction makes it data consistent


\subsubsection{Registering in transaction index}
Transaction Index alone was described in previous section, but here we put more light on it in context of multi partition paxos algorithm. When transaction registers in index, from that moment on transaction participates in paxos round. It might participate alone or with other concurrent transactions that are known to be conflicting. Therefore transaction is vulnerable to rollback by other transaction as soon as it has registered itself in the index.
        Since Transaction Index is accessed by many concurrent transactions, it has to be exclusively acquired by transaction before any operation that changes it state takes place. Locking, or other concurrency control mechanism can be used to guarantee that state of Transaction Index stays consistent.
        Paxos round id used to find Paxos State data at each node locally is synthetic id and is different at each replica during multi partition paxos. I put forward the claim that difference in id between replicas does not have negative effect as long as conflicting transactions are put into same round. If, for some reason, we wanted to use same paxos round id it would require another consensus algorithm of its own which would inherently increase cost of whole algorithm. It is suitable to avoid as many round-trip requests as possible.
        
\subsubsection{Making data consistent}
Transaction’s data is kept in Private Transaction Storage. Private writes by transaction use quorum semantics. I would like to present an example which argues why consistency checking is inevitable.


Example of inconsistent private data
For sake of argument, assume N=3 and two modifications of different keys which point to same replica set.


1. Modification of K1 is sent to replicas.
2. Node 1 has received it
3. Node 2 has received it[i]
4. Node 3 has not received it due to network faults
5. Modification of K1 is successfully written to quorum of replicas.
6. Modification of K2 is sent to replicas.
7. Node 1 has not received it due to network faults
8. Node 2 has received it
9. Node 3 has received it
10. Modification of K2 is again successfully written.


But what is the state of replica nodes?
Node 1 -> { K1 }
Node 2 -> { K1, K2 }
Node 3 -> { K2 }


Only single node has two modifications which illustrates that we don’t have quorum of replica nodes in consistent state. However paxos phases depend on quorum semantics, concretely that quorum of nodes within replica group such as this one is able to proceed and whole algorithm can make progress. 


On these grounds, we can argue that making data consistent at each replica is a mandatory step.
Algorithm to making private transaction data consistent looks as follows:


Inputs:
        TransactionState - state of transaction, it is source of truth of all modifications that happened during transaction. Originally send by client, further forwarded by leader node to this replica node that has to run pre prepare operation.


        Make data consistent algorithm:        
1. Filter transaction items for which this replica is responsible
   1. transaction item has a token
   2. This node knows its token range
   3. Node can identify which transaction item should have reflection in data being present in private transaction storage
1. Find other replicas for these items
   1. Each transaction item has data replicated at other nodes including this one
   2. Topology of nodes on token ring allows to precisely identify them
1. Group transaction items by same replica sets -- it can reduce number of round trips
2. For each replica set, read data from their private transaction storage for transaction items
3. Quorum of responses is required for each replica set -- we know for a fact that private write had to happen to at least quorum of nodes
4. Merge received modifications with those at private transaction storage
5. Freeze transaction’s data so it will not allow any further modifications.


\subsubsection{Optimization on number of requests}
% TODO 'unikac seems'. Przepisac to jakos
We could not limit number of requests to only those transaction items which are missing in Private Transaction Storage. Note that, items have only tokens whereas private data has full information about keys and modifications for these keys. If we check whether we have key in private storage which matches with token then we could omit further requests to other replicas. 
Such optimization could work only under the condition that transaction cannot modify same keys twice. 


For example, given row with key K and columns C1 and C2 we could have updates:
1. Update C1 for K
2. Update C2 for K
3. Update C1 and C2 for K


Each case is valid as long as others do not happen. If we allow 1) and later on 2) we won’t have any guarantees if we have data for both updates. If one update is lost on the way, replica won’t know whether it has all the updates associated with given key or not. This is because it has only one token and one key. 
Without such restriction, replica would think it[k] has consistent data, but in reality is has missing second update.
% TODO Replika nie mysli, unikac 
To summarize, optimizations are possible, but only with further constraints. I argue that decision whether such constraints are acceptable or not should be made by users given their use case. For sake of this research, proposed algorithm does not do such optimizations. Note however, such optimization could remain opt-in and configurable even per transaction. Which would yield best performance for specific cases without restricting other kinds of transactions.
        
To conclude pre prepare phase, it lays out foundation for further paxos phases and makes guarantees that if replica is in pre prepare phase, then it is eligible to participate in paxos round and its private data is eligible for commit. Whether commit occurs or not, it depends on results of next phases.


\subsection{Rollback phase}
Rollback phase was briefly mentioned at high level overview of multi partition paxos algorithm. 
Rollback phase for replica, means that transaction was rolled back at given replica node. 
This phase is special, because if even one replica group decides to move to rollback phase, then all the other replicas have to move to rollback phase. Secondly, there is no transition out of rollback phase. Multi partition paxos algorithm ends and yields result that transaction has been rolled back.
Any replica affected by transaction can transition to rollback phase just after pre prepare phase, but only from following phases:


* Prepare phase
* Propose phase
* Repairing phase


Transaction cannot be rolled back once its proposal was accepted by all replica groups. Paxos guarantees that once proposal has been accepted, current or other leader will finish in progress proposal in that round and effectively transaction will be committed. Process of repairing in progress transaction is described in repairing phase.


If replica has to move to rollback phase upon decision of all replica groups and not by its own. It should notify node about rollback. Node will then purge its private data, remove transaction from index and add information about rolled back transaction to transaction log.


\subsection{Prepare phase}
Prepare phase is the expected next phase after pre prepare phase. During prepare each replica receives ballot, which is just a name for timestamp. Replica needs to check Paxos State to see if ballot is the highest it has seen and only then replica can reply with promise. However, Paxos State is identified by id which is stored in Transaction Index. To find that id, we need to look up index using Transaction State. That requires that ballot is passed along with transaction state, but this state cannot be treated as paxos value. That’s not how paxos algorithm was designed. Transaction State is used only to find id of Paxos State and to compare ballots.


In order to find Paxos Id using Transaction State, Transaction Index is exclusively acquired and queried. Assumption is that transaction has successfully registered in index beforehand. Therefore if it is not found in index it can mean one of:
* Transaction was rolled back by concurrent transaction
* Transaction was committed by other leader which repaired in progress transaction - this is the case, when replicas go back in phase but at least one of them has successfully proposed transaction state.
* This replica was down during pre prepare phase and did not get the message. It could happen due to network partition or hardware failure.


In order to check which case it is, Transaction Log can be queried. Transaction Log knows which transactions have committed and which were rolled back. If transaction id exists in transaction log, then it was either committed or rolled back.


In that case replica can answer that it does not promise anything, but it knows that transaction has committed or rolled back.


In other case, when transaction is neither in index nor in transaction log we can assume that this replica missed pre prephase phase. In this situation replica cannot promise value, because it doesn’t know what Paxos State is. Other replicas can continue without it. Replica might answer without a promise, but also that it doesn’t know what state of transaction is. Response looks exactly the same as in the case when ballot is not the highest. That means, that if we want to know what the situation really is, we need to return more information.


Answer from prepare request has information:
* Boolean promised - whether this replica promises to listen to leader
* Boolean present in index - if transaction was present in the index at this node
* Committed/Rolled back/Unknown - enum value representing status of transaction. It is either known from Transaction Log and then it is one of committed or rolled back or it is unknown
But also it has information as in original paxos
* In progress value - if replica has already accepted other transaction state and this transaction interrupted its execution becoming new leader then the in progress proposal - some other transaction state - is returned to the leader. In that case leader will have to repair multi partition paxos round. Replica will transition into repairing phase.


To summarize prepare phase, it extends original paxos’s prepare phase, because it returns more information. Its purpose stays the same. Replica has to compare ballots and promise to listen to a leader with highest ballot. There might be in progress proposal that has to be completed. Additional information allows to identify whether transaction can continue or not. Responses from all replicas are considered according to moving in phase logic.
After successful transition of all replicas to prepare phase, next transition can be applied.


\subsection{Propose phase}
Propose phase is the next expected phase from prepare phase, but also from repairing phase. Its purpose is to propose transaction state as paxos value to replicas and get majority of acceptances. 


Similarly to prepare phase it is slightly extended. Since previous phase is prepare phase and it was successful, hence transition to proposal, transaction had to exist in Transaction Index. If now, during proposal,  transaction index does not know about transaction then it has to be present in Transaction Log. It has to be either Committed or Rolled back. 


Proposal phase is quite simple. Proposal with Transaction State can be accepted if leader still is the leader and has highest ballot. Proposal is saved in the Paxos State and replica can reply with success. Otherwise leader could have lost his leadership and value cannot be accepted which results in refusal response.


When transaction cannot be found in index, it has to be present in Transaction Log which also helps the leader to make decision whether algorithm should continue.


\subsection{Commit phase}
Commit phase comes after successful propose phase. It can only happen when transaction was accepted by all quorums in all replica groups. Upon commit message replica has to do couple of things:
* Flush transaction’s data locally
   * Find transaction’s data in Private Transaction Storage 
   * Assert that it is consistent hence frozen
   * Apply modifications locally on database -- move modifications from private storage into targeted tables
* Add this transaction to Transaction Log marked as Committed.[l]
* Rollback other concurrent and conflicting transactions
   * Look up other participants in Transaction Index
   * Rollback each conflicting transaction
   * Add conflicting transaction to Transaction Log marked as Rolled back.[m]
* Remove itself from Transaction Index
* Respond with acknowledge


When leader sees that all replica groups received acknowledges it can safely finish multi partition paxos round and respond to a client that transaction has been committed.


\subsection{Repairing phase}
It might happen that during prepare leader receives in progress proposal. It means that it has interrupted some other leader which was executing his transaction. As required by paxos algorithm, in progress proposal has to be completed before first transaction can continue. When replica group sees that it has in progress proposal, this replica group has to transition to repairing phase. 
Other replica groups are also affected because they cannot advance in phase until in progress transaction is repaired. Repaired means that multi partition paxos round is run on in progress transaction until it finishes with commit or rollback. Only after repair first transaction might continue. If it turns out that repaired transaction was rolled back, first transaction has a chance to be committed. If however repaired transaction is committed, main transaction will see that it was rolled back and will also stop.
This means that another multi partition paxos is nested in main multi partition paxos. It is slightly different because it can start from more advanced phase.
If other transaction managed to proposed itself it means that is was in prepared phase hence in pre prepared phase also. Therefore repairing in progress transaction can start with replicas in pre prepare phase. First transition run on replicas is transition to prepare phase skipping registration in index and making transactions data consistent as it was already done by previous leader.


\subsubsection{Moving in phases and repair phase}
Since all replicas have to wait with further transitions until in progress transactions are repaired for all replica groups, repair phase has to be less advanced than prepare phase, but more advanced than pre prepare phase since without being in pre prepare none of replicas would notice any in progress transaction. This is an example where replicas have to move back in phase in order to stay in sync with all replicas executing transaction.




\section{Theoretical number of requests during algorithm}
There can be many modifications to different keys resulting in many transaction items, but what is important are the number of replicas. At worst case each key will reside on different replica set. Most likely case is that some replica groups have replicas in common.


Assuming worst case scenario we have number of replica groups: RGC (Replica Group count)
And replication factor N. For sake of argument let’s assume that all modifications share same replication factor although it is not required by algorithm.


Total number of replicas: TR = N * RGC


Pre prepare phase requires reads from other replicas to make data consistent. Number of requests to make data consistent depends on number of replica groups to which keys owned by given node are replicated. So we have:
keys per node: KPN
And replica groups which own data for these keys: RGOC
We know that RGOC < RGC and at worst case it will be just less than 1 so RGOC = RGC - 1


To make data consistent each node has to make at least quorum of RGOC, but since each node belongs to all the other replica groups, number of requests can be further reduced to QUORUM-1 which equals to (((RGC -1 ) / 2 + 1) - 1 = (RGC -1) / 2
Since each nodes has to do it pre prepare phase requires TR * (RGC -1) / 2 of requests. Plus each replica has to receive message in order to do pre prepare phase therefore pre prephase phase looking at whole algorithm requires TR + TR * ((RGC -1) / 2) 


Prepare phase, propose phase and commit phase require TR requests each.


To sum it up, total number of requests at worst case equals to 
TR ((RGC-1) / 2) + 4*TR


For a five nodes cluster with replication factor N=3 it has
10 different replica sets.




\subsection{Key points regarding phases} 
-- TODO summary is better instead, chyba trzeba to przeniesc po prostu do Phases


\subsection{Example of multi partition paxos}
tutaj diagram musi jakis byc




Summary of algorithm
MPPaxos algorithm supports multi partition transactions which are serializable. Serializability has been achieved by means of distributed consensus provided by extended paxos algorithm and other components required for isolation and conflict resolution. Extended paxos proceeds only when quorum of each replica group agree on phase of algorithm. At its core it is still paxos, but with higher number of constraints which guarantee atomicity and serializability of whole transaction spanning many partitions.