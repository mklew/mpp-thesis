%!TEX root = ../thesis.tex

\section{The algorithm's description}
\mpt algorithm depends on previously described \paxos concepts and guarantees they provide. 
This part of whole solution is responsible for committing \transaction. 

\client starts commit procedure by sending \txCommitMessage and then \node{i} becomes the executor of the \transaction and sends a series of messages called \emph{transitions}, which transition $\mathit{N}^'$ between \emph{phases}, which are steps of the algorithm. When all $\mathit{N}^'$ reach \emph{committed phase} or \emph{rolled back phase} \node{i} replies to a client with \txCommitResonseMessage.  \transaction is committed by performing multi partition transactions \paxos round, 
in which \txState is accepted by majority in each replica group.

\subsection{Replica groups}
\label{sec:mpp:replicaGroups}
\lwt supports single $k$, thus it communicates with single replica group $\tau(k) = N^{RF}_{k} \in \mathit{N}$, whereas \mpt is designed for mutations of multiple keys, thus there are multiple replica groups 
$(\tau(k_1) \cup \tau(k_2) \cup ... \cup \tau(k_i) ) = (N^{RF}_{k_1} \cup N^{RF}_{k_2} \cup ... \cup N^{RF}_{k_i} ) = \mathit{N^'} \in \mathit{N}$, where $(N^{RF}_{k_1} \cap N^{RF}_{k_2} \cap ... \cap N^{RF}_{k_i})$ can be non empty set, due to the fact that a node is a replica for different keys.

Assume we have $\text{\txState}(\text{\txItemi{1}, \txItemi{2}})$, \RF{3} and $N=5$. Let $\mathit{N^{RF}_{k_1}} = \tau(\text{\txItemi{1}}), \mathit{N^{RF}_{k_2}} = \tau(\text{\txItemi{2}})$, where $\mathit{N^{RF}_{k_1}} = (n_1,n_2,n_3), \mathit{N^{RF}_{k_2}} = (n_2,n_3,n_5)$ thus $\mathit{N}^' = (N^{RF}_{k_1} \cup N^{RF}_{k_2}) = (n_1, n_2, n_3, n_5)$.
In this example transaction has mutations on $4$ nodes and has
$2$ replica groups, which have nodes $n_2, n_3$ in common.  


\subsection{Phases}
The algorithm execution occurs in \emph{phases}, which are the following: 
\begin{enumerate*}
\item idle phase (start phase),
\item setup phase,
\item repairng phase (optional),
\item prepared phase,
\item accepted phase,
\item committed phase (end phase),
\item rolled back phase (end phase).
\end{enumerate*}

Replicas advance in phase after successful \emph{transitions}. The algorithm is done when quorum of each replica group is in an end phase or when a timeout occurs, in which case commit procedure is aborted. Figure \ref{fig:phasesBasicPath} presents the transitions from idle phase to committed phase assuming successful transitions of each $n\in \text{\nodesTx}$.

%Three phases: prepare, propose and commit, might be already recognizable, since they are parts of the \paxos algorithm. 

%One of phases, specific to \mpt, is a \emph{rollback phase}. Replicas can transition to rollback phase at anytime after setup phase. When replica is in such phase it means that transaction was rolled back by other concurrent transaction at that replica. 

\newcommand{\setupTransition}{$\mathit{setup\_transition()}$\xspace}
\newcommand{\prepareTransition}{$\mathit{prepare\_transition()}$\xspace}
\newcommand{\proposeTransition}{$\mathit{propose\_transition()}$\xspace}
\newcommand{\commitTransition}{$\mathit{commit\_transition()}$\xspace}
\newcommand{\repairingTransition}{$\mathit{reparing\_transition()}$\xspace}

\input{mpp/mpt-diagram-basic-transitions.tex}

\subsubsection{Transitions between phases}
Transitions are:
\begin{itemize}
\item \setupTransition presented on Figure \ref{fig:transitionToSetup}, which can transition to:
	\begin{itemize}
		\item setup phase, when \node{i} fails
		\item committed phase, when \txState is recorded as committed in \txLog  
		\item rolled back phase, when \txState is recorded as rolled back in \txLog
		\item idle phase, when \node{i} does not respond to the transition to setup message
	\end{itemize}
\item \prepareTransition shown on Figure \ref{fig:transitionToPrepare} which can transition to:
	\begin{itemize}
		\item committed phase, when \txState is recorded as committed in \txLog  
		\item rolled back phase, when \txState is recorded as rolled back in \txLog\item setup phase, when \node{i} can not promise
		\item repair phase, when \node{i} promises and replies with in-progress \txState
		\item prepare phase, when \node{i} promises and does not have in-progress \txState 
	\end{itemize}
\item \repairingTransition presented on Figure \ref{fig:transitionRepairing}, which can transition to:
	\begin{itemize}
		\item repairing phase, when repair of in-progress transaction is not done,
		\item transition to prepared phase, when repair is done
		\item transition to setup phase, when repair is done, but \node{i} did not receive promise
	\end{itemize}
\item \proposeTransition depicted on Figure \ref{fig:transitionToPropose}, which can transition to:
	\begin{itemize}
		\item accepted phase, when proposal is accepted by \node{i}
		\item rolled back phase, when \node{i} refuses the proposal and notifies that \txState was rolled back
		\item setup phase, when \node{i} refuses the proposal
	\end{itemize}
\item \commitTransition presented on Figure \ref{fig:transitionToCommitted}, which transitions to committed phase
\end{itemize}

\input{mpp/mpt-diagram-setup-transition.tex}

\input{mpp/mpt-diagram-prepare-transition.tex}

\input{mpp/mpt-diagram-propose-transition.tex}

\input{mpp/mpt-diagram-commit-transition.tex}

\input{mpp/mpt-diagram-repairing-transition.tex}

%         High level description
% Algorithm works in phases. Phases are:
% 1. Pre prepare phase
% At each node:
%    1. Transaction has to successfully register in Transaction Index
%    2. Node has to make transaction’s data consistent and freeze it
% 1. Prepare phase
% Send ballot and receive promise
% 1. Propose phase
% Propose TransactionState, wait for acceptance
% 1. Commit phase
%         Broadcast commit message. Each replica on commit has to
%    1. Flush transaction’s data from Private Transaction Storage
%    2. Rollback other round participants




%As a corollary, any operation done in \mpt must be done once per node that is affected by the transaction, thus once per $n1$, $n2$, $n3$, $n4$, $n5$, and $n7$ in order to not repeat operation on same nodes. However the results of these operations must be evaluated in the context of the whole replica group. 


\subsection{Moving forward in phases}
Phase transition messages are sent to each $n\in \mathit{N}^'$.

Replicas start in \emph{idle phase}, which is a starting phase for all replicas, and move to more advanced phases through phase transitions, which are operations that include node to node communication, until they reach an end phase, which is either committed or rolled back phase.

The most basic optimistic phase transition sequence is: \begin{enumerate*} \item idle phase, \item setup phase, \item prepared phase, \item proposed phase, \item committed phase, \end{enumerate*} after which the transaction is committed. Phase transition sequence, during normal operations, might be different due to concurrent, conflicting transactions and network issues.
%Its transitions however depend on results of transitions of other replicas and other replica groups.


Replica advances in phase only if conditions are met: \begin{description}
\item[successful transition] -- its transition operation is successful, and
\item[same advancement within replica group] -- each replica group to which that node belongs has quorum of nodes with same advancement of phase, and
\item[in sync with other replica groups] -- minimum phase among quorums in all replica groups must have same advancement, for example all replica groups must promise to the leader, before any proposal is sent to any replica, therefore all replica groups must be in \emph{prepared phase}.
\end{description}


In consequence, all replicas and all replica groups move in sync with each other. None of nodes will do a proposal if there is some replica group for which quorum is not in prepare phase. 
Similarly, none of nodes will do the commit if there exists at least one replica group which did not get acceptance of the proposal.  

Given contention with other transactions, it might happen that replica groups need to go back in phase and retry operation in order to stay in synchronized phase with other replica groups, then all of them proceed in same phase.

\section{Replica phases}
In this section we discuss details of each phase.

\subsection{Idle phase}
Idle phase is a technical representation of starting phase.

\subsection{Setup phase}
Setup phase provides a foundation for further phases and provides guarantees that if a replica is in the phase, then it is eligible to participate in a \paxos round and its private data are ready to be committed. Two main steps in this phase are: \begin{enumerate*}
\item transaction registers in a transaction index, in order to participate in a paxos round,
\item transaction performs mutations consistency check, in order for \node{i} to have full subset of \mutations.
\end{enumerate*}


\subsubsection{Registration in transaction index}
When transaction registers in the \txIndex (Subsection \ref{sec:mpp:txIndex}), from that moment on transaction participates in a \paxos round.
It might participate alone or with other concurrent transactions that are known to be conflicting. 
Transaction is vulnerable to rollback by other transaction, as soon as it has registered itself in the index.
Since \txIndex is accessed by multiple concurrent transactions, it has to be exclusively acquired by transaction before any operation that changes its state takes place. 
Locking, or other concurrency control mechanism can be used to guarantee that state of \txIndex stays consistent.


        
\subsubsection{Making data consistent}
Transaction’s data is kept in Private Transaction Storage \ref{sec:mpp:privateTxStorage}. Private writes by transaction use quorum semantics. We would like to present an example which argues why consistency checking is inevitable.


Example of inconsistent private data
For sake of argument, assume \RF{3} and two modifications of different keys which point to same replica set.


1. Modification of K1 is sent to replicas.
2. Node 1 has received it
3. Node 2 has received it[i]
4. Node 3 has not received it due to network faults
5. Modification of K1 is successfully written to quorum of replicas.
6. Modification of K2 is sent to replicas.
7. Node 1 has not received it due to network faults
8. Node 2 has received it
9. Node 3 has received it
10. Modification of K2 is again successfully written.


But what is the state of replica nodes?
Node 1 -> { K1 }
Node 2 -> { K1, K2 }
Node 3 -> { K2 }


Only \node{2} has two modifications which means that we don’t have quorum of replica nodes in consistent state. \paxos phases depend on quorum semantics, which means that quorum of nodes within replica group, such as this one is able to proceed and whole algorithm makes progress. 
We have to achieve data consistency on each node in order to move mutations from private transaction storage to main storage at each node during the commit procedure. To this end, the following algorithm was employed:

TODO dodać diagram

Inputs:
        TransactionState - state of transaction, it is source of truth of all modifications that happened during transaction. Originally send by client, further forwarded by leader node to this replica node that has to run setup operation.


        Make data consistent algorithm:        
1. Filter transaction items for which this replica is responsible
   1. transaction item has a token
   2. This node knows its token range
   3. Node can identify which transaction item should have reflection in data being present in private transaction storage
1. Find other replicas for these items
   1. Each transaction item has data replicated at other nodes including this one
   2. Topology of nodes on token ring allows to precisely identify them
1. Group transaction items by same replica sets -- it can reduce number of round trips
2. For each replica set, read data from their private transaction storage for transaction items
3. Quorum of responses is required for each replica set -- we know for a fact that private write had to happen to at least quorum of nodes
4. Merge received modifications with those at private transaction storage
5. Freeze transaction’s data so it will not allow any further modifications.


\subsubsection{Optimization on number of requests}
TODO może wywalić cały podrozdział.
% TODO 'unikac seems'. Przepisac to jakos
We could not limit number of requests to only those transaction items which are missing in Private Transaction Storage. Note that, those items include only tokens whereas private data has full information about keys and modifications for these keys. If we check whether we have key in private storage which matches token we can omit further requests to other replicas. 
Such optimization can work only under the condition that transaction cannot modify same keys twice. 


For example, given row with key K and columns C1 and C2 we could have updates:
1. Update C1 for K
2. Update C2 for K
3. Update C1 and C2 for K


Each case is valid as long as others do not happen. If we allow 1. and later on 2. we will not have any guarantees to have data for both updates. If one update is lost on the way, replica won’t know whether it has all the updates associated with given key or not. This is because it has only one token and one key. 
Without such restriction, replica would think it[k] has consistent data, but in reality is has missing second update.
% TODO Replika nie mysli, unikac 

 We argue that decision whether such constraints are acceptable or not should be made by users given their use case. For sake of this research, proposed algorithm does not do such optimizations. Note however, such optimization remain opt-in and configurable even per transaction, which could yield best performance for specific cases without restricting other kinds of transactions.
       

\subsection{Rollback phase}
Replica in a rollback phase, means that transaction was rolled back at the given replica node. 
This phase is special, because even if one replica group decides to move to rollback phase, then all the other replicas have to move to rollback phase. Secondly, there is no transition out of rollback phase. \mpt algorithm ends and yields result that transaction was rolled back.
Any replica affected by transaction can transition to rollback  from the following phases: \begin{enumerate*}
\item prepare phase,
\item propose phase,
\item repairing phase.
\end{enumerate*}

Transaction cannot be rolled back once its proposal was accepted by all replica groups. Paxos guarantees that once proposal has been accepted, current or other leader will finish proposal in progress in that round and effectively transaction will be committed. Process of repairing in progress transaction is described in repairing phase.

If replica has to move to rollback phase upon decision of all replica groups and not by its own, then it should notify the node about rollback, which then purges its private data, remove transaction from the index and adds information about rolled back transaction to the transaction log.


\subsection{Prepare phase}
During prepare each replica receives a ballot, which is just a name for a timeuuid\footnote{which is universal unique identifier with encoded timestamp}. Replica needs to check Paxos State to see if ballot is the highest it has seen and only then replica can reply with promise. However, Paxos State is identified by id which is stored in Transaction Index. To find that id, we need to look up index using Transaction State. That requires that ballot is passed along with transaction state, but this state cannot be treated as paxos value. Transaction State is used only to find id of Paxos State and to compare ballots.

In order to find \paxosRoundId using \txState, \txIndex is exclusively acquired and queried. Assumption is that transaction has successfully registered in index beforehand. Therefore if it is not found in index it can mean one of:
\begin{enumerate*}
\item transaction was rolled back by concurrent transaction,
\item transaction was committed by other leader which repaired in progress transaction - this is the case, when replicas go back in phase but at least one of them has successfully proposed transaction state,
\item this replica was down during setup phase and did not get the message. It could happen due to network partition or hardware failure.
\end{enumerate*}


In order to check which case it is, Transaction Log can be queried. Transaction Log knows which transactions have committed and which were rolled back. If transaction id exists in transaction log, then it was either committed or rolled back.


In that case replica can answer that it can not promise anything, but it knows that transaction was committed or rolled back.


In other case, when transaction is neither in index nor in transaction log we can assume that this replica missed pre prephase phase. In this situation replica cannot promise value, because it doesn’t know what Paxos State is. Other replicas can continue without it. Replica might answer without a promise, but also that it doesn’t know what state of transaction is. Response looks exactly the same as in the case when ballot is not the highest. That means, that if we want to know what the situation really is, we need to return more information.


Answer from prepare request has the following information:
TODO przerobić na tabele
\begin{itemize}
\item Boolean promised -- whether this replica promises to listen to leader
\item Boolean present in index -- if transaction was present in the index at this node
\item Committed/Rolled back/Unknown -- enum value representing status of transaction. It is either known from Transaction Log and then it is one of committed or rolled back or it is unknown
But also it has information as in original paxos
\item In progress value -- if replica has already accepted other transaction state and this transaction interrupted its execution becoming new leader then the in progress proposal - some other transaction state - is returned to the leader. In that case leader will have to repair \mpt round. Replica will transition into repairing phase.
\end{itemize}


Prepare phase extends original paxos’s prepare phase, since it returns more information, but its purpose stays the same: replica has to compare ballots and promise to listen to a leader with the highest ballot. 
There might be in progress proposal that has to be completed. Additional information allows to identify whether transaction can continue or not. Responses from all replicas are considered according to moving in phase logic.
After successful transition of all replicas to prepare phase, next transition can be applied.


\subsection{Propose phase}
Propose phase is the next expected phase after prepare phase, but after repairing phase. Its purpose is to propose transaction state as paxos value to replicas and to get majority of acceptances. 

In the propose phase, it is known that transaction existed in the Transaction Index, therefore if during proposal,  transaction index does not know about transaction then it has to be recorded in the Transaction Log, as either committed or rolled back transaction.

Proposal with \txState can be accepted if the proposer is still the leader and has the highest ballot. Proposal is saved in the Paxos State and replica can reply with success. Otherwise leader could have lost his leadership and value cannot be accepted which would result in refusal response.

%When transaction cannot be found in index, it has to be present in Transaction Log which also helps the leader to make decision whether algorithm should continue.

\subsection{Commit phase}
Commit phase can only happen when \txState was accepted by all quorums in all replica groups. Upon commit message replica performs:
\begin{enumerate}
\item flush of transaction’s data locally
\begin{enumerate}
   \item find transaction’s data in the Private Transaction Storage
   \item apply mutations to \database -- move \mutations from private transaction storage into database
   \end{enumerate}
\item record this transaction in \txLog as committed.
\item rollback other concurrent and conflicting transactions
 \begin{enumerate}
   \item Look up other participants in the Transaction Index
   \item Rollback each conflicting transaction
   \item Add conflicting transaction to the Transaction Log marked as rolled back.
  \end{enumerate}
\item remove transaction from \txIndex
\item respond with acknowledge message
\end{enumerate}

When the leader receives acknowledgments from all replica groups, it can safely finish \mpt round and respond to the client, that transaction was committed.


\subsection{Repairing phase}
It might happen that during prepare leader receives in-progress proposal which means that it has interrupted some other leader which was executing his transaction. As required by \paxos algorithm, in-progress proposal has to be completed before proposer proposes his own value. When replica group sees that it has in-progress proposal, this replica group has to transition to repairing phase. 
Other replica groups are also affected because they cannot advance in phase until in-progress transaction is repaired. Repaired means that \mpt round is run on in-progress transaction until it finishes with commit or rollback. 
After repair first transaction might continue. If it turns out that repaired transaction was rolled back, first transaction has a chance to be committed. If however repaired transaction is committed, main transaction will see that it was rolled back and will stop.
This means that another \mpt round is nested in main \mpt round. It is slightly different because it can start from more advanced phase.
TODO przepisać do końca akapitu
If other transaction managed to proposed itself it means that is was in prepared phase hence in setupd phase also. Therefore repairing in progress transaction can start with replicas in setup phase. First transition run on replicas is transition to prepare phase skipping registration in index and making transactions data consistent as it was already done by previous leader.


\subsubsection{Moving in phases and repair phase}
Since all replicas have to wait with further transitions until in-progress transactions are repaired for all replica groups, repair phase has to be less advanced than prepare phase, but more advanced than setup phase since without being in setup none of replicas would notice any in-progress transaction. This is an example where replicas have to move back in phase in order to stay in sync with all replicas executing transaction.


\subsection{The example of \mpt}
TODO dodać rysunki pokazujace co jest gdzie w danym momencie
TODO zapisać formalnie dane
Let us consider a cluster of $5$ nodes, and a transaction $t$, which modifies two keys: $k_{1}, k_{2}$ with updates: $u_{1}, u_{2}$. Assume that key $k_{1}$ is replicated by replica group $RG1$, which consists of nodes: $n1$, $n2$ and $n3$, and key $k_{2}$ is replicated by $RG2$, which consists of nodes: $n3, n4, n5$.
Let us assume that updates are already existing in the private transaction storage at each node, and transaction is ready for the commit process, which we analyze step by step:



\begin{enumerate}
\item Nodes start at \emph{idle phase}, and first transition has to move them to setupd phase.

\item Run transition to setupd phase. Assume all nodes do the setup operation successfully. \\
 After the transition, quorums in $RG1$, $RG2$ are in \emph{setup phase}. Minimum phase among replica groups is \emph{setup phase}, therefore all replica groups have same advancement in phase and can continue. Each replica upon receiving a message does:
 \begin{enumerate}
 	\item registers transaction $t$ in the Transaction Index, since there are no concurrent transactions, a new paxos round is created for transaction,
 	\item execute data consistency procedure, for example $n2$ responsible for $k_{1}$ has to read from other replicas of the key: $n1, n3$ and then merge the data, and save it in the private transaction storage.
 \end{enumerate}

\item Run transition to prepare phase. Let us assume successful transition, after which all replica groups are in prepared phase. \\
  After the transition, quorums in $RG1$, $RG2$ are in \emph{prepare phase}. Minimum phase among replica groups is \emph{prepare phase}, therefore all replica groups have same advancement in phase and can continue. Each replica upon receiving a message does:

  \begin{enumerate}
  	\item finds paxos state querying transaction index using transaction state from the message and checks whether the ballot is the higher than promised ballot, since it will be the first ballot in this round, all replicas promise successfully and register the ballot in the paxos state.
  \end{enumerate}

\item Run transition to \emph{propose phase}, let us assume successful transition. \\
	Each replica upon receing a propose message performs:

	\begin{enumerate}
		\item finds paxos state using transaction state,
		\item checks whether ballot from the message is the promised one,
		\item records the proposal from the message, which is the transaction state in the paxos state for transaction $t$, and replies with acceptance.
	\end{enumerate}

\item Run transition to \emph{commit phase}, in which each replica receives a commit message and performs:

	\begin{enumerate}
		\item finds the transaction's data in private transaction storage. In case of $RG1$ it is $k_{1} -> u_{1}$, and in case of $k_{2} -> u_{2}$,
		\item applies the modifications locally,
		\item performs rollback on other concurrent conflicting transactions present in the Transaction Index which are assigned to same paxos round,
		\item adds transaction $t$ to the Transaction Log as committed,
		\item replies to the message with acknowledgement.
	\end{enumerate}

\item The algorithm ends with the committed transaction $t$.

\end{enumerate}

%TODO wczesniej byl tutaj bardziej skomplikowany przyklad, ale wydaje mi sie, że jego opis nie wyjasni za wiele.
%Moze jako nastepny przyklad? Na pewno musze miec ten prosty bazowy przyklad zeby bylo wiadomo o co chodzi.

% 8. Next transition will do proposal in order to move to proposed phase, however in the meantime other transaction did prepare on node3 and node5 and received promise. Now things should get complicated. 
% 9. Node 1 gets proposal and accepts it. Replica 1 wants to transition to Proposed phase
% 10. Node 2 gets proposal and accepts it. Replica 2 wants to transition to Proposed phase
% 11. Node 3 receives proposal, but it promised to accept higher ballot therefore it responds with refusal. Replica 3 has failed to transition, it is no longer prepared, because it sees that proposal was refused. It knows that it is at most in setupd phase. 
% 12. Node 4 gets proposal and accepts it. Replica 4 wants to transition to Proposed phase
% 13. Node 5 does exactly same as Node 3.
% 14. Phases of replicas in RG1 were { node 1 -> prepared, node 2 -> prepared, node 3 -> prepared } and in RG2 { node 3 -> prepared, node 4 -> prepared, node 5 -> prepared }
%         After transition replica groups look as following:


% RG1 { node 1 -> proposed, node 2 -> proposed, node 3 -> setupd }
% RG2 { node 3 -> setupd, node 4 -> proposed, node 5 -> setupd }


% Quorum of replicas in RG1 is in proposed phase.
% Quorum of replicas in RG2 is in setupd phase.
% Minimum phase among replica groups is setup phase.
%         Therefore replicas cannot advance further than setupd phase. However node 1 for example has already advanced to proposed phase. It will be forced to go back in phase in order to stay in sync with all replica groups. It does not necessarily mean that it has repeat operation. Actual transition performed on replica depends on current phase for all replica groups and current phase of replica. Replica can just do nothing and wait till rest of replicas catch up in phase.
% 1. Phase of all replica groups is setupd phase.
% 2. Next transition has to transition replica from its phase to prepared phase.
% 3. Node 1, node 2, node 4 can do a no-operation and just wait.
% 4. Node 3, node 5 have to do actual prepare request with new high ballot
% 5. Phases after transitions are evaluated and algorithm continues.
        
% Above example shows how different \mpt is from LWT. Example shows how phase depends on all replica groups which depend on replicas and all the other replica groups. All of that is required to simply have replicas that stay in sync and where each replica group has satisfied quorum. That is requirement which comes from paxos and all these dependencies relations make it possible.
