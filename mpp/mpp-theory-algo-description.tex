%!TEX root = ../thesis.tex

\section{The algorithm's description}
\mpp algorithm depends on previously described concepts and guarantees they provide. 
This part of whole solution is responsible for committing transaction. 
A transaction is committed by performing multi partition paxos round, 
in which transaction is accepted by majority in each replica group, 
which is a set of nodes that replicates the key $k$, for all replica groups \ref{sec:mpp:replicaGroups}. 

\subsection{Phases}
The algorithm execution occurs in \emph{phases}, which are the following: 
\begin{enumerate*}
\item no phase,
\item pre prepare phase,
\item repair phase,
\item prepare phase,
\item propose phase,
\item commit phase,
\item rollback phase.
\end{enumerate*}

Three phases: prepare, propose and commit, might be already recognizable, since they are parts of the \paxos algorithm. Similar phases could be identified for \lwt, since it also relies on \paxos.

One of phases, specific to \mpp, is a \emph{rollback phase}. Replicas can transition to rollback phase at anytime after pre prepare phase. When replica is in such phase it means that transaction was rolled back by other concurrent transaction at that replica. 

%         High level description
% Algorithm works in phases. Phases are:
% 1. Pre prepare phase
% At each node:
%    1. Transaction has to successfully register in Transaction Index
%    2. Node has to make transaction’s data consistent and freeze it
% 1. Prepare phase
% Send ballot and receive promise
% 1. Propose phase
% Propose TransactionState, wait for acceptance
% 1. Commit phase
%         Broadcast commit message. Each replica on commit has to
%    1. Flush transaction’s data from Private Transaction Storage
%    2. Rollback other round participants



\subsection{Replica groups}
\label{sec:mpp:replicaGroups}
\mpp is designed for operations on many partitions, therefore there are many replica groups. \lwt supports single key, thus it communicates with single replica group, whereas \mpp supports many keys, therefore it communicates with many replica groups, which can overlap, due to the fact that a node is a replica for many different keys. 

Let us consider an example of transaction affecting different replica groups. Let us assume $N=3$ and transaction for keys: $k1$, $k2$, $k3$, $k4$.
Key $k1$ belongs to replica group $n1$, $n2$, $n3$.
Key $k2$ belongs to replica group $n2$, $n3$, $n4$.
Key $k3$ belongs to replica group $n1$, $n5$, $n7$.
Key $k4$ belongs to replica group $n1$, $n2$, $n3$.

In this example, $k1$ and $k4$ belong to the same replica group. Keys $k2$, $k3$ however have one node in common with $k1$ and $k4$.

As a corollary, any operation done in \mpp must be done once per node that is affected by the transaction, thus once per $n1$, $n2$, $n3$, $n4$, $n5$, and $n7$ in order to not repeat operation on same nodes. However the results of these operations must be evaluated in the context of the whole replica group. 


\subsection{Moving forward in phases}
Replicas start in \emph{no phase}, which is a starting phase for all replicas, and move to more advanced phases through phase transitions, which are operations that include node to node communication, until they reach an end phase, which is either committed or rolled back phase.

The most basic optimistic phase transition sequence is: \begin{enumerate*} \item no phase, \item pre prepared phase, \item prepared phase, \item proposed phase, \item committed phase, \end{enumerate*} after which the transaction is committed. Phase transition sequence, during normal operations, might be different due to concurrent, conflicting transactions and network issues.
%Its transitions however depend on results of transitions of other replicas and other replica groups.


Replica advances in phase only if conditions are met: \begin{description}
\item[successful transition] -- its transition operation is successful, and
\item[same advancement within replica group] -- each replica group to which that node belongs has quorum of nodes with same advancement of phase, and
\item[in sync with other replica groups] -- minimum phase among quorums in all replica groups must have same advancement, for example all replica groups must promise to the leader, before any proposal is sent to any replica, therefore all replica groups must be in \emph{prepared phase}.
\end{description}


In consequence, all replicas and all replica groups move in sync with each other. None of nodes will do a proposal if there is some replica group for which quorum is not in prepare phase. 
Similarly, none of nodes will do the commit if there exists at least one replica group which did not get acceptance of the proposal.  

Given contention with other transactions, it might happen that replica groups need to go back in phase and retry operation in order to stay in synchronized phase with other replica groups, then all of them proceed in same phase.


\subsection{Algorithm requirements on cluster}
Algorithm can make progress as long as quorum in each replica group is satisfied, thus the algorithm is resilient to failures of n nodes:

% TODO tableka, 
% best case, N, wniosek
% worst case, N, wniosek

        N-denotes number of nodes that can be down[g]
* At best case, when replica groups are disjoint, n = (number of replica groups) * (N - (N/2 + 1)), that is, the minority of replicas can be down for each replica group.
* At worst cas[h]e, when all replica groups intersect, only minority of replicas can be down therefore n= N - (N/2 + 1)



Therefore resilience scales with size of transaction.


\section{Replica phases}
In this section discussion points to details of each phase.

\subsection{No phase}
No phase is a technical representation of starting phase.

\subsection{Pre prepare phase}
Pre prepare phase provides a foundation for further phases and makes guarantees that if a replica is in pre prepare phase, then it is eligible to participate in paxos round, and its private data is eligible for commit. Whether commit occurs or not depends on results of the next phases. Two main components in the phase are: \begin{enumerate*}
\item transaction registers in transaction index, in order to participate in a paxos round,
\item transaction performs data consistency check, in order to have local data ready for commit.
\end{enumerate*}


\subsubsection{Registering in transaction index}
When transaction registers in the transaction index \ref{sec:mpp:txIndex}, from that moment on transaction participates in a paxos round.
It might participate alone or with other concurrent transactions that are known to be conflicting. 
Transaction is vulnerable to rollback by other transaction, as soon as it has registered itself in the index.
Since Transaction Index is accessed by many concurrent transactions, it has to be exclusively acquired by transaction before any operation that changes it state takes place. 
Locking, or other concurrency control mechanism can be used to guarantee that state of Transaction Index stays consistent.


\emph{Paxos round id}, used to find Paxos State data at each node locally, is synthetic id and is different at each replica during multi partition paxos. 
I put forward the claim that difference in id between replicas does not have negative effect, as long as conflicting transactions are put into same round. 
Providing same paxos round id would require another consensus algorithm of its own which would inherently increase cost of the whole algorithm, thus it is suitable to avoid as many round-trip requests as possible.
        
\subsubsection{Making data consistent}
Transaction’s data is kept in Private Transaction Storage. Private writes by transaction use quorum semantics. I would like to present an example which argues why consistency checking is inevitable.


Example of inconsistent private data
For sake of argument, assume N=3 and two modifications of different keys which point to same replica set.


1. Modification of K1 is sent to replicas.
2. Node 1 has received it
3. Node 2 has received it[i]
4. Node 3 has not received it due to network faults
5. Modification of K1 is successfully written to quorum of replicas.
6. Modification of K2 is sent to replicas.
7. Node 1 has not received it due to network faults
8. Node 2 has received it
9. Node 3 has received it
10. Modification of K2 is again successfully written.


But what is the state of replica nodes?
Node 1 -> { K1 }
Node 2 -> { K1, K2 }
Node 3 -> { K2 }


Only single node has two modifications which illustrates that we don’t have quorum of replica nodes in consistent state. However paxos phases depend on quorum semantics, concretely that quorum of nodes within replica group such as this one is able to proceed and whole algorithm can make progress. 


On these grounds, we can argue that making data consistent at each replica is a mandatory step.
Algorithm to making private transaction data consistent looks as follows:


Inputs:
        TransactionState - state of transaction, it is source of truth of all modifications that happened during transaction. Originally send by client, further forwarded by leader node to this replica node that has to run pre prepare operation.


        Make data consistent algorithm:        
1. Filter transaction items for which this replica is responsible
   1. transaction item has a token
   2. This node knows its token range
   3. Node can identify which transaction item should have reflection in data being present in private transaction storage
1. Find other replicas for these items
   1. Each transaction item has data replicated at other nodes including this one
   2. Topology of nodes on token ring allows to precisely identify them
1. Group transaction items by same replica sets -- it can reduce number of round trips
2. For each replica set, read data from their private transaction storage for transaction items
3. Quorum of responses is required for each replica set -- we know for a fact that private write had to happen to at least quorum of nodes
4. Merge received modifications with those at private transaction storage
5. Freeze transaction’s data so it will not allow any further modifications.


\subsubsection{Optimization on number of requests}
% TODO 'unikac seems'. Przepisac to jakos
We could not limit number of requests to only those transaction items which are missing in Private Transaction Storage. Note that, items have only tokens whereas private data has full information about keys and modifications for these keys. If we check whether we have key in private storage which matches with token then we could omit further requests to other replicas. 
Such optimization could work only under the condition that transaction cannot modify same keys twice. 


For example, given row with key K and columns C1 and C2 we could have updates:
1. Update C1 for K
2. Update C2 for K
3. Update C1 and C2 for K


Each case is valid as long as others do not happen. If we allow 1) and later on 2) we won’t have any guarantees if we have data for both updates. If one update is lost on the way, replica won’t know whether it has all the updates associated with given key or not. This is because it has only one token and one key. 
Without such restriction, replica would think it[k] has consistent data, but in reality is has missing second update.
% TODO Replika nie mysli, unikac 
To summarize, optimizations are possible, but only with further constraints. I argue that decision whether such constraints are acceptable or not should be made by users given their use case. For sake of this research, proposed algorithm does not do such optimizations. Note however, such optimization could remain opt-in and configurable even per transaction. Which would yield best performance for specific cases without restricting other kinds of transactions.
       

\subsection{Rollback phase}
Rollback phase was briefly mentioned at high level overview of multi partition paxos algorithm. 
Rollback phase for replica, means that transaction was rolled back at given replica node. 
This phase is special, because if even one replica group decides to move to rollback phase, then all the other replicas have to move to rollback phase. Secondly, there is no transition out of rollback phase. Multi partition paxos algorithm ends and yields result that transaction has been rolled back.
Any replica affected by transaction can transition to rollback phase just after pre prepare phase, but only from following phases:


* Prepare phase
* Propose phase
* Repairing phase


Transaction cannot be rolled back once its proposal was accepted by all replica groups. Paxos guarantees that once proposal has been accepted, current or other leader will finish in progress proposal in that round and effectively transaction will be committed. Process of repairing in progress transaction is described in repairing phase.


If replica has to move to rollback phase upon decision of all replica groups and not by its own. It should notify node about rollback. Node will then purge its private data, remove transaction from index and add information about rolled back transaction to transaction log.


\subsection{Prepare phase}
Prepare phase is the expected next phase after pre prepare phase. During prepare each replica receives ballot, which is just a name for timestamp. Replica needs to check Paxos State to see if ballot is the highest it has seen and only then replica can reply with promise. However, Paxos State is identified by id which is stored in Transaction Index. To find that id, we need to look up index using Transaction State. That requires that ballot is passed along with transaction state, but this state cannot be treated as paxos value. That’s not how paxos algorithm was designed. Transaction State is used only to find id of Paxos State and to compare ballots.


In order to find Paxos Id using Transaction State, Transaction Index is exclusively acquired and queried. Assumption is that transaction has successfully registered in index beforehand. Therefore if it is not found in index it can mean one of:
* Transaction was rolled back by concurrent transaction
* Transaction was committed by other leader which repaired in progress transaction - this is the case, when replicas go back in phase but at least one of them has successfully proposed transaction state.
* This replica was down during pre prepare phase and did not get the message. It could happen due to network partition or hardware failure.


In order to check which case it is, Transaction Log can be queried. Transaction Log knows which transactions have committed and which were rolled back. If transaction id exists in transaction log, then it was either committed or rolled back.


In that case replica can answer that it does not promise anything, but it knows that transaction has committed or rolled back.


In other case, when transaction is neither in index nor in transaction log we can assume that this replica missed pre prephase phase. In this situation replica cannot promise value, because it doesn’t know what Paxos State is. Other replicas can continue without it. Replica might answer without a promise, but also that it doesn’t know what state of transaction is. Response looks exactly the same as in the case when ballot is not the highest. That means, that if we want to know what the situation really is, we need to return more information.


Answer from prepare request has information:
* Boolean promised - whether this replica promises to listen to leader
* Boolean present in index - if transaction was present in the index at this node
* Committed/Rolled back/Unknown - enum value representing status of transaction. It is either known from Transaction Log and then it is one of committed or rolled back or it is unknown
But also it has information as in original paxos
* In progress value - if replica has already accepted other transaction state and this transaction interrupted its execution becoming new leader then the in progress proposal - some other transaction state - is returned to the leader. In that case leader will have to repair multi partition paxos round. Replica will transition into repairing phase.


To summarize prepare phase, it extends original paxos’s prepare phase, because it returns more information. Its purpose stays the same. Replica has to compare ballots and promise to listen to a leader with highest ballot. There might be in progress proposal that has to be completed. Additional information allows to identify whether transaction can continue or not. Responses from all replicas are considered according to moving in phase logic.
After successful transition of all replicas to prepare phase, next transition can be applied.


\subsection{Propose phase}
Propose phase is the next expected phase from prepare phase, but also from repairing phase. Its purpose is to propose transaction state as paxos value to replicas and get majority of acceptances. 


Similarly to prepare phase it is slightly extended. Since previous phase is prepare phase and it was successful, hence transition to proposal, transaction had to exist in Transaction Index. If now, during proposal,  transaction index does not know about transaction then it has to be present in Transaction Log. It has to be either Committed or Rolled back. 


Proposal phase is quite simple. Proposal with Transaction State can be accepted if leader still is the leader and has highest ballot. Proposal is saved in the Paxos State and replica can reply with success. Otherwise leader could have lost his leadership and value cannot be accepted which results in refusal response.


When transaction cannot be found in index, it has to be present in Transaction Log which also helps the leader to make decision whether algorithm should continue.


\subsection{Commit phase}
Commit phase comes after successful propose phase. It can only happen when transaction was accepted by all quorums in all replica groups. Upon commit message replica has to do couple of things:
* Flush transaction’s data locally
   * Find transaction’s data in Private Transaction Storage 
   * Assert that it is consistent hence frozen
   * Apply modifications locally on database -- move modifications from private storage into targeted tables
* Add this transaction to Transaction Log marked as Committed.[l]
* Rollback other concurrent and conflicting transactions
   * Look up other participants in Transaction Index
   * Rollback each conflicting transaction
   * Add conflicting transaction to Transaction Log marked as Rolled back.[m]
* Remove itself from Transaction Index
* Respond with acknowledge


When leader sees that all replica groups received acknowledges it can safely finish multi partition paxos round and respond to a client that transaction has been committed.


\subsection{Repairing phase}
It might happen that during prepare leader receives in progress proposal. It means that it has interrupted some other leader which was executing his transaction. As required by paxos algorithm, in progress proposal has to be completed before first transaction can continue. When replica group sees that it has in progress proposal, this replica group has to transition to repairing phase. 
Other replica groups are also affected because they cannot advance in phase until in progress transaction is repaired. Repaired means that multi partition paxos round is run on in progress transaction until it finishes with commit or rollback. Only after repair first transaction might continue. If it turns out that repaired transaction was rolled back, first transaction has a chance to be committed. If however repaired transaction is committed, main transaction will see that it was rolled back and will also stop.
This means that another multi partition paxos is nested in main multi partition paxos. It is slightly different because it can start from more advanced phase.
If other transaction managed to proposed itself it means that is was in prepared phase hence in pre prepared phase also. Therefore repairing in progress transaction can start with replicas in pre prepare phase. First transition run on replicas is transition to prepare phase skipping registration in index and making transactions data consistent as it was already done by previous leader.


\subsubsection{Moving in phases and repair phase}
Since all replicas have to wait with further transitions until in progress transactions are repaired for all replica groups, repair phase has to be less advanced than prepare phase, but more advanced than pre prepare phase since without being in pre prepare none of replicas would notice any in progress transaction. This is an example where replicas have to move back in phase in order to stay in sync with all replicas executing transaction.


\subsection{The example of \mpp}
Let us consider a cluster of $5$ nodes, and a transaction $t$, which modifies two keys: $k_{1}, k_{2}$ with updates: $u_{1}, u_{2}$. Assume that key $k_{1}$ is replicated by replica group $RG1$, which consists of nodes: $n1$, $n2$ and $n3$, and key $k_{2}$ is replicated by $RG2$, which consists of nodes: $n3, n4, n5$.
Let us assume that updates are already existing in the private transaction storage at each node, and transaction is ready for the commit process, which we analyze step by step:



\begin{enumerate}
\item Nodes start at \emph{no phase}, and first transition has to move them to pre prepared phase.

\item Run transition to pre prepared phase. Assume all nodes do the pre prepare operation successfully. \\
 After the transition, quorums in $RG1$, $RG2$ are in \emph{pre prepare phase}. Minimum phase among replica groups is \emph{pre prepare phase}, therefore all replica groups have same advancement in phase and can continue. Each replica upon receiving a message does:
 \begin{enumerate}
 	\item registers transaction $t$ in the Transaction Index, since there are no concurrent transactions, a new paxos round is created for transaction,
 	\item execute data consistency procedure, for example $n2$ responsible for $k_{1}$ has to read from other replicas of the key: $n1, n3$ and then merge the data, and save it in the private transaction storage.
 \end{enumerate}

\item Run transition to prepare phase. Let us assume successful transition, after which all replica groups are in prepared phase. \\
  After the transition, quorums in $RG1$, $RG2$ are in \emph{prepare phase}. Minimum phase among replica groups is \emph{prepare phase}, therefore all replica groups have same advancement in phase and can continue. Each replica upon receiving a message does:

  \begin{enumerate}
  	\item finds paxos state querying transaction index using transaction state from the message and checks whether the ballot is the higher than promised ballot, since it will be the first ballot in this round, all replicas promise successfully and register the ballot in the paxos state.
  \end{enumerate}

\item Run transition to \emph{propose phase}, let us assume successful transition. \\
	Each replica upon receing a propose message performs:

	\begin{enumerate}
		\item finds paxos state using transaction state,
		\item checks whether ballot from the message is the promised one,
		\item records the proposal from the message, which is the transaction state in the paxos state for transaction $t$, and replies with acceptance.
	\end{enumerate}

\item Run transition to \emph{commit phase}, in which each replica receives a commit message and performs:

	\begin{enumerate}
		\item finds the transaction's data in private transaction storage. In case of $RG1$ it is $k_{1} -> u_{1}$, and in case of $k_{2} -> u_{2}$,
		\item applies the modifications locally,
		\item performs rollback on other concurrent conflicting transactions present in the Transaction Index which are assigned to same paxos round,
		\item adds transaction $t$ to the Transaction Log as committed,
		\item replies to the message with acknowledgement.
	\end{enumerate}

\item The algorithm ends with the committed transaction $t$.

\end{enumerate}

TODO wczesniej byl tutaj bardziej skomplikowany przyklad, ale wydaje mi sie, że jego opis nie wyjasni za wiele.
Moze jako nastepny przyklad? Na pewno musze miec ten prosty bazowy przyklad zeby bylo wiadomo o co chodzi.

% 8. Next transition will do proposal in order to move to proposed phase, however in the meantime other transaction did prepare on node3 and node5 and received promise. Now things should get complicated. 
% 9. Node 1 gets proposal and accepts it. Replica 1 wants to transition to Proposed phase
% 10. Node 2 gets proposal and accepts it. Replica 2 wants to transition to Proposed phase
% 11. Node 3 receives proposal, but it promised to accept higher ballot therefore it responds with refusal. Replica 3 has failed to transition, it is no longer prepared, because it sees that proposal was refused. It knows that it is at most in pre prepared phase. 
% 12. Node 4 gets proposal and accepts it. Replica 4 wants to transition to Proposed phase
% 13. Node 5 does exactly same as Node 3.
% 14. Phases of replicas in RG1 were { node 1 -> prepared, node 2 -> prepared, node 3 -> prepared } and in RG2 { node 3 -> prepared, node 4 -> prepared, node 5 -> prepared }
%         After transition replica groups look as following:


% RG1 { node 1 -> proposed, node 2 -> proposed, node 3 -> pre prepared }
% RG2 { node 3 -> pre prepared, node 4 -> proposed, node 5 -> pre prepared }


% Quorum of replicas in RG1 is in proposed phase.
% Quorum of replicas in RG2 is in pre prepared phase.
% Minimum phase among replica groups is pre prepare phase.
%         Therefore replicas cannot advance further than pre prepared phase. However node 1 for example has already advanced to proposed phase. It will be forced to go back in phase in order to stay in sync with all replica groups. It does not necessarily mean that it has repeat operation. Actual transition performed on replica depends on current phase for all replica groups and current phase of replica. Replica can just do nothing and wait till rest of replicas catch up in phase.
% 1. Phase of all replica groups is pre prepared phase.
% 2. Next transition has to transition replica from its phase to prepared phase.
% 3. Node 1, node 2, node 4 can do a no-operation and just wait.
% 4. Node 3, node 5 have to do actual prepare request with new high ballot
% 5. Phases after transitions are evaluated and algorithm continues.
        
% Above example shows how different multi partition paxos is from LWT. Example shows how phase depends on all replica groups which depend on replicas and all the other replica groups. All of that is required to simply have replicas that stay in sync and where each replica group has satisfied quorum. That is requirement which comes from paxos and all these dependencies relations make it possible.
