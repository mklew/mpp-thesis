%!TEX root = ../thesis.tex

%\section{Multi partition paxos algorithm}

\input{mpp/mpp-theory-algo-idea.tex}

\input{mpp/mpp-theory-algo-rationale.tex}

\input{mpp/mpp-theory-also-requirements.tex}

\input{mpp/mpp-theory-algo-tx-state.tex}

\input{mpp/mpp-theory-also-isolation.tex}





\section{Addressing paxos requirements}
Paxos requirements have to be met. What was easy in case of LWT becomes troublesome considering many partitions. In order to meet those requirements we have to take a deeper look at how MPP algorithm uses paxos.


\subsection{Paxos value}
Value that we need to have consensus about is transaction state. It is because given transaction state, we can identify all modifications in transaction. It also relates to finishing in progress proposal. Other leader has to be able to finish in progress round having only proposed value. Since transaction state allows to find everything related to transaction then proposed value must be transaction state. Transaction state always is an entry point to algorithm.


\subsection{Ability to find paxos state at node given transaction state (value).}
This requirement alone is easy, but if thought about in context of serializability it becomes quite hard. In order to support transactions we need to have property:


If there are many transactions being committed at same time, and those transactions are in conflict with each other, then only one transaction should get committed and rest of them should be rolled back. 
That’s serializability property which we need. How to guarantee it? Let’s look at it differently.


If there are many values being proposed at same time, only one value should be accepted and nodes should consensus about what that value is.


So paxos can do it as long as we can guarantee that conflicting transactions participate in same paxos round. Therefore we need some function, with following properties:


* It maps transaction to paxos round id
* It maps conflicting transactions to same paxos round id 
* It maps non-conflicting transactions to different paxos round id -- otherwise it would not scale well.


\subsection{Conflicting transactions}
In relational world it is well defined what a conflict is. There are different ways of concurrency control which include locks and other mechanisms. In distributed setting in order to have absolute certainty whether two transactions are conflicting it would require to compare their private modifications and see whether they modify same keys. That would result in as many round trips as there are items being modified by transactions multiplied by number of concurrent transactions. It might cause overhead bigger than what it is worth. 


There are other ways to look at conflict, which might not require any further communication but as a tradeoff might have false positives. 


Let’s define function signature and take a look at couple of possibilities how it could be implemented.
Boolean are_transactions_conflicting (transaction state 1, transaction state 2)


Possible conflict functions:
1. Conflict on same transaction items
Transactions are considered conflicting if they share transaction item.


If transactions modify at least one same key in same table, that would result in same token, therefore same transaction item would appear in two transaction states. If transaction states share at least one item then are considered to be in conflict with each other.


However if these two transactions modify different columns in some row for same key they would also be marked as conflicting when in reality, if we really checked it at the column level it would appear that transactions are not conflicting at all. This kind of check would require more communication between nodes increasing overall number of round-trips required by the algorithm.


But simply looking at tokens is an easy way to determine whether two transactions are conflicting or not and no other round-trips are necessary.


1. Conflict on same table
        This function marks transactions as conflicting if they have transaction item for same table. It greatly increases number of conflicts, but reduces number of paxos rounds since all conflicting transactions will participate in same paxos round.
1. Conflict on same token range
        Function has configurable parameter number of token range slices
        It will slice token range evenly for configured number of slices and then assign slice number to transaction item. If two transactions belong to same token range slice then they are conflicting.
        Similarly to previous function, it increases conflicts but allows to have configurable number of concurrent paxos rounds since there will be at most as many rounds as there are slices.


To summarize, conflict functions allow to reduce round trips and also to put bounds on number of concurrent transactions. Algorithm can work with any of above conflict resolution function. Additionally, these functions might be configurable per table or per some other namespace. 




\subsection{Transactions index}


Coming back to first requirement. We need to receive paxos round id given transaction state but also it has to be same round id for different transactions that are in conflict.


To solve this problem, I propose Transaction Index. This is index or registry if you like, in which transactions have to successfully register before they can proceed further in algorithm.        Transaction Index exists on each node in cluster.


Transaction Index has to guarantee that:
* Transactions that are in conflict must be registered in index with same paxos id
* Transaction can register in index only if single paxos id can be assigned to this transaction


Second guarantee requires an example.


Let’s assume that:
* There are three transactions represented by transaction states: Tx1, Tx2, Tx3.
* Tx1 modifies items Ti1, Ti2, Ti3
* Tx2 modifies items Ti2, Ti4
* Tx3 modifies items Ti4
* conflict resolution is defined as conflict on same transaction items.
* All transactions try to execute at same time


Order of transactions registering in transaction index can be arbitrary. Let’s analyze two interesting cases
1. Tx1 -> Tx2 -> Tx3
2. Tx1 -> Tx3 -> Tx2


Case 1)
1. Tx1 registers in index
   1. There are no conflicting transactions
   2. New paxos round is started with paxos id PaxosId_1
1. Tx2 registers in index
   1. Tx1 exists in index and has conflict on item Ti2
   2. Tx2 joins paxos round of Tx1 
   3. Tx2 receives round id PaxosId_1
1. Tx3 registers in index
   1. There are 2 conflicting transactions
   2. But both of them belong to same round. 
   3. Tx3 joins round with id PaxosId_1


In that case, all transactions go into same paxos round, therefore after paxos round, replicas will agree on single transaction. That’s exactly what we wanted and what will guarantee serializability.


Case 2)
1. Tx1 registers in index and receives PaxosId_1
2. Tx3 registers in index
   1. There are no other transactions that modify Ti4
   2. New paxos round is started with paxos id PaxosId_2
1. Tx2 registers in index
   1. There are 2 conflicting transactions: Tx1 and Tx3
   2. Transactions belong to different paxos rounds
   3. Tx2 cannot start and has to rollback or try again.


What if Tx2 was somehow registered in both rounds? Then it might be a case where Tx2 is committed at one round, but rolled back at the other which is illegal because it breaks serializability. We need to guarantee that two concurrent conflicting transactions will not get committed at same time. Allowing Tx2 to participate in any of these paxos rounds could potentially break such guarantee.


Rolling back concurrent transactions
Transaction Index knows which paxos round id to assign to which transaction, but also it knows all the other transactions that share same paxos round id. This knowledge can be used to rollback concurrent transactions when paxos round is finished at this node for some transaction.


To summarize, transaction index main responsibility is to assign paxos round id for each transaction in a way that doesn’t break any of requirements. Index itself changes a lot since there are many concurrent transactions that begin, try to commit and at the end are either committed or rolled back. 




\subsection{Rollback transaction}
Transaction rollback can happen either by:
1. Request of the client
2. Successfully committed concurrent transaction - which happens at the end of paxos round.


Client that wants to rollback transaction has to pass transaction state. Then replicas responsible for items are found and rollback messages are sent. When node receives rollback message it purges all the private data stored in Private Transaction Storage associated with transaction id.


Successfully committed transaction can use Transaction Index to look up locally all other transactions that participate in its round and use Private Transaction Storage to delete their data.


\subsection{Transaction Log}
Transaction Log is a similar concept to Transaction Index. It also has to be present at each node in the cluster. Its responsibility is to track committed and rolled back transactions. Transaction Index is very dynamic and can change frequently. Since transaction index is the only way to find Paxos State and since transactions are removed from index once their are finished, we need something that tells us after the fact whether transaction was committed or not. That is the responsibility of transaction log.


It has to support operations:
* addAsCommitted(transaction id)
* addAsRolledBack(transaction id)
* LogState findTransaction(transaction id) where LogState is an enum with values: 
   * Committed - when transaction has been committed
   * Rolled_back - when transaction has been rolled back
   * Unknown - when transaction with given id hasn’t been added to a log yet


\subsection{The algorithm's description}
Multi partition paxos (MPPaxos) depends on previously described concepts and guarantees they provide. This part of whole solution is responsible for committing transaction. To commit transaction actually means to perform multi partition paxos round. 


        High level description
Algorithm works in phases. Phases are:
1. Pre prepare phase
At each node:
   1. Transaction has to successfully register in Transaction Index
   2. Node has to make transaction’s data consistent and freeze it
1. Prepare phase
Send ballot and receive promise
1. Propose phase
Propose TransactionState, wait for acceptance
1. Commit phase
        Broadcast commit message. Each replica on commit has to
   1. Flush transaction’s data from Private Transaction Storage
   2. Rollback other round participants
        
It looks very like paxos implementation, but details are significantly different. 
Paxos works for single partition, therefore it communicates with replica group of size N. It can proceed to next phase after quorum successfully responds. 
MPP is designed for operations on many partitions therefore there are many replica groups. 
Furthermore, there is another phase not existing in paxos, which is named rollback phase. Replicas can transition to rollback phase at anytime after pre prepare phase. When replica is in such phase it simply means that transaction was rolled back by other concurrent transaction at that replica. 


\subsection{Replica groups in details}
Assume N=3 and transaction for keys: k1, k2, k3, k4 
K1 belongs to replica group n1, n2, n3
K2 belongs to replica group n2, n3, n4
K3 belongs to replica group n1, n5, n7
K4 belongs to replica group n1, n2, n3


Replica groups for key (each transaction item) might have some or all nodes in common.
In this example, K1 and K4 belong to same replica group. K2, K3 however have one node in common with K1 and K4.


As a corollary, any operation done in MPP must be done once per node that is affected by transaction, so in this example, once per n1, n2, n3, n4, n5, n7 in order to not repeat operation on same nodes, but results of these operations have to be evaluated in context of replica groups. 


\subsubsection{Moving forward in phases}
Each replica starts without any phase. In ideal scenario it has to transition from no phase to pre prepared phase, then to prepared phase followed by proposed phase finishing by committed phase. Its transitions however depend on results of transitions of other replicas and other replica groups.


Replica can advance in phase only if:
* Its operation is successful, and
* Each replica group to which that node belongs has quorum of nodes with same advancement of phase, and
* Minimum phase among quorums in replica groups has same advancement


In consequence, all replicas and all replica groups move in sync with each other. None of nodes will do a proposal if there is some replica group for which quorum is not in prepare phase. Similarly, none of nodes will do commit if there exists at least one replica group which didn’t get acceptance of proposal.


Given contention with other transactions, it is possible that replica groups have to go back in phase in order to stay with other replica groups. Either all of them proceed in same phase or none.


\subsubsection{Example}
Minimal example with two replica groups and 4 nodes.
To see it more clearly, let’s go through following scenario where there are 2 replica groups and only 5 nodes. 


Replica group 1 (RG1) has nodes node1, node2 and node3. 
Replica group 2 (RG2) has nodes node3, node4 and node5.


1. Nodes start at no phase.[d] First transition should bring them to pre prepared phase.
2. Transition to pre prepared phase: assume all nodes do pre prepare operation successfully.
3. Quorum in RG1, RG2 is in pre prepare phase.[e]
4. Minimum phase among replica groups is pre prepare phase.
5. Therefore all replica groups are in pre prepare phase.
6. Transition to prepare phase. Let’s assume same successful transition.
7. All replica groups are in prepared phase.
8. Next transition will do proposal in order to move to proposed phase, however in the meantime other transaction did prepare on node3 and node5 and received promise. Now things should get complicated. 
9. Node 1 gets proposal and accepts it. Replica 1 wants to transition to Proposed phase
10. Node 2 gets proposal and accepts it. Replica 2 wants to transition to Proposed phase
11. Node 3 receives proposal, but it promised to accept higher ballot therefore it responds with refusal. Replica 3 has failed to transition, it is no longer prepared, because it sees that proposal was refused. It knows that it is at most in pre prepared phase. 
12. Node 4 gets proposal and accepts it. Replica 4 wants to transition to Proposed phase
13. Node 5 does exactly same as Node 3.
14. Phases of replicas in RG1 were { node 1 -> prepared, node 2 -> prepared, node 3 -> prepared } and in RG2 { node 3 -> prepared, node 4 -> prepared, node 5 -> prepared }
        After transition replica groups look as following:


RG1 { node 1 -> proposed, node 2 -> proposed, node 3 -> pre prepared }
RG2 { node 3 -> pre prepared, node 4 -> proposed, node 5 -> pre prepared }


Quorum of replicas in RG1 is in proposed phase.
Quorum of replicas in RG2 is in pre prepared phase.
Minimum phase among replica groups is pre prepare phase.
        Therefore replicas cannot advance further than pre prepared phase. However node 1 for example has already advanced to proposed phase. It will be forced to go back in phase in order to stay in sync with all replica groups. It does not necessarily mean that it has repeat operation. Actual transition performed on replica depends on current phase for all replica groups and current phase of replica. Replica can just do nothing and wait till rest of replicas catch up in phase.
1. Phase of all replica groups is pre prepared phase.
2. Next transition has to transition replica from its phase to prepared phase.
3. Node 1, node 2, node 4 can do a no-operation and just wait.
4. Node 3, node 5 have to do actual prepare request with new high ballot
5. Phases after transitions are evaluated and algorithm continues.
        
Above example shows how different multi partition paxos is from LWT. Example shows how phase depends on all replica groups which depend on replicas and all the other replica groups. All of that is required to simply have replicas that stay in sync and where each replica group has satisfied quorum. That is requirement which comes from paxos and all these dependencies relations make it possible.


\subsubsection{Algorithm requirements on cluster}
Algorithm can make progress as long as quorum in each replica group is satisfied, thus the algorithm is resilient to failures of n nodes:

% TODO tableka, 
% best case, N, wniosek
% worst case, N, wniosek

        N-denotes number of nodes that can be down[g]
* At best case, when replica groups are disjoint, n = (number of replica groups) * (N - (N/2 + 1)), that is, the minority of replicas can be down for each replica group.
* At worst cas[h]e, when all replica groups intersect, only minority of replicas can be down therefore n= N - (N/2 + 1)



Therefore resilience scales with size of transaction.


\section{Replica phases}
TODO co to jest, po co, lista tych phases a potem konkretny opis.
        Detailed description of phases
In this section discussion will point to details of each phase.


\subsection{Pre prepare phase}
Pre prepare phase is the first phase in multi partition paxos. There two main components in that phase:
* Transaction registers in transaction index 
* Transaction makes it data consistent


\subsubsection{Registering in transaction index}
Transaction Index alone was described in previous section, but here we put more light on it in context of multi partition paxos algorithm. When transaction registers in index, from that moment on transaction participates in paxos round. It might participate alone or with other concurrent transactions that are known to be conflicting. Therefore transaction is vulnerable to rollback by other transaction as soon as it has registered itself in the index.
        Since Transaction Index is accessed by many concurrent transactions, it has to be exclusively acquired by transaction before any operation that changes it state takes place. Locking, or other concurrency control mechanism can be used to guarantee that state of Transaction Index stays consistent.
        Paxos round id used to find Paxos State data at each node locally is synthetic id and is different at each replica during multi partition paxos. I put forward the claim that difference in id between replicas does not have negative effect as long as conflicting transactions are put into same round. If, for some reason, we wanted to use same paxos round id it would require another consensus algorithm of its own which would inherently increase cost of whole algorithm. It is suitable to avoid as many round-trip requests as possible.
        
\subsubsection{Making data consistent}
Transaction’s data is kept in Private Transaction Storage. Private writes by transaction use quorum semantics. I would like to present an example which argues why consistency checking is inevitable.


Example of inconsistent private data
For sake of argument, assume N=3 and two modifications of different keys which point to same replica set.


1. Modification of K1 is sent to replicas.
2. Node 1 has received it
3. Node 2 has received it[i]
4. Node 3 has not received it due to network faults
5. Modification of K1 is successfully written to quorum of replicas.
6. Modification of K2 is sent to replicas.
7. Node 1 has not received it due to network faults
8. Node 2 has received it
9. Node 3 has received it
10. Modification of K2 is again successfully written.


But what is the state of replica nodes?
Node 1 -> { K1 }
Node 2 -> { K1, K2 }
Node 3 -> { K2 }


Only single node has two modifications which illustrates that we don’t have quorum of replica nodes in consistent state. However paxos phases depend on quorum semantics, concretely that quorum of nodes within replica group such as this one is able to proceed and whole algorithm can make progress. 


On these grounds, we can argue that making data consistent at each replica is a mandatory step.
Algorithm to making private transaction data consistent looks as follows:


Inputs:
        TransactionState - state of transaction, it is source of truth of all modifications that happened during transaction. Originally send by client, further forwarded by leader node to this replica node that has to run pre prepare operation.


        Make data consistent algorithm:        
1. Filter transaction items for which this replica is responsible
   1. transaction item has a token
   2. This node knows its token range
   3. Node can identify which transaction item should have reflection in data being present in private transaction storage
1. Find other replicas for these items
   1. Each transaction item has data replicated at other nodes including this one
   2. Topology of nodes on token ring allows to precisely identify them
1. Group transaction items by same replica sets -- it can reduce number of round trips
2. For each replica set, read data from their private transaction storage for transaction items
3. Quorum of responses is required for each replica set -- we know for a fact that private write had to happen to at least quorum of nodes
4. Merge received modifications with those at private transaction storage
5. Freeze transaction’s data so it will not allow any further modifications.


\subsubsection{Optimization on number of requests}
% TODO 'unikac seems'. Przepisac to jakos
We could not limit number of requests to only those transaction items which are missing in Private Transaction Storage. Note that, items have only tokens whereas private data has full information about keys and modifications for these keys. If we check whether we have key in private storage which matches with token then we could omit further requests to other replicas. 
Such optimization could work only under the condition that transaction cannot modify same keys twice. 


For example, given row with key K and columns C1 and C2 we could have updates:
1. Update C1 for K
2. Update C2 for K
3. Update C1 and C2 for K


Each case is valid as long as others do not happen. If we allow 1) and later on 2) we won’t have any guarantees if we have data for both updates. If one update is lost on the way, replica won’t know whether it has all the updates associated with given key or not. This is because it has only one token and one key. 
Without such restriction, replica would think it[k] has consistent data, but in reality is has missing second update.
% TODO Replika nie mysli, unikac 
To summarize, optimizations are possible, but only with further constraints. I argue that decision whether such constraints are acceptable or not should be made by users given their use case. For sake of this research, proposed algorithm does not do such optimizations. Note however, such optimization could remain opt-in and configurable even per transaction. Which would yield best performance for specific cases without restricting other kinds of transactions.
        
To conclude pre prepare phase, it lays out foundation for further paxos phases and makes guarantees that if replica is in pre prepare phase, then it is eligible to participate in paxos round and its private data is eligible for commit. Whether commit occurs or not, it depends on results of next phases.


\subsection{Rollback phase}
Rollback phase was briefly mentioned at high level overview of multi partition paxos algorithm. 
Rollback phase for replica, means that transaction was rolled back at given replica node. 
This phase is special, because if even one replica group decides to move to rollback phase, then all the other replicas have to move to rollback phase. Secondly, there is no transition out of rollback phase. Multi partition paxos algorithm ends and yields result that transaction has been rolled back.
Any replica affected by transaction can transition to rollback phase just after pre prepare phase, but only from following phases:


* Prepare phase
* Propose phase
* Repairing phase


Transaction cannot be rolled back once its proposal was accepted by all replica groups. Paxos guarantees that once proposal has been accepted, current or other leader will finish in progress proposal in that round and effectively transaction will be committed. Process of repairing in progress transaction is described in repairing phase.


If replica has to move to rollback phase upon decision of all replica groups and not by its own. It should notify node about rollback. Node will then purge its private data, remove transaction from index and add information about rolled back transaction to transaction log.


\subsection{Prepare phase}
Prepare phase is the expected next phase after pre prepare phase. During prepare each replica receives ballot, which is just a name for timestamp. Replica needs to check Paxos State to see if ballot is the highest it has seen and only then replica can reply with promise. However, Paxos State is identified by id which is stored in Transaction Index. To find that id, we need to look up index using Transaction State. That requires that ballot is passed along with transaction state, but this state cannot be treated as paxos value. That’s not how paxos algorithm was designed. Transaction State is used only to find id of Paxos State and to compare ballots.


In order to find Paxos Id using Transaction State, Transaction Index is exclusively acquired and queried. Assumption is that transaction has successfully registered in index beforehand. Therefore if it is not found in index it can mean one of:
* Transaction was rolled back by concurrent transaction
* Transaction was committed by other leader which repaired in progress transaction - this is the case, when replicas go back in phase but at least one of them has successfully proposed transaction state.
* This replica was down during pre prepare phase and did not get the message. It could happen due to network partition or hardware failure.


In order to check which case it is, Transaction Log can be queried. Transaction Log knows which transactions have committed and which were rolled back. If transaction id exists in transaction log, then it was either committed or rolled back.


In that case replica can answer that it does not promise anything, but it knows that transaction has committed or rolled back.


In other case, when transaction is neither in index nor in transaction log we can assume that this replica missed pre prephase phase. In this situation replica cannot promise value, because it doesn’t know what Paxos State is. Other replicas can continue without it. Replica might answer without a promise, but also that it doesn’t know what state of transaction is. Response looks exactly the same as in the case when ballot is not the highest. That means, that if we want to know what the situation really is, we need to return more information.


Answer from prepare request has information:
* Boolean promised - whether this replica promises to listen to leader
* Boolean present in index - if transaction was present in the index at this node
* Committed/Rolled back/Unknown - enum value representing status of transaction. It is either known from Transaction Log and then it is one of committed or rolled back or it is unknown
But also it has information as in original paxos
* In progress value - if replica has already accepted other transaction state and this transaction interrupted its execution becoming new leader then the in progress proposal - some other transaction state - is returned to the leader. In that case leader will have to repair multi partition paxos round. Replica will transition into repairing phase.


To summarize prepare phase, it extends original paxos’s prepare phase, because it returns more information. Its purpose stays the same. Replica has to compare ballots and promise to listen to a leader with highest ballot. There might be in progress proposal that has to be completed. Additional information allows to identify whether transaction can continue or not. Responses from all replicas are considered according to moving in phase logic.
After successful transition of all replicas to prepare phase, next transition can be applied.


\subsection{Propose phase}
Propose phase is the next expected phase from prepare phase, but also from repairing phase. Its purpose is to propose transaction state as paxos value to replicas and get majority of acceptances. 


Similarly to prepare phase it is slightly extended. Since previous phase is prepare phase and it was successful, hence transition to proposal, transaction had to exist in Transaction Index. If now, during proposal,  transaction index does not know about transaction then it has to be present in Transaction Log. It has to be either Committed or Rolled back. 


Proposal phase is quite simple. Proposal with Transaction State can be accepted if leader still is the leader and has highest ballot. Proposal is saved in the Paxos State and replica can reply with success. Otherwise leader could have lost his leadership and value cannot be accepted which results in refusal response.


When transaction cannot be found in index, it has to be present in Transaction Log which also helps the leader to make decision whether algorithm should continue.


\subsection{Commit phase}
Commit phase comes after successful propose phase. It can only happen when transaction was accepted by all quorums in all replica groups. Upon commit message replica has to do couple of things:
* Flush transaction’s data locally
   * Find transaction’s data in Private Transaction Storage 
   * Assert that it is consistent hence frozen
   * Apply modifications locally on database -- move modifications from private storage into targeted tables
* Add this transaction to Transaction Log marked as Committed.[l]
* Rollback other concurrent and conflicting transactions
   * Look up other participants in Transaction Index
   * Rollback each conflicting transaction
   * Add conflicting transaction to Transaction Log marked as Rolled back.[m]
* Remove itself from Transaction Index
* Respond with acknowledge


When leader sees that all replica groups received acknowledges it can safely finish multi partition paxos round and respond to a client that transaction has been committed.


\subsection{Repairing phase}
It might happen that during prepare leader receives in progress proposal. It means that it has interrupted some other leader which was executing his transaction. As required by paxos algorithm, in progress proposal has to be completed before first transaction can continue. When replica group sees that it has in progress proposal, this replica group has to transition to repairing phase. 
Other replica groups are also affected because they cannot advance in phase until in progress transaction is repaired. Repaired means that multi partition paxos round is run on in progress transaction until it finishes with commit or rollback. Only after repair first transaction might continue. If it turns out that repaired transaction was rolled back, first transaction has a chance to be committed. If however repaired transaction is committed, main transaction will see that it was rolled back and will also stop.
This means that another multi partition paxos is nested in main multi partition paxos. It is slightly different because it can start from more advanced phase.
If other transaction managed to proposed itself it means that is was in prepared phase hence in pre prepared phase also. Therefore repairing in progress transaction can start with replicas in pre prepare phase. First transition run on replicas is transition to prepare phase skipping registration in index and making transactions data consistent as it was already done by previous leader.


\subsubsection{Moving in phases and repair phase}
Since all replicas have to wait with further transitions until in progress transactions are repaired for all replica groups, repair phase has to be less advanced than prepare phase, but more advanced than pre prepare phase since without being in pre prepare none of replicas would notice any in progress transaction. This is an example where replicas have to move back in phase in order to stay in sync with all replicas executing transaction.




\section{Theoretical number of requests during algorithm}
There can be many modifications to different keys resulting in many transaction items, but what is important are the number of replicas. At worst case each key will reside on different replica set. Most likely case is that some replica groups have replicas in common.


Assuming worst case scenario we have number of replica groups: RGC (Replica Group count)
And replication factor N. For sake of argument let’s assume that all modifications share same replication factor although it is not required by algorithm.


Total number of replicas: TR = N * RGC


Pre prepare phase requires reads from other replicas to make data consistent. Number of requests to make data consistent depends on number of replica groups to which keys owned by given node are replicated. So we have:
keys per node: KPN
And replica groups which own data for these keys: RGOC
We know that RGOC < RGC and at worst case it will be just less than 1 so RGOC = RGC - 1


To make data consistent each node has to make at least quorum of RGOC, but since each node belongs to all the other replica groups, number of requests can be further reduced to QUORUM-1 which equals to (((RGC -1 ) / 2 + 1) - 1 = (RGC -1) / 2
Since each nodes has to do it pre prepare phase requires TR * (RGC -1) / 2 of requests. Plus each replica has to receive message in order to do pre prepare phase therefore pre prephase phase looking at whole algorithm requires TR + TR * ((RGC -1) / 2) 


Prepare phase, propose phase and commit phase require TR requests each.


To sum it up, total number of requests at worst case equals to 
TR ((RGC-1) / 2) + 4*TR


For a five nodes cluster with replication factor N=3 it has
10 different replica sets.




\subsection{Key points regarding phases} 
-- TODO summary is better instead, chyba trzeba to przeniesc po prostu do Phases


\subsection{Example of multi partition paxos}
tutaj diagram musi jakis byc




Summary of algorithm
MPPaxos algorithm supports multi partition transactions which are serializable. Serializability has been achieved by means of distributed consensus provided by extended paxos algorithm and other components required for isolation and conflict resolution. Extended paxos proceeds only when quorum of each replica group agree on phase of algorithm. At its core it is still paxos, but with higher number of constraints which guarantee atomicity and serializability of whole transaction spanning many partitions.