%!TEX root = ../thesis.tex

%\section{Multi partition paxos algorithm}

\input{mpp/mpp-theory-algo-idea.tex}

\input{mpp/mpp-theory-algo-rationale.tex}

\input{mpp/mpp-theory-also-requirements.tex}

\input{mpp/mpp-theory-algo-tx-state.tex}

\input{mpp/mpp-theory-also-isolation.tex}


\section{Addressing paxos requirements}
Paxos requirements have to be met.
 What is simple in case of LWT becomes troublesome in \mpp considering many partitions. 
%In order to meet those requirements we have to take a deeper look at how MPP algorithm uses paxos.


\subsection{Paxos value}
Proposed \emph{value} in \mpp is transaction state. Given transaction state, we can identify all modifications in transaction. Given transaction state, any node can try to commit transaction, because it can check which nodes participate in transaction and where is the private data stored. This fact is used to satisfy requirement, as to proposing highest value (\ref{sec:mpp:requirements:finishInProgress}).
Transaction state always is an entry point to algorithm.
%It also relates to finishing in progress proposal. Other leader has to be able to finish in progress round having only proposed value. Since transaction state allows to find everything related to transaction then proposed value must be transaction state. 


\subsection{Ability to find paxos state at node given transaction state}
In order to have serializability and also to find paxos state given transaction state we need to have two properties:
%This requirement alone is easy, but if thought about in context of serializability it becomes quite hard. In order to support transactions we need to have property:

\begin{definition}
  \label{def:samePaxosStateProperty}
  Conflicting transactions must participate in same paxos round, thus have same \emph{paxos round id} assigned.  
\end{definition}

\begin{definition}
  \label{def:txSerializability}
  If there are many transactions being committed at same time, and those transactions are in conflict with each other, then only one transaction should get committed and rest of them should be rolled back.   
\end{definition}

%If there are many transactions being committed at same time, and those transactions are in conflict with each other, then only one transaction should get committed and rest of them should be rolled back. 
%That’s serializability property which we need. How to guarantee it? Let’s look at it differently.

If there are many values being proposed at same time, only one value should be accepted and nodes should consensus about what that value is. In case of transactions, other values are concurrent conflicting transactions that should be rolled back.

\paxos can guarantee serializability as long as conflicting transactions participate in same paxos round. In case of \lwt \emph{paxos round id} is determined by key \emph{k}. \mpp has support for more than one key, therefore we need a function which maps \emph{transaction} $\rightarrow $ \emph{paxos round id} with properties: 
\begin{enumerate*}
%\item It maps transaction to paxos round id
\item It maps conflicting transactions to same paxos round id 
\item It maps non-conflicting transactions to different paxos round id
\end{enumerate*}


\subsection{Conflicting transactions}\label{sec:theory:conflictFunctions}
In distributed setting in order to have absolute certainty whether two transactions are conflicting it requires to compare their private modifications and see whether they modify same keys. That results in as many round trips as there are items being modified by transactions multiplied by number of concurrent transactions. In distributed settings algorithms should do minimum round trips, because it costs resources and hurts performance \cite{rotem2006fallacies}.

Conflict functions, which consider only local data, and do not require round trips, but as a tradeoff have false positives, are as follows:
\begin{description}
\item[Conflict on same transaction items] \hfill \\
	Transactions are considered conflicting if they share transaction item. 
	\\
	If transactions $tx_{1}, tx{_2}$ modify row with key $k_{1}$ in table $t_{1}$ then both transactions have same transaction item. Given transaction states of two transactions, transaction items can be checked for intersection.
	If they intersect then transactions are considered to be in conflict with each other. \\
	False positives occur when transactions modify row with key $k_{1}$, but $tx_{1}$ modifies column $c_{1}$ and $tx_{2}$ modifies column $c_{2}$. Since transaction item is not resolved to level of column, but to level of row, then function yields conflict when there is none, because different columns are modified.	
\item[Conflict on same table] \hfill \\
		Function marks transactions as conflicting if they have transaction item for same table. It greatly increases number of conflicts, but reduces number of paxos rounds since all conflicting transactions participate in same paxos round.
		False positives occur when transactions modify different keys $k_{1}, k_{2}$ in same table $t_{1}$. 
\item[Conflict on same token range slice] \hfill \\
		Function slices token range into $s$ slices, where $s$ is a configurable parameter. Function takes transaction item and finds slice number for token in transaction item that falls into sliced token range. If two transactions have transaction items which fall into same slice, then transactions are considered conflicting.
		Fuction allows to have configurable number of concurrent paxos rounds since there will be at most as many rounds as there are slices. False positives are expected result of the function.
\end{description}

Conflict functions allow to eliminate round trips, put bounds on number of concurrent transactions, but with a cost of false positives. \mpp algorithm works with any conflict resolution function. Conflict resolution functions can be defined per table.

\subsection{Transactions index}
Paxos round id for a transaction is obtained through \emph{Transactions Index}. This is an index in which transactions have to successfully register. Registration in index of part of \mpp algorithm. Transaction Index exists on each node and is local to a node, thus independent from other nodes.

Transaction Index has to guarantee that: 
\begin{enumerate*}
\item Transactions that are in conflict must be registered in index and obtain same paxos id
\item Transaction can register in index only if there is single paxos id which can be assigned to this transaction.
\end{enumerate*}

In this work, I put forward the claim that \emph{paxos round id} does not have to be globally assigned to same id, but only locally at each node. Therefore, at each node transaction can participate in paxos round with different id. However, at any node, transaction always participates in the same round as conflicting transactions.

Transaction cannot be registered in index when locally it can obtain more than one paxos round id. Algorithm to register is presented at Fig. \ref{alg:indexRegistration}.

\begin{algorithm}
\algblockdefx[NAME]{ForEach}{EndForEach}%
   [2]{\textbf{for each} \emph{#1} $ \gets $ \emph{#2}}%
   {\textbf{end}}   

  \caption{Registration in transaction index}
  \label{alg:indexRegistration}
  \begin{algorithmic}
  	\State $TS \gets$ transaction state of transaction to register
  	\State $rounds \gets $ empty set 
  	\State $items \gets $ subset of $TS.transactionItems$ owned by this node
  	\ForEach{ ti }{TS.transactionItems}
  		\State $conflicting \gets $ \Call{Find Conflicting}{ti} \Comment 
  		\If {conflicting is not empty } 
        	\State add paxos round id from conflicting participants to $rounds$
      \EndIf	
  	\EndForEach    
  	\State $round \gets$ unassigned round id
  	\If {rounds size = 0}
  		\State $round \gets$ generate new round id 
  		\State \Call{Append Transaction}{TS, items, round} 		
  	\ElsIf {rounds size = 1}
  		\State $round \gets$ get round from $rounds$
  		\State \Call{Append Transaction}{TS, items, round}
  	\Else
  		\State transaction cannot be registered in index
  	\EndIf


  	\Procedure{Append Transaction}{$TS, items, round$} 
	  	\ForEach{ ti }{items}
	  		\State $participants \gets$ get participants from $index$ map by key $ti$ 
	  		\State append $(TS, round)$ to participants
	  	\EndForEach       		
	\EndProcedure  	
  \end{algorithmic}
   
\end{algorithm}

\subsubsection{Example of registration}
Assume $3$ transactions: $tx_{1}, tx_{2}, tx_{3}$ and $4$ different transaction items: $ti_{1}, ti_{2}, ti_{3}, ti_{4}$. Assume conflict function on same transaction items.
Assume items in transactions $tx_{1} \rightarrow (ti_{1}, ti_{2}, ti_{3})$, 
 $tx_{2} \rightarrow (ti_{2}, ti_{4})$, $tx_{3} \rightarrow (ti_{4})$
Assume that registration of transactions happens in arbitrary order. Registration in index is done according to algorithm (\ref{alg:indexRegistration}). Two different orderings yield different outcomes. 
 \begin{description}
 \item[Order $tx_{1} \rightarrow tx_{2} \rightarrow tx_{3}$] \hfill \\
 	$tx_{1}$ registers in index and obtains paxos id $id_{1}$ \\
 	$tx_{2}$ registers in index and obtains paxos id from conflicting transaction $tx_{1}$, $id_{1}$ \\
 	$tx_{3}$ registers in index and obtains paxos id $id_{1}$ from conflicting transactions: $tx_{1}, tx_{2}$ All transactions participate in the same paxos round. Only one transaction is chosen during paxos round. 
 \item[Order $tx_{1} \rightarrow tx_{3} \rightarrow tx_{2}$] \hfill \\
 	$tx_{1}$ registers in index and obtains paxos id $id_{1}$ \\
 	$tx_{3}$ registers in index, there are no conflicts, thus obtains paxos id $id_{3}$\\
 	$tx_{2}$ registers in index and it has $2$ conflicting transactions participating in different paxos rounds with ids: $id_{1}$, $id_{3}$. It is illegal to register $tx_{2}$, because if $tx_{2}$ is registered in both rounds it can be accepted in round $id_{1}$, but not accepted in round $id_{3}$ in which case it is committed and rolled back at the same time which is illegal behaviour. 

 \end{description}

% Let’s assume that:
% * There are three transactions represented by transaction states: Tx1, Tx2, Tx3.
% * Tx1 modifies items Ti1, Ti2, Ti3
% * Tx2 modifies items Ti2, Ti4
% * Tx3 modifies items Ti4
% * conflict resolution is defined as conflict on same transaction items.
% * All transactions try to execute at same time


% Order of transactions registering in transaction index can be arbitrary. Let’s analyze two interesting cases
% 1. Tx1 -> Tx2 -> Tx3
% 2. Tx1 -> Tx3 -> Tx2


% Case 1)
% 1. Tx1 registers in index
%    1. There are no conflicting transactions
%    2. New paxos round is started with paxos id PaxosId_1
% 1. Tx2 registers in index
%    1. Tx1 exists in index and has conflict on item Ti2
%    2. Tx2 joins paxos round of Tx1 
%    3. Tx2 receives round id PaxosId_1
% 1. Tx3 registers in index
%    1. There are 2 conflicting transactions
%    2. But both of them belong to same round. 
%    3. Tx3 joins round with id PaxosId_1


% In that case, all transactions go into same paxos round, therefore after paxos round, replicas will agree on single transaction. That’s exactly what we wanted and what will guarantee serializability.


% Case 2)
% 1. Tx1 registers in index and receives PaxosId_1
% 2. Tx3 registers in index
%    1. There are no other transactions that modify Ti4
%    2. New paxos round is started with paxos id PaxosId_2
% 1. Tx2 registers in index
%    1. There are 2 conflicting transactions: Tx1 and Tx3
%    2. Transactions belong to different paxos rounds
%    3. Tx2 cannot start and has to rollback or try again.


% What if Tx2 was somehow registered in both rounds? Then it might be a case where Tx2 is committed at one round, but rolled back at the other which is illegal because it breaks serializability. We need to guarantee that two concurrent conflicting transactions will not get committed at same time. Allowing Tx2 to participate in any of these paxos rounds could potentially break such guarantee.


\subsubsection{Rolling back concurrent transactions}
\emph{Transaction Index} knows which paxos round id to assign to which transaction, but also it knows other transactions that share same paxos round id. This knowledge is used to rollback concurrent transactions when paxos round is finished at this node and transaction is committed.


% To summarize, transaction index main responsibility is to assign paxos round id for each transaction in a way that doesn’t break any of requirements. Index itself changes a lot since there are many concurrent transactions that begin, try to commit and at the end are either committed or rolled back. 


\subsection{Time of rollback}
Transaction rollback happens either by:
\begin{enumerate*}
\item successfully committed concurrent transaction - which happens at the end of paxos round
\item request of the client
\end{enumerate*}

Successfully committed transaction uses \emph{Transaction Index} to find conflicting transactions that participate in its round and use \emph{Private Transaction Storage} to delete their data, thus rollback transaction locally at the node.

Client that wants to rollback transaction has to pass transaction state. Then replicas responsible for items are found and rollback messages are sent. When node receives rollback message it purges all the private data stored in \emph{Private Transaction Storage} associated with transaction id.

\section{Transaction Log}
\emph{Transaction Log} is a similar concept to \emph{Transaction Index}. It also has to be present at each node in the cluster. Its responsibility is to record committed and rolled back transactions. \emph{Transaction Index} changes frequently, because transactions are registered in it and removed on rollback. Since \emph{transaction index} is the only way to find Paxos State and since transactions are removed from index once are finished, we need a log that has information whether transaction is committed or not. It has to support following operations:
\begin{description}
\item[\code{addAsCommitted (transaction id)}] -- records committed transaction
\item[\code{addAsRolledBack (transaction id)}] -- records rolled back transaction
\item[\code{findTransaction(transaction id)}] -- finds information about transaction. Returns \emph{LogState} which can be one of: \begin{enumerate*} 
		\item \code{Committed} -- when transaction has been committed
		\item \code{Rolled_back} -- when transaction has been rolled back
		\item \code{Unknown} -- when transaction with given id hasn’t been added to a log yet
		\end{enumerate*} 
\end{description}
The log stores information off memory, in dedicated table, in order to preserve information about transaction after it is removed from \emph{Transaction Index}.

\section{The algorithm's description}
Multi partition paxos algorithm depends on previously described concepts and guarantees they provide. This part of whole solution is responsible for committing transaction. Transaction is committed by performing multi partition paxos round, in which transaction is accepted by majority from all replica groups \ref{sec:mpp:replicaGroups}. 
% TODO wyjaśnić czym są replica groups.

\subsection{Phases}
The algorithm consists of \emph{phases}, which are: 
\begin{enumerate*}
\item no phase
\item pre prepare phase
\item repair phase
\item prepare phase
\item propose phase
\item commit phase
\item rollback phase
\end{enumerate*}
Following sections contain descriptions of each phase.



        High level description
Algorithm works in phases. Phases are:
1. Pre prepare phase
At each node:
   1. Transaction has to successfully register in Transaction Index
   2. Node has to make transaction’s data consistent and freeze it
1. Prepare phase
Send ballot and receive promise
1. Propose phase
Propose TransactionState, wait for acceptance
1. Commit phase
        Broadcast commit message. Each replica on commit has to
   1. Flush transaction’s data from Private Transaction Storage
   2. Rollback other round participants
        
It looks very like paxos implementation, but details are significantly different. 
Paxos works for single partition, therefore it communicates with replica group of size N. It can proceed to next phase after quorum successfully responds. 
MPP is designed for operations on many partitions therefore there are many replica groups. 
Furthermore, there is another phase not existing in paxos, which is named rollback phase. Replicas can transition to rollback phase at anytime after pre prepare phase. When replica is in such phase it simply means that transaction was rolled back by other concurrent transaction at that replica. 


\subsection{Replica groups in details}
\label{sec:mpp:replicaGroups}
Assume $N=3$ and transaction for keys: $k1$, $k2$, $k3$, $k4$.
Key $k1$ belongs to replica group $n1$, $n2$, $n3$.
Key $k2$ belongs to replica group $n2$, $n3$, $n4$.
Key $k3$ belongs to replica group $n1$, $n5$, $n7$.
Key $k4$ belongs to replica group $n1$, $n2$, $n3$.


Replica groups for a key\footnote{each transaction item} might have some or all nodes in common.
In this example, $k1$ and $k4$ belong to the same replica group. Keys $k2$, $k3$ however have one node in common with $k1$ and $k4$.

As a corollary, any operation done in \mpp must be done once per node that is affected by the transaction, thus once per $n1$, $n2$, $n3$, $n4$, $n5$, $n7$ in order to not repeat operation on same nodes, but the results of these operations have to be evaluated in context of the replica groups. 


\subsubsection{Moving forward in phases}
Each replica starts without any phase. In ideal scenario it has to transition from no phase to pre prepared phase, then to prepared phase followed by proposed phase finishing by committed phase. Its transitions however depend on results of transitions of other replicas and other replica groups.


Replica advances in phase only if conditions are met: \begin{enumerate*}
\item its operation is successful, and
\item each replica group to which that node belongs has quorum of nodes with same advancement of phase, and
\item minimum phase among quorums in all replica groups has same advancement.
\end{enumerate*}


In consequence, all replicas and all replica groups move in sync with each other. None of nodes will do a proposal if there is some replica group for which quorum is not in prepare phase. Similarly, none of nodes will do commit if there exists at least one replica group which didn’t get acceptance of proposal.


Given contention with other transactions, it is possible that replica groups have to go back in phase in order to stay with other replica groups. Either all of them proceed in same phase or none.


\subsubsection{Example}
For example, let us consider two replica groups within a cluster of $5$ nodes.
Replica group 1 ($RG1$) has nodes: $node1$, $node2$ and $node3$. 
Replica group 2 ($RG2$) has nodes $node3$, $node4$ and $node5$.

\begin{enumerate}
\item Nodes start at no phase, and first transition has to move them to pre prepared phase.
\item Run transition to pre prepared phase. Assume all nodes do the pre prepare operation successfully.
\item Quorums in $RG1$, $RG2$ are in \emph{pre prepare phase}.
\item Minimum phase among replica groups is \emph{pre prepare phase}, therefore all replica groups have same advancement in phase.
\item Run transition to prepare phase. Let us assume successful transition, after which all replica groups are in prepared phase.
\item a
\end{enumerate}

8. Next transition will do proposal in order to move to proposed phase, however in the meantime other transaction did prepare on node3 and node5 and received promise. Now things should get complicated. 
9. Node 1 gets proposal and accepts it. Replica 1 wants to transition to Proposed phase
10. Node 2 gets proposal and accepts it. Replica 2 wants to transition to Proposed phase
11. Node 3 receives proposal, but it promised to accept higher ballot therefore it responds with refusal. Replica 3 has failed to transition, it is no longer prepared, because it sees that proposal was refused. It knows that it is at most in pre prepared phase. 
12. Node 4 gets proposal and accepts it. Replica 4 wants to transition to Proposed phase
13. Node 5 does exactly same as Node 3.
14. Phases of replicas in RG1 were { node 1 -> prepared, node 2 -> prepared, node 3 -> prepared } and in RG2 { node 3 -> prepared, node 4 -> prepared, node 5 -> prepared }
        After transition replica groups look as following:


RG1 { node 1 -> proposed, node 2 -> proposed, node 3 -> pre prepared }
RG2 { node 3 -> pre prepared, node 4 -> proposed, node 5 -> pre prepared }


Quorum of replicas in RG1 is in proposed phase.
Quorum of replicas in RG2 is in pre prepared phase.
Minimum phase among replica groups is pre prepare phase.
        Therefore replicas cannot advance further than pre prepared phase. However node 1 for example has already advanced to proposed phase. It will be forced to go back in phase in order to stay in sync with all replica groups. It does not necessarily mean that it has repeat operation. Actual transition performed on replica depends on current phase for all replica groups and current phase of replica. Replica can just do nothing and wait till rest of replicas catch up in phase.
1. Phase of all replica groups is pre prepared phase.
2. Next transition has to transition replica from its phase to prepared phase.
3. Node 1, node 2, node 4 can do a no-operation and just wait.
4. Node 3, node 5 have to do actual prepare request with new high ballot
5. Phases after transitions are evaluated and algorithm continues.
        
Above example shows how different multi partition paxos is from LWT. Example shows how phase depends on all replica groups which depend on replicas and all the other replica groups. All of that is required to simply have replicas that stay in sync and where each replica group has satisfied quorum. That is requirement which comes from paxos and all these dependencies relations make it possible.


\subsubsection{Algorithm requirements on cluster}
Algorithm can make progress as long as quorum in each replica group is satisfied, thus the algorithm is resilient to failures of n nodes:

% TODO tableka, 
% best case, N, wniosek
% worst case, N, wniosek

        N-denotes number of nodes that can be down[g]
* At best case, when replica groups are disjoint, n = (number of replica groups) * (N - (N/2 + 1)), that is, the minority of replicas can be down for each replica group.
* At worst cas[h]e, when all replica groups intersect, only minority of replicas can be down therefore n= N - (N/2 + 1)



Therefore resilience scales with size of transaction.


\section{Replica phases}
TODO co to jest, po co, lista tych phases a potem konkretny opis.
        Detailed description of phases
In this section discussion will point to details of each phase.


\subsection{Pre prepare phase}
Pre prepare phase is the first phase in multi partition paxos. There two main components in that phase:
* Transaction registers in transaction index 
* Transaction makes it data consistent


\subsubsection{Registering in transaction index}
Transaction Index alone was described in previous section, but here we put more light on it in context of multi partition paxos algorithm. When transaction registers in index, from that moment on transaction participates in paxos round. It might participate alone or with other concurrent transactions that are known to be conflicting. Therefore transaction is vulnerable to rollback by other transaction as soon as it has registered itself in the index.
        Since Transaction Index is accessed by many concurrent transactions, it has to be exclusively acquired by transaction before any operation that changes it state takes place. Locking, or other concurrency control mechanism can be used to guarantee that state of Transaction Index stays consistent.
        Paxos round id used to find Paxos State data at each node locally is synthetic id and is different at each replica during multi partition paxos. I put forward the claim that difference in id between replicas does not have negative effect as long as conflicting transactions are put into same round. If, for some reason, we wanted to use same paxos round id it would require another consensus algorithm of its own which would inherently increase cost of whole algorithm. It is suitable to avoid as many round-trip requests as possible.
        
\subsubsection{Making data consistent}
Transaction’s data is kept in Private Transaction Storage. Private writes by transaction use quorum semantics. I would like to present an example which argues why consistency checking is inevitable.


Example of inconsistent private data
For sake of argument, assume N=3 and two modifications of different keys which point to same replica set.


1. Modification of K1 is sent to replicas.
2. Node 1 has received it
3. Node 2 has received it[i]
4. Node 3 has not received it due to network faults
5. Modification of K1 is successfully written to quorum of replicas.
6. Modification of K2 is sent to replicas.
7. Node 1 has not received it due to network faults
8. Node 2 has received it
9. Node 3 has received it
10. Modification of K2 is again successfully written.


But what is the state of replica nodes?
Node 1 -> { K1 }
Node 2 -> { K1, K2 }
Node 3 -> { K2 }


Only single node has two modifications which illustrates that we don’t have quorum of replica nodes in consistent state. However paxos phases depend on quorum semantics, concretely that quorum of nodes within replica group such as this one is able to proceed and whole algorithm can make progress. 


On these grounds, we can argue that making data consistent at each replica is a mandatory step.
Algorithm to making private transaction data consistent looks as follows:


Inputs:
        TransactionState - state of transaction, it is source of truth of all modifications that happened during transaction. Originally send by client, further forwarded by leader node to this replica node that has to run pre prepare operation.


        Make data consistent algorithm:        
1. Filter transaction items for which this replica is responsible
   1. transaction item has a token
   2. This node knows its token range
   3. Node can identify which transaction item should have reflection in data being present in private transaction storage
1. Find other replicas for these items
   1. Each transaction item has data replicated at other nodes including this one
   2. Topology of nodes on token ring allows to precisely identify them
1. Group transaction items by same replica sets -- it can reduce number of round trips
2. For each replica set, read data from their private transaction storage for transaction items
3. Quorum of responses is required for each replica set -- we know for a fact that private write had to happen to at least quorum of nodes
4. Merge received modifications with those at private transaction storage
5. Freeze transaction’s data so it will not allow any further modifications.


\subsubsection{Optimization on number of requests}
% TODO 'unikac seems'. Przepisac to jakos
We could not limit number of requests to only those transaction items which are missing in Private Transaction Storage. Note that, items have only tokens whereas private data has full information about keys and modifications for these keys. If we check whether we have key in private storage which matches with token then we could omit further requests to other replicas. 
Such optimization could work only under the condition that transaction cannot modify same keys twice. 


For example, given row with key K and columns C1 and C2 we could have updates:
1. Update C1 for K
2. Update C2 for K
3. Update C1 and C2 for K


Each case is valid as long as others do not happen. If we allow 1) and later on 2) we won’t have any guarantees if we have data for both updates. If one update is lost on the way, replica won’t know whether it has all the updates associated with given key or not. This is because it has only one token and one key. 
Without such restriction, replica would think it[k] has consistent data, but in reality is has missing second update.
% TODO Replika nie mysli, unikac 
To summarize, optimizations are possible, but only with further constraints. I argue that decision whether such constraints are acceptable or not should be made by users given their use case. For sake of this research, proposed algorithm does not do such optimizations. Note however, such optimization could remain opt-in and configurable even per transaction. Which would yield best performance for specific cases without restricting other kinds of transactions.
        
To conclude pre prepare phase, it lays out foundation for further paxos phases and makes guarantees that if replica is in pre prepare phase, then it is eligible to participate in paxos round and its private data is eligible for commit. Whether commit occurs or not, it depends on results of next phases.


\subsection{Rollback phase}
Rollback phase was briefly mentioned at high level overview of multi partition paxos algorithm. 
Rollback phase for replica, means that transaction was rolled back at given replica node. 
This phase is special, because if even one replica group decides to move to rollback phase, then all the other replicas have to move to rollback phase. Secondly, there is no transition out of rollback phase. Multi partition paxos algorithm ends and yields result that transaction has been rolled back.
Any replica affected by transaction can transition to rollback phase just after pre prepare phase, but only from following phases:


* Prepare phase
* Propose phase
* Repairing phase


Transaction cannot be rolled back once its proposal was accepted by all replica groups. Paxos guarantees that once proposal has been accepted, current or other leader will finish in progress proposal in that round and effectively transaction will be committed. Process of repairing in progress transaction is described in repairing phase.


If replica has to move to rollback phase upon decision of all replica groups and not by its own. It should notify node about rollback. Node will then purge its private data, remove transaction from index and add information about rolled back transaction to transaction log.


\subsection{Prepare phase}
Prepare phase is the expected next phase after pre prepare phase. During prepare each replica receives ballot, which is just a name for timestamp. Replica needs to check Paxos State to see if ballot is the highest it has seen and only then replica can reply with promise. However, Paxos State is identified by id which is stored in Transaction Index. To find that id, we need to look up index using Transaction State. That requires that ballot is passed along with transaction state, but this state cannot be treated as paxos value. That’s not how paxos algorithm was designed. Transaction State is used only to find id of Paxos State and to compare ballots.


In order to find Paxos Id using Transaction State, Transaction Index is exclusively acquired and queried. Assumption is that transaction has successfully registered in index beforehand. Therefore if it is not found in index it can mean one of:
* Transaction was rolled back by concurrent transaction
* Transaction was committed by other leader which repaired in progress transaction - this is the case, when replicas go back in phase but at least one of them has successfully proposed transaction state.
* This replica was down during pre prepare phase and did not get the message. It could happen due to network partition or hardware failure.


In order to check which case it is, Transaction Log can be queried. Transaction Log knows which transactions have committed and which were rolled back. If transaction id exists in transaction log, then it was either committed or rolled back.


In that case replica can answer that it does not promise anything, but it knows that transaction has committed or rolled back.


In other case, when transaction is neither in index nor in transaction log we can assume that this replica missed pre prephase phase. In this situation replica cannot promise value, because it doesn’t know what Paxos State is. Other replicas can continue without it. Replica might answer without a promise, but also that it doesn’t know what state of transaction is. Response looks exactly the same as in the case when ballot is not the highest. That means, that if we want to know what the situation really is, we need to return more information.


Answer from prepare request has information:
* Boolean promised - whether this replica promises to listen to leader
* Boolean present in index - if transaction was present in the index at this node
* Committed/Rolled back/Unknown - enum value representing status of transaction. It is either known from Transaction Log and then it is one of committed or rolled back or it is unknown
But also it has information as in original paxos
* In progress value - if replica has already accepted other transaction state and this transaction interrupted its execution becoming new leader then the in progress proposal - some other transaction state - is returned to the leader. In that case leader will have to repair multi partition paxos round. Replica will transition into repairing phase.


To summarize prepare phase, it extends original paxos’s prepare phase, because it returns more information. Its purpose stays the same. Replica has to compare ballots and promise to listen to a leader with highest ballot. There might be in progress proposal that has to be completed. Additional information allows to identify whether transaction can continue or not. Responses from all replicas are considered according to moving in phase logic.
After successful transition of all replicas to prepare phase, next transition can be applied.


\subsection{Propose phase}
Propose phase is the next expected phase from prepare phase, but also from repairing phase. Its purpose is to propose transaction state as paxos value to replicas and get majority of acceptances. 


Similarly to prepare phase it is slightly extended. Since previous phase is prepare phase and it was successful, hence transition to proposal, transaction had to exist in Transaction Index. If now, during proposal,  transaction index does not know about transaction then it has to be present in Transaction Log. It has to be either Committed or Rolled back. 


Proposal phase is quite simple. Proposal with Transaction State can be accepted if leader still is the leader and has highest ballot. Proposal is saved in the Paxos State and replica can reply with success. Otherwise leader could have lost his leadership and value cannot be accepted which results in refusal response.


When transaction cannot be found in index, it has to be present in Transaction Log which also helps the leader to make decision whether algorithm should continue.


\subsection{Commit phase}
Commit phase comes after successful propose phase. It can only happen when transaction was accepted by all quorums in all replica groups. Upon commit message replica has to do couple of things:
* Flush transaction’s data locally
   * Find transaction’s data in Private Transaction Storage 
   * Assert that it is consistent hence frozen
   * Apply modifications locally on database -- move modifications from private storage into targeted tables
* Add this transaction to Transaction Log marked as Committed.[l]
* Rollback other concurrent and conflicting transactions
   * Look up other participants in Transaction Index
   * Rollback each conflicting transaction
   * Add conflicting transaction to Transaction Log marked as Rolled back.[m]
* Remove itself from Transaction Index
* Respond with acknowledge


When leader sees that all replica groups received acknowledges it can safely finish multi partition paxos round and respond to a client that transaction has been committed.


\subsection{Repairing phase}
It might happen that during prepare leader receives in progress proposal. It means that it has interrupted some other leader which was executing his transaction. As required by paxos algorithm, in progress proposal has to be completed before first transaction can continue. When replica group sees that it has in progress proposal, this replica group has to transition to repairing phase. 
Other replica groups are also affected because they cannot advance in phase until in progress transaction is repaired. Repaired means that multi partition paxos round is run on in progress transaction until it finishes with commit or rollback. Only after repair first transaction might continue. If it turns out that repaired transaction was rolled back, first transaction has a chance to be committed. If however repaired transaction is committed, main transaction will see that it was rolled back and will also stop.
This means that another multi partition paxos is nested in main multi partition paxos. It is slightly different because it can start from more advanced phase.
If other transaction managed to proposed itself it means that is was in prepared phase hence in pre prepared phase also. Therefore repairing in progress transaction can start with replicas in pre prepare phase. First transition run on replicas is transition to prepare phase skipping registration in index and making transactions data consistent as it was already done by previous leader.


\subsubsection{Moving in phases and repair phase}
Since all replicas have to wait with further transitions until in progress transactions are repaired for all replica groups, repair phase has to be less advanced than prepare phase, but more advanced than pre prepare phase since without being in pre prepare none of replicas would notice any in progress transaction. This is an example where replicas have to move back in phase in order to stay in sync with all replicas executing transaction.




\section{Theoretical number of requests during algorithm}
There can be many modifications to different keys resulting in many transaction items, but what is important are the number of replicas. At worst case each key will reside on different replica set. Most likely case is that some replica groups have replicas in common.


Assuming worst case scenario we have number of replica groups: RGC (Replica Group count)
And replication factor N. For sake of argument let’s assume that all modifications share same replication factor although it is not required by algorithm.


Total number of replicas: TR = N * RGC


Pre prepare phase requires reads from other replicas to make data consistent. Number of requests to make data consistent depends on number of replica groups to which keys owned by given node are replicated. So we have:
keys per node: KPN
And replica groups which own data for these keys: RGOC
We know that RGOC < RGC and at worst case it will be just less than 1 so RGOC = RGC - 1


To make data consistent each node has to make at least quorum of RGOC, but since each node belongs to all the other replica groups, number of requests can be further reduced to QUORUM-1 which equals to (((RGC -1 ) / 2 + 1) - 1 = (RGC -1) / 2
Since each nodes has to do it pre prepare phase requires TR * (RGC -1) / 2 of requests. Plus each replica has to receive message in order to do pre prepare phase therefore pre prephase phase looking at whole algorithm requires TR + TR * ((RGC -1) / 2) 


Prepare phase, propose phase and commit phase require TR requests each.


To sum it up, total number of requests at worst case equals to 
TR ((RGC-1) / 2) + 4*TR


For a five nodes cluster with replication factor N=3 it has
10 different replica sets.




\subsection{Key points regarding phases} 
-- TODO summary is better instead, chyba trzeba to przeniesc po prostu do Phases


\subsection{Example of multi partition paxos}
tutaj diagram musi jakis byc




Summary of algorithm
MPPaxos algorithm supports multi partition transactions which are serializable. Serializability has been achieved by means of distributed consensus provided by extended paxos algorithm and other components required for isolation and conflict resolution. Extended paxos proceeds only when quorum of each replica group agree on phase of algorithm. At its core it is still paxos, but with higher number of constraints which guarantee atomicity and serializability of whole transaction spanning many partitions.